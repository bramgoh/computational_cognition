---
title: "Exploring discrepancy encoding with ITS (Part 2)"
author: "Bram Goh"
date: "2023-04-01"
categories: [code]
image: ""
---

# Implementing discrepancy encoding with TASA

Goals for this exercise:

-   Replicate MDS solution for TASA corpus as in Jamieson et al. (2018)

-   Identify words with varying frequencies and plot intensity graphs as per Hintzman (1988)

-   Repeat while applying discrepancy encoding

## Word frequencies with no discrepancy encoding

```{r}
library(RsemanticLibrarian)
library(tidyverse)
library(lsa)

TASA_full <- read.table("tasaDocsPara.txt", 
                          sep="\t", 
                          fill=FALSE, 
                          strip.white=TRUE)

training <- TASA_full$V1[1:2000]

l_value <- 0.7
```

### Functions

```{r}

# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)

get_activations <- function(probe, mem, type) {
  if(type == "hintzman"){
    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))
  }
  if(type == "cosine"){
    temp_activ <- as.numeric(cosine_x_to_m(probe,mem))
    temp_activ[is.nan(temp_activ) == TRUE] <- 0
    return(temp_activ)
  }
}

# Generate echo (borrowed from Matt)
get_echo <- function(probe, mem, tau=3, output='intensity', type) {
    activations <- get_activations(probe, mem, type)
    if(output == "intensity"){
      return(sum(activations^tau))
    }
    if(output == "echo"){
      weighted_memory <- mem * (activations^tau)  
      summed_echo <- colSums(weighted_memory)
      return(summed_echo)
    }
    if(output == "both"){
      weighted_memory <- mem * (activations^tau)  
      summed_echo <- colSums(weighted_memory)
      model_output <- list(intensity = sum(activations^tau),
                           echo = summed_echo)
      return(model_output)
    }
    
}

# Create dictionary (modified from RsemanticLibrarian)

corpus_dictionary <- function(words){
  neat_words <- words %>%
                as.character() %>%
                strsplit(split=" ") %>%
                unlist() %>%
                qdapRegex::rm_white_lead_trail() %>%
                strsplit(split=" ") %>%
                unlist() %>%
                unique()
  return(neat_words)
}

# Functions to ensure strings only contain words (modified from RsemanticLibrarian)
clean <- function(words){
  if(length(words) == 1) {
    neat_words <- words %>%
      as.character() %>%
      strsplit(split=" ") %>%
      unlist() %>%
      qdapRegex::rm_white_lead_trail() %>%
      strsplit(split=" ") %>%
      unlist() 
    return(neat_words)
  }
}

clean_vector <- function(words){
  return(lapply(unlist(strsplit(words,split="[.]")), clean))
}

# Function to generate echoes for a list of words (from Matt)
subset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){
  
  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],
                         nrow = length(strings))
  
  for (i in 1:length(strings)) {
    word_id <- which(dictionary %in% strings[i])
    probe <- e_vectors[word_id, ]
    echo <- get_echo(probe, sentence_memory, tau = tau, output = "echo", type = "cosine")
    word_meaning[i, ] <- echo
  }
  row.names(word_meaning) <- strings
  
  return(word_meaning)
}

# Find top n similar words in terms of semantic meaning
find_similar_words <- function(meaning_matrix, dictionary, word_to_find, n) {
  similarities <- cosine_x_to_m(meaning_matrix[which(dictionary$word == word_to_find), ], meaning_matrix)
similarities_df <- data.frame(dictionary, similarities = similarities)
sim_top <- similarities_df %>% arrange(desc(similarities)) %>% slice(1:n)
return(sim_top)
}
```

### Pre-processing TASA corpus (with 2000 sentences) and MDS solution

```{r}

dictionary <- corpus_dictionary(training)

clean_sentences <- clean_vector(training)

word_ids <- sl_word_ids_sentences(clean_sentences, dictionary)

env_vectors <- sl_create_riv(5000, length(dictionary), 4)

# Setting up prior memory without discrepancy encoding (also applying L value)
prior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(env_vectors), replace = TRUE)))
memory_no_de <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(word_ids)))

for(i in 1:length(word_ids)){
  current_sentence <- colSums(env_vectors[word_ids[[i]], ])
  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))
  memory_no_de[4+i, ] <- current_sentence*learning_vector
}

# Identifying words with different frequencies

JonesMewhort_Figure3 <- c("financial","savings","finance","pay","invested",
                          "loaned","borrow","lend","invest","investments",
                          "bank","spend","save","astronomy","physics",
                          "chemistry","psychology","biology","scientific",
                          "mathematics","technology","scientists","science",
                          "research","sports","team","teams","football",
                          "coach","sport","players","baseball","soccer",
                          "tennis","basketball")

word_freq_table <- clean_sentences %>% unlist() %>% table() %>% as_tibble() %>% filter(. %in% JonesMewhort_Figure3) %>% arrange(n)

word_subset <- c("astronomy", "loaned", "scientific", "save", "science", "bank", "scientists", "pay")

# Get echoes (semantic meaning vectors)
meaning_no_de <- subset_semantic_vectors(word_subset, dictionary, env_vectors, memory_no_de, tau = 3)

cosine_no_de <- cosine(t(meaning_no_de))

corrplot::corrplot(cosine_no_de,
                   method="circle",
                   order="hclust",
                   tl.col="black",
                   tl.cex=.7,
                   title="",
                   cl.cex=.6)
```

```{r}
mds_no_de <- cmdscale(1-cosine_no_de) %>% as_tibble(.name_repair = "minimal")
colnames(mds_no_de) <- c("Dim1", "Dim2")

ggplot(mds_no_de, aes(x = Dim1, y = Dim2, label = word_subset)) + geom_text() + geom_point()
```

Some evidence of clustering with 2000 sentences, though it's clear the low frequency words are not as close to the other words in their category.

### Echo intensities for words with different frequencies

Due to RAM limitations, I had to reduce the vector length for the word environment vectors to 5000 with sparsity = 4. Also, I am only simulating with 100 subjects.

```{r}

# No discrepancy encoding: Function to calculate intensity for one participant (modified from Matt's)

sim_intensity_once_no_de <- function(strings, dict, vector_length, sparsity, sentence_with_ids, tau = 3) {
  env_vecs <- sl_create_riv(vector_length, length(dict), sparsity)
  
  sentence_mem <- matrix(0, ncol = ncol(env_vecs), nrow = length(sentence_with_ids)+4)
  sentence_mem[1:4, ] <- t(replicate(4, sample(c(-1,0,1), ncol(env_vecs), replace = TRUE)))
  
  reordered_sentences <- sample(sentence_with_ids)
  
for(i in 1:length(reordered_sentences)){
  current_sentence <- colSums(env_vecs[reordered_sentences[[i]], ])
  learning_vector <- sample(c(0,1), ncol(env_vecs), replace = TRUE, prob = c(1-l_value, l_value))
  sentence_mem[4+i, ] <- current_sentence*learning_vector
}
  
  reordered_strings <- sample(strings)
  trial_df <- data.frame(subject = NA, word = reordered_strings, intensity = NA)
  
  for (i in 1:length(reordered_strings)) {
    word_id <- which(dict %in% reordered_strings[i])
    probe <- env_vecs[word_id, ]
    intensity <- get_echo(probe, sentence_mem, tau = tau, output = "intensity", type = "cosine")
    trial_df$intensity[i] <- intensity
  }
  return(trial_df)
}
```

```{r}

n_of_sim <- 100 
full_subjects_df <- data.frame()

for(i in 1:n_of_sim){
  current_sub <- sim_intensity_once_no_de(word_subset, dictionary, 5000, 4, word_ids, tau = 3)
  current_sub$subject <- i
  full_subjects_df <- bind_rows(full_subjects_df, current_sub)
}
```

```{r}
science_df <- full_subjects_df %>% filter(word %in% c("astronomy", "scientific", "science", "scientists"))
finance_df <- full_subjects_df %>% filter(word %in% c("loaned", "save", "bank", "pay"))

ggplot(science_df, aes(x = intensity, color = word, group = word)) + geom_density()
ggplot(finance_df, aes(x = intensity, color = word, group = word)) + geom_density()
```

Running 100 simulations took slightly more than an hour.

The science cluster graph makes sense (word frequency: astronomy \< scientific \< science \< scientists), although the lack of a difference between tje science and scientists curves is interesting.

The finance cluster graph is somewhat unexpected (word frequency: loaned \< save, bank, pay), given that the bank curve is flatter and has a higher intensity than the pay curve.

## Word frequencies with MINERVA-AL discrepancy encoding (50 subjects only)

Given the long processing time with the no DE condition, only 50 subjects will be simulated for the DE conditions, as the discrepancy encoding steps add roughly 4 mins per subject to the total duration.

```{r}

# Setting up prior memory with discrepancy encoding (also applying L value)
memory_AL_de <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(word_ids)))

for(i in 1:length(word_ids)){
  current_sentence <- colSums(env_vectors[word_ids[[i]], ])
  echo <- get_echo(current_sentence, memory_AL_de, tau = 3, output = "echo", type = "cosine")
  echo_norm <- echo/max(abs(echo))
  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))
  memory_AL_de[4+i, ] <- (current_sentence - echo_norm) * learning_vector
}

# Get echoes (semantic meaning vectors)
meaning_AL_de <- subset_semantic_vectors(word_subset, dictionary, env_vectors, memory_AL_de, tau = 3)

cosine_AL_de <- cosine(t(meaning_AL_de))

corrplot::corrplot(cosine_AL_de,
                   method="circle",
                   order="hclust",
                   tl.col="black",
                   tl.cex=.7,
                   title="",
                   cl.cex=.6)
```

The science cluster is somewhat visible, but the finance cluster is seemingly non-existent.

```{r}
mds_AL_de <- cmdscale(1-cosine_AL_de) %>% as_tibble(.name_repair = "minimal")
colnames(mds_AL_de) <- c("Dim1", "Dim2")

ggplot(mds_AL_de, aes(x = Dim1, y = Dim2, label = word_subset)) + geom_text() + geom_point()
```

At least the clusters are still somewhat distinguishable, although this could also be because there are only 2 clusters.

```{r}
# Use MINERVA-AL discrepancy encoding: Function to calculate intensity for one participant (modified from Matt's)

sim_intensity_once_AL_de <- function(strings, dict, vector_length, sparsity, sentence_with_ids, tau = 3) {
  env_vecs <- sl_create_riv(vector_length, length(dict), sparsity)
  
  sentence_mem <- matrix(0, ncol = ncol(env_vecs), nrow = length(sentence_with_ids)+4)
  sentence_mem[1:4, ] <- t(replicate(4, sample(c(-1,0,1), ncol(env_vecs), replace = TRUE)))
  
  reordered_sentences <- sample(sentence_with_ids)
  
for(i in 1:length(reordered_sentences)){
  current_sentence <- colSums(env_vecs[reordered_sentences[[i]], ])
  echo <- get_echo(current_sentence, sentence_mem, tau = 3, output = "echo", type = "cosine")
  echo_norm <- echo/max(abs(echo))
  learning_vector <- sample(c(0,1), ncol(env_vecs), replace = TRUE, prob = c(1-l_value, l_value))
  sentence_mem[4+i, ] <- (current_sentence - echo_norm) *learning_vector
}
  
  reordered_strings <- sample(strings)
  trial_df <- data.frame(subject = NA, word = reordered_strings, intensity = NA)
  
  for (i in 1:length(reordered_strings)) {
    word_id <- which(dict %in% reordered_strings[i])
    probe <- env_vecs[word_id, ]
    intensity <- get_echo(probe, sentence_mem, tau = tau, output = "intensity", type = "cosine")
    trial_df$intensity[i] <- intensity
  }
  return(trial_df)
}

full_subjects_df_AL <- data.frame()

for(i in 1:50){
  current_sub <- sim_intensity_once_AL_de(word_subset, dictionary, 5000, 4, word_ids, tau = 3)
  current_sub$subject <- i
  full_subjects_df_AL <- bind_rows(full_subjects_df_AL, current_sub)
}
```

```{r}
science_df_AL <- full_subjects_df_AL %>% filter(word %in% c("astronomy", "scientific", "science", "scientists"))
finance_df_AL <- full_subjects_df_AL %>% filter(word %in% c("loaned", "save", "bank", "pay"))

ggplot(science_df_AL, aes(x = intensity, color = word, group = word)) + geom_density()
ggplot(finance_df_AL, aes(x = intensity, color = word, group = word)) + geom_density()
```

This took 4 hours! The graphs seem similar to those in the no DE condition. The anomaly of the bank curve having a higher mean and variance than the pay curve is seen here as well.

## Word frequencies with Collins et al. (2020) discrepancy encoding

Since this is faster than the MINERVA-AL condition, 100 subjects were simulated.

```{r}
l_max <- 1.0

# Setting up prior memory with discrepancy encoding (also applying L value)
memory_collins_de <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(word_ids)))

for(i in 1:length(word_ids)){
  current_sentence <- colSums(env_vectors[word_ids[[i]], ])
  current_intensity <- get_echo(current_sentence, memory_collins_de, tau = 3, output = "intensity", type = "cosine")
    l_collins <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))
  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_collins, l_collins))
  memory_collins_de[4+i, ] <- current_sentence * learning_vector
}

# Get echoes (semantic meaning vectors)
meaning_collins_de <- subset_semantic_vectors(word_subset, dictionary, env_vectors, memory_collins_de, tau = 3)

cosine_collins_de <- cosine(t(meaning_collins_de))

corrplot::corrplot(cosine_collins_de,
                   method="circle",
                   order="hclust",
                   tl.col="black",
                   tl.cex=.7,
                   title="",
                   cl.cex=.6)
```

The science words cluster well and the finance words cluster to a lesser degree.

```{r}
mds_collins_de <- cmdscale(1-cosine_collins_de) %>% as_tibble(.name_repair = "minimal")
colnames(mds_collins_de) <- c("Dim1", "Dim2")

ggplot(mds_collins_de, aes(x = Dim1, y = Dim2, label = word_subset)) + geom_text() + geom_point()
```

Discernible clusters, though this could once again be because there are only 2 clusters to differentiate.

```{r}
# Use Collins et al. (2020) discrepancy encoding: Function to calculate intensity for one participant (modified from Matt's)

sim_intensity_once_collins <- function(strings, dict, vector_length, sparsity, sentence_with_ids, tau = 3) {
  env_vecs <- sl_create_riv(vector_length, length(dict), sparsity)
  
  sentence_mem <- matrix(0, ncol = ncol(env_vecs), nrow = length(sentence_with_ids)+4)
  sentence_mem[1:4, ] <- t(replicate(4, sample(c(-1,0,1), ncol(env_vecs), replace = TRUE)))
  
  reordered_sentences <- sample(sentence_with_ids)
  
for(i in 1:length(reordered_sentences)){
  current_sentence <- colSums(env_vecs[reordered_sentences[[i]], ])
  current_intensity <- get_echo(current_sentence, sentence_mem, tau = 3, output = "intensity", type = "cosine")
  l_collins <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))
  learning_vector <- sample(c(0,1), ncol(env_vecs), replace = TRUE, prob = c(1-l_collins, l_collins))
  sentence_mem[4+i, ] <- current_sentence *learning_vector
}
  
  reordered_strings <- sample(strings)
  trial_df <- data.frame(subject = NA, word = reordered_strings, intensity = NA)
  
  for (i in 1:length(reordered_strings)) {
    word_id <- which(dict %in% reordered_strings[i])
    probe <- env_vecs[word_id, ]
    intensity <- get_echo(probe, sentence_mem, tau = tau, output = "intensity", type = "cosine")
    trial_df$intensity[i] <- intensity
  }
  return(trial_df)
}

full_subjects_df_collins <- data.frame()

for(i in 1:100){
  current_sub <- sim_intensity_once_collins(word_subset, dictionary, 5000, 4, word_ids, tau = 3)
  current_sub$subject <- i
  full_subjects_df_collins <- bind_rows(full_subjects_df_collins, current_sub)
}
```

That took slightly more than 2.5 hours.

```{r}
science_df_collins <- full_subjects_df_collins %>% filter(word %in% c("astronomy", "scientific", "science", "scientists"))
finance_df_collins <- full_subjects_df_collins %>% filter(word %in% c("loaned", "save", "bank", "pay"))

ggplot(science_df_collins, aes(x = intensity, color = word, group = word)) + geom_density()
ggplot(finance_df_collins, aes(x = intensity, color = word, group = word)) + geom_density()
```

Same patterns as in the other two conditions. Does discrepancy encoding not make a difference for frequency judgments?
