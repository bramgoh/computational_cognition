---
title: "Discrepancy Encoding in MINERVA (Part 1)"
author: "Bram Goh"
date: "2023-03-10"
categories: [code]
image: "intensity_no_de.png"
---

# Applying discrepancy encoding to frequency judgments

The goal of this exploration is to look into how the discrepancy encoding assumption introduced in MINERVA-AL (Jamieson et al., 2012) affects Hintzman's (1988) frequency judgment simulations.

```{r}
library(tidyverse)
set.seed(30)

# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)

get_activations <- function(probe, mem, type) {
  if(type == "hintzman"){
    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))
  }
  if(type == "cosine"){
    temp_activ <- as.numeric(RsemanticLibrarian::cosine_x_to_m(probe,mem))
    temp_activ[is.nan(temp_activ) == TRUE] <- 0
    return(temp_activ)
  }
}

# Generate echo (borrowed from Matt)
get_echo <- function(probe, mem, tau=3, output='intensity', type) {
    activations <- get_activations(probe, mem, type)
    if(output == "intensity"){
      return(sum(activations^tau))
    }
    if(output == "echo"){
      weighted_memory <- mem * (activations^tau)  
      summed_echo <- colSums(weighted_memory)
      return(summed_echo)
    }
    if(output == "both"){
      weighted_memory <- mem * (activations^tau)  
      summed_echo <- colSums(weighted_memory)
      model_output <- list(intensity = sum(activations^tau),
                           echo = summed_echo)
      return(model_output)
    }
    
}

# Item generation function (borrowed from Matt)

generate_item <- function(item_size=20,prob=c(1/3,1/3,1/3)){
  item <- sample(c(1,0,-1),
           size = item_size,
           replace = TRUE,
           prob = prob)
  return(item)
}

# Item matrix (original, before applying learning rate) function

gen_item_matrix <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3)) {
  item_matrix <- t(replicate(n = matrix_size, generate_item(item_size = item_size, prob = prob)))
  return(item_matrix)
}

# Form probe matrix i.e. item_matrix multiplied by respective frequencies + 4 more random items

gen_probes <- function(item_matrix, prob = c(1/3, 1/3, 1/3), max_num_of_copies = 5, num_of_traces_per_freq = 4) {
  freq_multiplier <- rep(1:max_num_of_copies, each = num_of_traces_per_freq)
  probe_matrix <- c()
  for(i in 1:length(freq_multiplier)) {
    current_rows <- matrix(rep(item_matrix[i, ], freq_multiplier[i]), nrow = freq_multiplier[i], ncol = ncol(item_matrix), byrow = TRUE)
    probe_matrix <- rbind(probe_matrix, current_rows)
}
  return(probe_matrix)
}

```

```{r}
# Overall simulation function (no discrepancy encoding)

sim_intensity_once <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {
  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)
  frequency_vec <- rep(c(1,2,3,4,5), each = 4)
  df_items <- data.frame(subject = NA, item = 1:nrow(item_matrix), frequency = frequency_vec, intensity = NA)
  
  trial_df <- data.frame()
  for(i in 1:nrow(df_items)){
    current_rows <- bind_rows(replicate(df_items[i, ]$frequency, df_items[i, ], simplify = FALSE))
    trial_df <- rbind(trial_df, current_rows)
  }
  trial_df <- trial_df %>% slice(sample(1:n()))

  # Starting state secondary memory
memory <- rbind(t(replicate(n = 4, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3)))), matrix(0, nrow = nrow(trial_df), ncol = item_size))

# Caluclating intensities and storing each probe
  for(i in 1:nrow(trial_df)){
    current_intensity <- get_echo(item_matrix[trial_df[i, ]$item, ], memory, output = "intensity", type = "cosine")
    trial_df[i, ]$intensity <- current_intensity
    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))
    learned_probe <- item_matrix[trial_df[i, ]$item, ] * learning_filter
    memory[4 + i, ] <- learned_probe
  }
return(trial_df)
}

sim_intensity_multiple <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {
  all_subs_df <- data.frame()
  for(s in 1:n_of_sim) {
    sub_df <- sim_intensity_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)
    sub_df$subject <- s
    all_subs_df <- rbind(all_subs_df, sub_df)
  }
  return(all_subs_df)
}
```

```{r}
raw_df_no_de <- sim_intensity_multiple(1000)

subject_sum <- raw_df_no_de %>% group_by(subject, frequency) %>% summarize(mean_int = mean(intensity))
ggplot(subject_sum, aes(x = mean_int, group = frequency, color = as.factor(frequency))) + geom_density()
```

We see a similar overall shape to that found in Hintzman (1988), with the amplitudes decreasing as frequency increases. However, the variance does not increase with increasing frequency. The details are different, probably because the setup here is different from Hintzman (1988): Hintzman had all the traces pre-stored in memory, while here we are adding them to memory one after the other as they are presented.

Also, for this "learning" paradigm where input traces are stored in memory, should we only save and plot the intensity value the final time a probe is presented? For example, if the probe has frequency = 4, frequency will only equal 4 when the probe is presented for the 5th time. The first four times, the frequency will be 0 - 3 respectively.

```{r}

# Overall simulation function (with discrepancy encoding)

sim_intensity_once_de <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {
  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)
  frequency_vec <- rep(c(1,2,3,4,5), each = 4)
  df_items <- data.frame(subject = NA, item = 1:nrow(item_matrix), frequency = frequency_vec, intensity = NA)
  
  trial_df <- data.frame()
  for(i in 1:nrow(df_items)){
    current_rows <- bind_rows(replicate(df_items[i, ]$frequency, df_items[i, ], simplify = FALSE))
    trial_df <- rbind(trial_df, current_rows)
  }
  trial_df <- trial_df %>% slice(sample(1:n()))

  # Starting state secondary memory
memory <- rbind(t(replicate(n = 4, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3)))), matrix(0, nrow = nrow(trial_df), ncol = item_size))

# Caluclating intensities and storing each probe
  for(i in 1:nrow(trial_df)){
    current_output <- get_echo(item_matrix[trial_df[i, ]$item, ], memory, output = "both", type = "cosine")
    trial_df[i, ]$intensity <- current_output[[1]]
    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))
    norm_echo <- current_output[[2]]/max(abs(current_output[[2]]))
    discrep <- item_matrix[trial_df[i, ]$item, ] - norm_echo
    learned_trace <- discrep * learning_filter
    memory[4 + i, ] <- learned_trace
  }
return(trial_df)
}

sim_intensity_multiple_de <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {
  all_subs_df <- data.frame()
  for(s in 1:n_of_sim) {
    sub_df <- sim_intensity_once_de(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)
    sub_df$subject <- s
    all_subs_df <- rbind(all_subs_df, sub_df)
  }
  return(all_subs_df)
}
```

```{r}
raw_dfs_with_de <- sim_intensity_multiple_de(1000)

subject_sum_de <- raw_dfs_with_de %>% group_by(subject, frequency) %>% summarize(mean_int = mean(intensity))
ggplot(subject_sum_de, aes(x = mean_int, group = frequency, color = as.factor(frequency))) + geom_density()
```

These results don't look too different from the no discrepancy encoding graphs, except the curves are squashed toward zero, resulting in lower means and smaller differences between the curves.

```{r}
# Overall simulation function (with Collins et al. 2020 discrepancy encoding)

sim_intensity_once_collins <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_max = 1.0, max_num_of_copies = 5, num_of_traces_per_freq = 4) {
  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)
  frequency_vec <- rep(c(1,2,3,4,5), each = 4)
  df_items <- data.frame(subject = NA, item = 1:nrow(item_matrix), frequency = frequency_vec, intensity = NA)
  
  trial_df <- data.frame()
  for(i in 1:nrow(df_items)){
    current_rows <- bind_rows(replicate(df_items[i, ]$frequency, df_items[i, ], simplify = FALSE))
    trial_df <- rbind(trial_df, current_rows)
  }
  trial_df <- trial_df %>% slice(sample(1:n()))

  # Starting state secondary memory
memory <- rbind(t(replicate(n = 4, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3)))), matrix(0, nrow = nrow(trial_df), ncol = item_size))

# Caluclating intensities and storing each probe
  for(i in 1:nrow(trial_df)){
    current_output <- get_echo(item_matrix[trial_df[i, ]$item, ], memory, output = "both", type = "cosine")
    trial_df[i, ]$intensity <- current_output[[1]]
    l_value <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_output[[1]] + 2)))
    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))
    learned_trace <- item_matrix[trial_df[i, ]$item, ] * learning_filter
    memory[4 + i, ] <- learned_trace
  }
return(trial_df)
}

sim_intensity_multiple_collins <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_max = 1.0, max_num_of_copies = 5, num_of_traces_per_freq = 4) {
  all_subs_df <- data.frame()
  for(s in 1:n_of_sim) {
    sub_df <- sim_intensity_once_collins(matrix_size = matrix_size, item_size = item_size, prob = prob, l_max = l_max, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)
    sub_df$subject <- s
    all_subs_df <- rbind(all_subs_df, sub_df)
  }
  return(all_subs_df)
}
```

```{r}
raw_df_collins <- sim_intensity_multiple_collins(1000)

subject_sum_collins <- raw_df_collins %>% group_by(subject, frequency) %>% summarize(mean_int = mean(intensity))
ggplot(subject_sum_collins, aes(x = mean_int, group = frequency, color = as.factor(frequency))) + geom_density()
```

This is interesting. The pattern for frequency = 2 to 5 is similar, although the difference between curves seems to decrease as frequency increases, as Matt suggested would happen as frequency increases beyond 5. The frequency = 1 curve being centered at 0 makes sense for this encoding formula, because I is very small due to there being no similar traces in memory.
