{
  "hash": "29d321997716cbc4b0d236664808b3bf",
  "result": {
    "markdown": "---\ntitle: \"ITS (part 1)\"\nauthor: \"Bram Goh\"\ndate: \"2023-03-20\"\ncategories: [code]\nimage: \"\"\n---\n\n\n# Getting familiar with ITS\n\nI couldn't get the LSAfun package to work (something to do with rgl). Here is my own crude attempt at modeling ITS:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RsemanticLibrarian)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'data.table'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    transpose\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(RsemanticLibrarian::cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Function to replace words with their dictionary ids\nreplace_word_with_id <- function(sentence_df, dict_df, max_sent_length){\n  sentence_id_matrix <- c()\n  for(i in 1:length(sentence_df$sentence)){\n    current_sent <- rep(0, max_sent_length)\n    words_list <- tokenize_words(sentence_df$sentence[i])\n    words <- words_list[[1]]\n    for(j in 1:length(words)){\n      current_id <- which(dict_df$word == words[j])\n      if(length(current_id) == 0){\n        current_id <- 0\n      }\n      current_sent[j] <- current_id\n    }\n    sentence_id_matrix <- rbind(sentence_id_matrix, current_sent)\n  }\n  return(sentence_id_matrix)\n}\n\n# Function to generate random environment vectors for each word in dictionary\nmake_env_vectors <- function(dict_df, length, sparsity) {\n  environment_matrix <- matrix(0, nrow = nrow(dict_df), ncol = length)\n  for(i in 1:nrow(dict_df)){\n    current <- rep(0, length)\n    positions <- sample(1:length, sparsity)\n    for(j in 1:length(positions)){\n      current[positions[j]] <- sample(c(1, -1), 1)\n    }\n    environment_matrix[i, ] <- current\n  }\n  return(environment_matrix)\n}\n\n# Function to add up environment vectors for all words in a sentence\nmake_sentence_vectors <- function(compiled_word_ids, env_matrix) {\n  sentence_ids <- matrix(0, nrow = nrow(compiled_word_ids), ncol = ncol(env_matrix))\n  for(i in 1:nrow(compiled_word_ids)){\n    current_word_ids <- compiled_word_ids[i, ]\n    current_sentence <- rep(0, ncol(env_matrix))\n    for(j in 1:length(current_word_ids)){\n      if(current_word_ids[j] == 0){\n        temp_vector <- rep(0, ncol(env_matrix))\n      } else {\n      temp_vector <- env_matrix[current_word_ids[j], ]\n      }\n      current_sentence <- current_sentence + temp_vector\n  }\n  sentence_ids[i, ] <- current_sentence\n  }\n  return(sentence_ids)\n}\n\n# Function to generate semantic meaning vectors for each word\nmake_meaning_vectors <- function(env_matrix, sentence_memory){\n  meaning_matrix <- matrix(0, nrow = nrow(env_matrix), ncol = ncol(env_matrix))\n  for(i in 1:nrow(env_matrix)){\n    meaning_matrix[i, ] <- get_echo(env_matrix[i, ], sentence_memory, output = \"echo\", type = \"cosine\")\n  }\n  return(meaning_matrix)\n}\n```\n:::\n\n\n## Attempt 1: TASA corpus\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pre-processing\nTASA_full <- read.table(\"tasaDocsPara.txt\", \n                          sep=\"\\t\", \n                          fill=FALSE, \n                          strip.white=TRUE)\ntraining_TASA <- data.frame(sentence = TASA_full$V1[1:2000])\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nTASA_dict <- training_TASA %>% unnest_tokens(output = \"word\", input = sentence, token = \"words\") %>% unique() %>% arrange(word)\n\nTASA_ids <- replace_word_with_id(training_TASA, TASA_dict, 200)\n\nenv_vectors_TASA <- make_env_vectors(TASA_dict, 100, 6)\n\nsent_vectors_TASA <- make_sentence_vectors(TASA_ids, env_vectors_TASA)\n\nmemory_TASA <- sent_vectors_TASA\n\nmeaning_mat_TASA <- make_meaning_vectors(env_vectors_TASA, memory_TASA)\n\nsimilarities_TASA <- cosine_x_to_m(meaning_mat_TASA[which(TASA_dict$word == \"american\"), ], meaning_mat_TASA)\nsim_df_TASA <- data.frame(TASA_dict, similarities = similarities_TASA)\nsim_top_TASA <- sim_df_TASA %>% arrange(desc(similarities)) %>% slice(1:10)\nsim_top_TASA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               word similarities\n1          american    1.0000000\n2       newburyport    0.7846028\n3              gave    0.6849152\n4          peterson    0.6682375\n5          unrolled    0.6531566\n6         syntheses    0.6504689\n7  unresponsiveness    0.6485559\n8      irrespective    0.6399367\n9           chicago    0.6387013\n10            towed    0.6175876\n```\n:::\n:::\n\n\nThe meaning vectors do not seem to reflect actual semantic meaning. It could be that my makeshift code is flawed. This could also be because the TASA corpus is arranged in paragraphs, rather than sentences.\n\n## Attempt 2: Stanford Natural Language Inference (SNLI) corpus\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pre-processing with Stanford sentences\ncorpus <- read_tsv(\"snli_1.0_train.txt\", col_names = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 550152 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (14): gold_label, sentence1_binary_parse, sentence2_binary_parse, senten...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncorpus_short <- corpus[ , 6]\ncorpus_sent <- unique(corpus_short)\ncorpus_sent_clean <- tolower(corpus_sent$sentence1)\ncorpus_sent_clean <- removeWords(corpus_sent_clean, stopwords(\"en\"))\ncorpus_sent_clean <- gsub(\"[[:punct:]]\", \" \", corpus_sent_clean)\ncorpus_sent_clean <- gsub(\"[[:digit:]]+\", \" \", corpus_sent_clean)\ncorpus_sent_clean <- gsub(\"\\\\s+\", \" \", corpus_sent_clean)\ncorpus_sent_clean <- data.frame(sentence = (trimws(corpus_sent_clean)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npartial_corpus <- corpus_sent_clean %>% slice(1:20000)\n\n# Make dictionary\ndictionary <- partial_corpus %>% unnest_tokens(output = \"word\", input = sentence, token = \"words\") %>% unique() %>% arrange(word)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_ids <- replace_word_with_id(partial_corpus, dictionary, 50)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nenv_vectors <- make_env_vectors(dictionary, 100, 6)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsentence_vectors <- make_sentence_vectors(corpus_ids, env_vectors)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmemory <- sentence_vectors\nmeaning_matrix <- make_meaning_vectors(env_vectors, memory)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimilarities <- cosine_x_to_m(meaning_matrix[which(dictionary$word == \"boy\"), ], meaning_matrix)\nsim_df <- data.frame(dictionary, similarities)\nsim_df_top <- sim_df %>% arrange(desc(similarities)) %>% slice(1:10)\nsim_df_top\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          word similarities\n1          boy    1.0000000\n2          ink    0.7270636\n3        early    0.7151881\n4         neat    0.6916188\n5    daffodils    0.6882416\n6      tribals    0.6738056\n7      blanket    0.6598177\n8  environment    0.6423749\n9         lots    0.6382897\n10       soars    0.6378569\n```\n:::\n:::\n\n\nThe words that are ostensibly semantically similar really aren't at all. Perhaps the sentences in the corpus are unrelated and don't form a coherent narrative. Below is an attempt using The Great Gatsby.\n\n## Attempt 3: The Great Gatsby\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gutenbergr)\nraw_gatsby <- gutenberg_download(64317)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDetermining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing mirror http://aleph.gutenberg.org\n```\n:::\n\n```{.r .cell-code}\ngatsby_text <- raw_gatsby %>% slice(31:6396) %>% select(-gutenberg_id)\ngatsby_pasted <- paste(gatsby_text$text, collapse = \" \")\ngatsby_sentences <- tokenize_sentences(gatsby_pasted, lowercase = TRUE, simplify = TRUE)\ngatsby_clean <- removeWords(gatsby_sentences, c(stopwords(\"en\"), \"ve\", \"t\", \"d\"))\ngatsby_clean <- gsub(\"[[:punct:]]\", \" \", gatsby_clean)\ngatsby_clean <- gsub(\"[[:digit:]]+\", \" \", gatsby_clean)\ngatsby_clean <- gsub(\"\\\\s+\", \" \", gatsby_clean)\ngatsby_clean_df <- data.frame(sentence = (trimws(gatsby_clean)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndictionary_gat <- gatsby_clean_df %>% unnest_tokens(output = \"word\", input = sentence, token = \"words\") %>% unique() %>% arrange(word)\ncomp_word_ids_gat <- replace_word_with_id(gatsby_clean_df, dictionary_gat, 50)\nenv_vectors_gat <- make_env_vectors(dictionary_gat, 100, 6)\nsent_vectors_gat <- make_sentence_vectors(comp_word_ids_gat, env_vectors_gat)\n\nmemory_gat <- sent_vectors_gat\nmeaning_mat_gat <- make_meaning_vectors(env_vectors_gat, memory_gat)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_gat <- cosine_x_to_m(meaning_mat_gat[which(dictionary_gat$word == \"party\"), ], meaning_mat_gat)\nsim_gat_df <- data.frame(dictionary_gat, similarity = sim_gat)\nsim_gat_top <- sim_gat_df %>% arrange(desc(similarity)) %>% slice(1:10)\nsim_gat_top\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        word similarity\n1      party  1.0000000\n2     accept  0.5097045\n3    parlour  0.4926566\n4  incurably  0.4703411\n5     gaiety  0.4550898\n6     bustle  0.4487545\n7       look  0.4322114\n8  testimony  0.4148566\n9  prolonged  0.4011821\n10    listen  0.3972736\n```\n:::\n:::\n\n\nSlightly better, but most words are still quite unrelated. Perhaps a longer novel with more sentences would work better?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}