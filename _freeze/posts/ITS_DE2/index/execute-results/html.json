{
  "hash": "5de3aa6c69095b73fe38127764a937fc",
  "result": {
    "markdown": "---\ntitle: \"Exploring discrepancy encoding with ITS (Part 1)\"\nauthor: \"Bram Goh\"\ndate: \"2023-03-28\"\ncategories: [code]\nimage: \"\"\n---\n\n\n# Implementing discrepancy encoding with TASA\n\nGoals for this exercise:\n\n-   Replicate MDS solution for TASA corpus as in Jamieson et al. (2018)\n\n-   Identify words with varying frequencies and plot intensity graphs as per Hintzman (1988)\n\n-   Repeat while applying discrepancy encoding\n\n## Word frequencies with no discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RsemanticLibrarian)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(lsa)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: SnowballC\n```\n:::\n\n```{.r .cell-code}\nTASA_full <- read.table(\"tasaDocsPara.txt\", \n                          sep=\"\\t\", \n                          fill=FALSE, \n                          strip.white=TRUE)\n\ntraining <- TASA_full$V1[1:2000]\n\nl_value <- 0.7\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Create dictionary (modified from RsemanticLibrarian)\n\ncorpus_dictionary <- function(words){\n  neat_words <- words %>%\n                as.character() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                qdapRegex::rm_white_lead_trail() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                unique()\n  return(neat_words)\n}\n\n# Functions to ensure strings only contain words (modified from RsemanticLibrarian)\nclean <- function(words){\n  if(length(words) == 1) {\n    neat_words <- words %>%\n      as.character() %>%\n      strsplit(split=\" \") %>%\n      unlist() %>%\n      qdapRegex::rm_white_lead_trail() %>%\n      strsplit(split=\" \") %>%\n      unlist() \n    return(neat_words)\n  }\n}\n\nclean_vector <- function(words){\n  return(lapply(unlist(strsplit(words,split=\"[.]\")), clean))\n}\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    echo <- get_echo(probe, sentence_memory, tau = tau, output = \"echo\", type = \"cosine\")\n    word_meaning[i, ] <- echo\n  }\n  row.names(word_meaning) <- strings\n  \n  return(word_meaning)\n}\n\n# Find top n similar words in terms of semantic meaning\nfind_similar_words <- function(meaning_matrix, dictionary, word_to_find, n) {\n  similarities <- cosine_x_to_m(meaning_matrix[which(dictionary$word == word_to_find), ], meaning_matrix)\nsimilarities_df <- data.frame(dictionary, similarities = similarities)\nsim_top <- similarities_df %>% arrange(desc(similarities)) %>% slice(1:n)\nreturn(sim_top)\n}\n```\n:::\n\n\n### Pre-processing TASA corpus (with 2000 sentences) and MDS solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndictionary <- corpus_dictionary(training)\n\nclean_sentences <- clean_vector(training)\n\nword_ids <- sl_word_ids_sentences(clean_sentences, dictionary)\n\nenv_vectors <- sl_create_riv(5000, length(dictionary), 4)\n\n# Setting up prior memory without discrepancy encoding (also applying L value)\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(env_vectors), replace = TRUE)))\nmemory_no_de <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(word_ids)))\n\nfor(i in 1:length(word_ids)){\n  current_sentence <- colSums(env_vectors[word_ids[[i]], ])\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))\n  memory_no_de[4+i, ] <- current_sentence*learning_vector\n}\n\n# Identifying words with different frequencies\n\nJonesMewhort_Figure3 <- c(\"financial\",\"savings\",\"finance\",\"pay\",\"invested\",\n                          \"loaned\",\"borrow\",\"lend\",\"invest\",\"investments\",\n                          \"bank\",\"spend\",\"save\",\"astronomy\",\"physics\",\n                          \"chemistry\",\"psychology\",\"biology\",\"scientific\",\n                          \"mathematics\",\"technology\",\"scientists\",\"science\",\n                          \"research\",\"sports\",\"team\",\"teams\",\"football\",\n                          \"coach\",\"sport\",\"players\",\"baseball\",\"soccer\",\n                          \"tennis\",\"basketball\")\n\nword_freq_table <- clean_sentences %>% unlist() %>% table() %>% as_tibble() %>% filter(. %in% JonesMewhort_Figure3) %>% arrange(n)\n\nword_subset <- c(\"astronomy\", \"loaned\", \"scientific\", \"save\", \"science\", \"bank\", \"scientists\", \"pay\")\n\n# Get echoes (semantic meaning vectors)\nmeaning_no_de <- subset_semantic_vectors(word_subset, dictionary, env_vectors, memory_no_de, tau = 3)\n\ncosine_no_de <- cosine(t(meaning_no_de))\n\ncorrplot::corrplot(cosine_no_de,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmds_no_de <- cmdscale(1-cosine_no_de) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_no_de) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_no_de, aes(x = Dim1, y = Dim2, label = word_subset)) + geom_text() + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nSome evidence of clustering with 2000 sentences, though it's clear the low frequency words are not as close to the other words in their category.\n\n### Echo intensities for words with different frequencies\n\nDue to RAM limitations, I had to reduce the vector length for the word environment vectors to 5000 with sparsity = 4. Also, I am only simulating with 100 subjects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# No discrepancy encoding: Function to calculate intensity for one participant (modified from Matt's)\n\nsim_intensity_once_no_de <- function(strings, dict, vector_length, sparsity, sentence_with_ids, tau = 3) {\n  env_vecs <- sl_create_riv(vector_length, length(dict), sparsity)\n  \n  sentence_mem <- matrix(0, ncol = ncol(env_vecs), nrow = length(sentence_with_ids)+4)\n  sentence_mem[1:4, ] <- t(replicate(4, sample(c(-1,0,1), ncol(env_vecs), replace = TRUE)))\n  \n  reordered_sentences <- sample(sentence_with_ids)\n  \nfor(i in 1:length(reordered_sentences)){\n  current_sentence <- colSums(env_vecs[reordered_sentences[[i]], ])\n  learning_vector <- sample(c(0,1), ncol(env_vecs), replace = TRUE, prob = c(1-l_value, l_value))\n  sentence_mem[4+i, ] <- current_sentence*learning_vector\n}\n  \n  reordered_strings <- sample(strings)\n  trial_df <- data.frame(subject = NA, word = reordered_strings, intensity = NA)\n  \n  for (i in 1:length(reordered_strings)) {\n    word_id <- which(dict %in% reordered_strings[i])\n    probe <- env_vecs[word_id, ]\n    intensity <- get_echo(probe, sentence_mem, tau = tau, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- intensity\n  }\n  return(trial_df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn_of_sim <- 100 \nfull_subjects_df <- data.frame()\n\nfor(i in 1:n_of_sim){\n  current_sub <- sim_intensity_once_no_de(word_subset, dictionary, 5000, 4, word_ids, tau = 3)\n  current_sub$subject <- i\n  full_subjects_df <- bind_rows(full_subjects_df, current_sub)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nscience_df <- full_subjects_df %>% filter(word %in% c(\"astronomy\", \"scientific\", \"science\", \"scientists\"))\nfinance_df <- full_subjects_df %>% filter(word %in% c(\"loaned\", \"save\", \"bank\", \"pay\"))\n\nggplot(science_df, aes(x = intensity, color = word, group = word)) + geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(finance_df, aes(x = intensity, color = word, group = word)) + geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\nRunning 100 simulations took slightly more than an hour.\n\nThe science cluster graph makes sense (word frequency: astronomy \\< scientific \\< science \\< scientists), although the lack of a difference between tje science and scientists curves is interesting.\n\nThe finance cluster graph is somewhat unexpected (word frequency: loaned \\< save, bank, pay), given that the bank curve is flatter and has a higher intensity than the pay curve.\n\n## Word frequencies with MINERVA-AL discrepancy encoding (50 subjects only)\n\nGiven the long processing time with the no DE condition, only 50 subjects will be simulated for the DE conditions, as the discrepancy encoding steps add roughly 4 mins per subject to the total duration.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setting up prior memory with discrepancy encoding (also applying L value)\nmemory_AL_de <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(word_ids)))\n\nfor(i in 1:length(word_ids)){\n  current_sentence <- colSums(env_vectors[word_ids[[i]], ])\n  echo <- get_echo(current_sentence, memory_AL_de, tau = 3, output = \"echo\", type = \"cosine\")\n  echo_norm <- echo/max(abs(echo))\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))\n  memory_AL_de[4+i, ] <- (current_sentence - echo_norm) * learning_vector\n}\n\n# Get echoes (semantic meaning vectors)\nmeaning_AL_de <- subset_semantic_vectors(word_subset, dictionary, env_vectors, memory_AL_de, tau = 3)\n\ncosine_AL_de <- cosine(t(meaning_AL_de))\n\ncorrplot::corrplot(cosine_AL_de,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe science cluster is somewhat visible, but the finance cluster is seemingly non-existent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmds_AL_de <- cmdscale(1-cosine_AL_de) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_AL_de) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_AL_de, aes(x = Dim1, y = Dim2, label = word_subset)) + geom_text() + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nAt least the clusters are still somewhat distinguishable, although this could also be because there are only 2 clusters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use MINERVA-AL discrepancy encoding: Function to calculate intensity for one participant (modified from Matt's)\n\nsim_intensity_once_AL_de <- function(strings, dict, vector_length, sparsity, sentence_with_ids, tau = 3) {\n  env_vecs <- sl_create_riv(vector_length, length(dict), sparsity)\n  \n  sentence_mem <- matrix(0, ncol = ncol(env_vecs), nrow = length(sentence_with_ids)+4)\n  sentence_mem[1:4, ] <- t(replicate(4, sample(c(-1,0,1), ncol(env_vecs), replace = TRUE)))\n  \n  reordered_sentences <- sample(sentence_with_ids)\n  \nfor(i in 1:length(reordered_sentences)){\n  current_sentence <- colSums(env_vecs[reordered_sentences[[i]], ])\n  echo <- get_echo(current_sentence, sentence_mem, tau = 3, output = \"echo\", type = \"cosine\")\n  echo_norm <- echo/max(abs(echo))\n  learning_vector <- sample(c(0,1), ncol(env_vecs), replace = TRUE, prob = c(1-l_value, l_value))\n  sentence_mem[4+i, ] <- (current_sentence - echo_norm) *learning_vector\n}\n  \n  reordered_strings <- sample(strings)\n  trial_df <- data.frame(subject = NA, word = reordered_strings, intensity = NA)\n  \n  for (i in 1:length(reordered_strings)) {\n    word_id <- which(dict %in% reordered_strings[i])\n    probe <- env_vecs[word_id, ]\n    intensity <- get_echo(probe, sentence_mem, tau = tau, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- intensity\n  }\n  return(trial_df)\n}\n\nfull_subjects_df_AL <- data.frame()\n\nfor(i in 1:50){\n  current_sub <- sim_intensity_once_AL_de(word_subset, dictionary, 5000, 4, word_ids, tau = 3)\n  current_sub$subject <- i\n  full_subjects_df_AL <- bind_rows(full_subjects_df_AL, current_sub)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nscience_df_AL <- full_subjects_df_AL %>% filter(word %in% c(\"astronomy\", \"scientific\", \"science\", \"scientists\"))\nfinance_df_AL <- full_subjects_df_AL %>% filter(word %in% c(\"loaned\", \"save\", \"bank\", \"pay\"))\n\nggplot(science_df_AL, aes(x = intensity, color = word, group = word)) + geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(finance_df_AL, aes(x = intensity, color = word, group = word)) + geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\nThis took 4 hours! The graphs seem similar to those in the no DE condition. The anomaly of the bank curve having a higher mean and variance than the pay curve is seen here as well.\n\n## Word frequencies with Collins et al. (2020) discrepancy encoding\n\nSince this is faster than the MINERVA-AL condition, 100 subjects were simulated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl_max <- 1.0\n\n# Setting up prior memory with discrepancy encoding (also applying L value)\nmemory_collins_de <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(word_ids)))\n\nfor(i in 1:length(word_ids)){\n  current_sentence <- colSums(env_vectors[word_ids[[i]], ])\n  current_intensity <- get_echo(current_sentence, memory_collins_de, tau = 3, output = \"intensity\", type = \"cosine\")\n    l_collins <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_collins, l_collins))\n  memory_collins_de[4+i, ] <- current_sentence * learning_vector\n}\n\n# Get echoes (semantic meaning vectors)\nmeaning_collins_de <- subset_semantic_vectors(word_subset, dictionary, env_vectors, memory_collins_de, tau = 3)\n\ncosine_collins_de <- cosine(t(meaning_collins_de))\n\ncorrplot::corrplot(cosine_collins_de,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThe science words cluster well and the finance words cluster to a lesser degree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmds_collins_de <- cmdscale(1-cosine_collins_de) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_collins_de) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_collins_de, aes(x = Dim1, y = Dim2, label = word_subset)) + geom_text() + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nDiscernible clusters, though this could once again be because there are only 2 clusters to differentiate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use Collins et al. (2020) discrepancy encoding: Function to calculate intensity for one participant (modified from Matt's)\n\nsim_intensity_once_collins <- function(strings, dict, vector_length, sparsity, sentence_with_ids, tau = 3) {\n  env_vecs <- sl_create_riv(vector_length, length(dict), sparsity)\n  \n  sentence_mem <- matrix(0, ncol = ncol(env_vecs), nrow = length(sentence_with_ids)+4)\n  sentence_mem[1:4, ] <- t(replicate(4, sample(c(-1,0,1), ncol(env_vecs), replace = TRUE)))\n  \n  reordered_sentences <- sample(sentence_with_ids)\n  \nfor(i in 1:length(reordered_sentences)){\n  current_sentence <- colSums(env_vecs[reordered_sentences[[i]], ])\n  current_intensity <- get_echo(current_sentence, sentence_mem, tau = 3, output = \"intensity\", type = \"cosine\")\n  l_collins <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))\n  learning_vector <- sample(c(0,1), ncol(env_vecs), replace = TRUE, prob = c(1-l_collins, l_collins))\n  sentence_mem[4+i, ] <- current_sentence *learning_vector\n}\n  \n  reordered_strings <- sample(strings)\n  trial_df <- data.frame(subject = NA, word = reordered_strings, intensity = NA)\n  \n  for (i in 1:length(reordered_strings)) {\n    word_id <- which(dict %in% reordered_strings[i])\n    probe <- env_vecs[word_id, ]\n    intensity <- get_echo(probe, sentence_mem, tau = tau, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- intensity\n  }\n  return(trial_df)\n}\n\nfull_subjects_df_collins <- data.frame()\n\nfor(i in 1:100){\n  current_sub <- sim_intensity_once_collins(word_subset, dictionary, 5000, 4, word_ids, tau = 3)\n  current_sub$subject <- i\n  full_subjects_df_collins <- bind_rows(full_subjects_df_collins, current_sub)\n}\n```\n:::\n\n\nThat took slightly more than 2.5 hours.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscience_df_collins <- full_subjects_df_collins %>% filter(word %in% c(\"astronomy\", \"scientific\", \"science\", \"scientists\"))\nfinance_df_collins <- full_subjects_df_collins %>% filter(word %in% c(\"loaned\", \"save\", \"bank\", \"pay\"))\n\nggplot(science_df_collins, aes(x = intensity, color = word, group = word)) + geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(finance_df_collins, aes(x = intensity, color = word, group = word)) + geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n:::\n\n\nSame patterns as in the other two conditions. Does discrepancy encoding not make a difference for frequency judgments?\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}