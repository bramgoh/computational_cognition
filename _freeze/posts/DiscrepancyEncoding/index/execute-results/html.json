{
  "hash": "98425a6b55f31380327105cf9d146c9b",
  "result": {
    "markdown": "---\ntitle: \"Discrepancy Encoding in MINERVA (Part 1)\"\nauthor: \"Bram Goh\"\ndate: \"2023-03-21\"\ncategories: [code]\nimage: \"intensity_no_de.png\"\n---\n\n\n# Applying discrepancy encoding to frequency judgments\n\nThe goal of this exploration is to look into how the discrepancy encoding assumption introduced in MINERVA-AL (Jamieson et al., 2012) affects Hintzman's (1988) frequency judgment simulations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nset.seed(30)\n\nmax_frequency <- 5\nnum_of_item_per_freq <- 4\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(RsemanticLibrarian::cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Item generation function (borrowed from Matt)\n\ngenerate_item <- function(item_size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = item_size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# Item matrix (original, before applying learning rate) function\n\ngen_item_matrix <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3)) {\n  item_matrix <- t(replicate(n = matrix_size, generate_item(item_size = item_size, prob = prob)))\n  return(item_matrix)\n}\n\n# Form probe matrix i.e. item_matrix multiplied by respective frequencies + 4 more random items\n\ngen_probes <- function(item_matrix, prob = c(1/3, 1/3, 1/3), max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  freq_multiplier <- rep(1:max_num_of_copies, each = num_of_traces_per_freq)\n  probe_matrix <- c()\n  for(i in 1:length(freq_multiplier)) {\n    current_rows <- matrix(rep(item_matrix[i, ], freq_multiplier[i]), nrow = freq_multiplier[i], ncol = ncol(item_matrix), byrow = TRUE)\n    probe_matrix <- rbind(probe_matrix, current_rows)\n}\n  return(probe_matrix)\n}\n\n# Vector of frequencies for each item\nfrequency_vec <- rep(c(1:max_frequency), each = num_of_item_per_freq)\n\n# Rows to extract (with experienced frequency = designated frequency)\nimportant_rows <- numeric(length = max_frequency*num_of_item_per_freq)\ntracker <- 0\n  for(h in 1:length(frequency_vec)){\n    current_value <- tracker + frequency_vec[h]\n    important_rows[h] <- current_value\n    tracker <- current_value\n  }\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall simulation function (no discrepancy encoding)\n\nsim_intensity_once <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  df_items <- data.frame(subject = NA, item = 1:nrow(item_matrix), frequency = frequency_vec)\n  \n  trial_df <- data.frame()\n  for(i in 1:nrow(df_items)){\n    current_rows <- bind_rows(replicate(df_items[i, ]$frequency, df_items[i, ], simplify = FALSE))\n    trial_df <- rbind(trial_df, current_rows)\n  }\n  trial_df <- trial_df %>% slice(sample(1:n()))\n  \n  # Starting state secondary memory\nmemory <- rbind(t(replicate(n = 4, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3)))), matrix(0, nrow = nrow(trial_df), ncol = item_size))\n\n# Caluclating intensities and storing each probe\n  for(i in 1:nrow(trial_df)){\n    current_intensity <- get_echo(item_matrix[trial_df[i, ]$item, ], memory, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- current_intensity\n    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))\n    learned_probe <- item_matrix[trial_df[i, ]$item, ] * learning_filter\n    memory[4 + i, ] <- learned_probe\n  }\ntrial_arr_slice <- trial_df %>% arrange(item) %>% slice(important_rows)\nreturn(trial_arr_slice)\n}\n\nsim_intensity_multiple <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  all_subs_df <- data.frame()\n  for(s in 1:n_of_sim) {\n    sub_df <- sim_intensity_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq)\n    sub_df$subject <- s\n    all_subs_df <- rbind(all_subs_df, sub_df)\n  }\n  return(all_subs_df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_df_no_de <- sim_intensity_multiple(1000)\n\nsubject_sum <- raw_df_no_de %>% group_by(subject, frequency) %>% summarize(mean_int = mean(intensity))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\nggplot(subject_sum, aes(x = mean_int, group = frequency, color = as.factor(frequency))) + geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe see a similar overall shape to that found in Hintzman (1988), with the amplitudes decreasing and the variance increasing as frequency increases. The actual values are higher than those reported in Hintzman (1988), probably because the setup here is different? Hintzman had all the traces pre-stored in memory, while here we are adding them to memory one after the other as they are presented. I'm not sure why the differences would be this large though.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall simulation function (with discrepancy encoding)\n\nsim_intensity_once_de <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  df_items <- data.frame(subject = NA, item = 1:nrow(item_matrix), frequency = frequency_vec, intensity = NA)\n\n# Setting up trial dataframe with randomized trial order\n  trial_df <- data.frame()\n  for(i in 1:nrow(df_items)){\n    current_rows <- bind_rows(replicate(df_items[i, ]$frequency, df_items[i, ], simplify = FALSE))\n    trial_df <- rbind(trial_df, current_rows)\n  }\n  trial_df <- trial_df %>% slice(sample(1:n()))\n\n  # Starting state secondary memory\nmemory <- rbind(t(replicate(n = 4, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3)))), matrix(0, nrow = nrow(trial_df), ncol = item_size))\n\n# Caluclating intensities and storing each probe\n  for(i in 1:nrow(trial_df)){\n    current_output <- get_echo(item_matrix[trial_df[i, ]$item, ], memory, output = \"both\", type = \"cosine\")\n    trial_df$intensity[i] <- current_output[[1]]\n    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))\n    norm_echo <- current_output[[2]]/max(abs(current_output[[2]]))\n    discrep <- item_matrix[trial_df[i, ]$item, ] - norm_echo\n    learned_trace <- discrep * learning_filter\n    memory[4 + i, ] <- learned_trace\n  }\ntrial_arr_slice <- trial_df %>% arrange(item) %>% slice(important_rows)\nreturn(trial_arr_slice)\n}\n\nsim_intensity_multiple_de <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  all_subs_df <- data.frame()\n  for(s in 1:n_of_sim) {\n    sub_df <- sim_intensity_once_de(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq)\n    sub_df$subject <- s\n    all_subs_df <- rbind(all_subs_df, sub_df)\n  }\n  return(all_subs_df)\n}\n```\n:::\n\n\n```         \n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_dfs_with_de <- sim_intensity_multiple_de(1000)\nsubject_sum_de <- raw_dfs_with_de %>% group_by(subject, frequency) %>%\n  summarize(mean_int = mean(intensity))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\nggplot(subject_sum_de, aes(x = mean_int, group = frequency, color = as.factor(frequency))) + geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThis surprisingly looks more like Hintzman's results, in comparison to the no discrepancy encoding condition. The shape is the same and the values are closer to Hintzman's.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_intensity_once_collins <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_max = 1.0, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  df_items <- data.frame(subject = NA, item = 1:nrow(item_matrix), frequency = frequency_vec)\n  \n  trial_df <- data.frame()\n  for(i in 1:nrow(df_items)){\n    current_rows <- bind_rows(replicate(df_items[i, ]$frequency, df_items[i, ], simplify = FALSE))\n    trial_df <- rbind(trial_df, current_rows)\n  }\n  trial_df <- trial_df %>% slice(sample(1:n()))\n  \n  # Starting state secondary memory\nmemory <- rbind(t(replicate(n = 4, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3)))), matrix(0, nrow = nrow(trial_df), ncol = item_size))\n\n# Caluclating intensities and storing each probe\n  for(i in 1:nrow(trial_df)){\n    current_intensity <- get_echo(item_matrix[trial_df[i, ]$item, ], memory, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- current_intensity\n    l_value <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))\n    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))\n    learned_trace <- item_matrix[trial_df[i, ]$item, ] * learning_filter\n    memory[4 + i, ] <- learned_trace\n  }\ntrial_arr_slice <- trial_df %>% arrange(item) %>% slice(important_rows)\nreturn(trial_arr_slice)\n}\n\nsim_intensity_multiple_collins <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_max = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  all_subs_df <- data.frame()\n  for(s in 1:n_of_sim) {\n    sub_df <- sim_intensity_once_collins(matrix_size = matrix_size, item_size = item_size, prob = prob, l_max = l_max, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq)\n    sub_df$subject <- s\n    all_subs_df <- rbind(all_subs_df, sub_df)\n  }\n  return(all_subs_df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_df_collins <- sim_intensity_multiple_collins(1000)\n\nsubject_sum_collins <- raw_df_collins %>% group_by(subject, frequency) %>% summarize(mean_int = mean(intensity))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\nggplot(subject_sum_collins, aes(x = mean_int, group = frequency, color = as.factor(frequency))) + geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis is unexpected, but it does seem to show that the increase in intensity becomes exponentially smaller with increasing frequency. The increase in amplitude is hard to explain though.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}