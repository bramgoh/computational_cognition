{
  "hash": "f1bb158b3dc4e0b556c5103835c7e71e",
  "result": {
    "markdown": "---\ntitle: \"Discrepancy Encoding in MINERVA (Part 1)\"\nauthor: \"Bram Goh\"\ndate: \"2023-03-10\"\ncategories: [code]\nimage: \"\"\n---\n\n\n# Applying discrepancy encoding to frequency judgments\n\nThe goal of this exploration is to look into how the discrepancy encoding assumption introduced in MINERVA-AL (Jamieson et al., 2012) affects Hintzman's (1988) frequency judgment simulations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nset.seed(30)\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations_3 <- function(probe, mem) {\n  \n  as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))^3)\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity') {\n    activations <- get_activations_3(probe,mem)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Item generation function (borrowed from Matt)\n\ngenerate_item <- function(item_size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = item_size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# Item matrix (original, before applying learning rate) function\n\ngen_item_matrix <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3)) {\n  item_matrix <- t(replicate(n = matrix_size, generate_item(item_size = item_size, prob = prob)))\n  return(item_matrix)\n}\n\n# Form probe matrix i.e. item_matrix multiplied by respective frequencies + 4 more random items\n\ngen_probes <- function(item_matrix, prob = c(1/3, 1/3, 1/3), max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  freq_multiplier <- rep(1:max_num_of_copies, each = num_of_traces_per_freq)\n  probe_matrix <- c()\n  for(i in 1:length(freq_multiplier)) {\n    current_rows <- matrix(rep(item_matrix[i, ], freq_multiplier[i]), nrow = freq_multiplier[i], ncol = ncol(item_matrix), byrow = TRUE)\n    probe_matrix <- rbind(probe_matrix, current_rows)\n}\n  return(probe_matrix)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall simulation function (no discrepancy encoding)\n\nsim_intensity_once <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n\n  # Starting state secondary memory\nmemory <- t(replicate(n = 60, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3))))\n\n# Caluclating intensities and storing each probe\nintensity_vector <- numeric(length = nrow(probe_matrix))\n  for(i in 1:nrow(probe_matrix)){\n    current_intensity <- get_echo(probe_matrix[i, ], memory, output = \"intensity\")\n    intensity_vector[i] <- current_intensity\n    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))\n    learned_probe <- probe_matrix[i, ] * learning_filter\n    memory[i, ] <- learned_probe\n  }\nintensity_df <- data.frame(intensity_value = intensity_vector)\nreturn(intensity_df)\n}\n\nsim_intensity_multiple <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  full_intensity_df <- data.frame(matrix(0, nrow = sum((1:max_num_of_copies)*4), ncol = n_of_sim))\n  for(i in 1:n_of_sim) {\n    intensity_column <- sim_intensity_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n    full_intensity_df[ , i] <- intensity_column\n  }\n  return(full_intensity_df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_df_no_de <- sim_intensity_multiple(1000)\n\nby_probe_no_de <- data.frame(probe_no = 1:60, intensity = rowMeans(raw_df_no_de))\nggplot(by_probe_no_de, aes(x = probe_no, y = intensity)) + geom_line() + coord_cartesian(ylim = c(-0.01, 0.075)) + scale_x_continuous(breaks = seq(1, 60, 2)) + geom_vline(xintercept = c(5, 13, 25, 41), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nby_freq_no_de <- data.frame(frequency = rep(c(1,2,3,4,5), each = 4), raw_df_no_de) %>% group_by(frequency) %>% summarize_all(mean) %>% pivot_longer(!frequency, names_to = \"Drop\", values_to = \"Intensity\") %>% select(-Drop)\nggplot(by_freq_no_de, aes(x = Intensity, color = factor(frequency))) + geom_density(show.legend = TRUE) + coord_cartesian(xlim = c(-0.01, 0.05))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\nThis looks like what we would expect: the intensities are higher when the presented probe has been seen before, and the more times it's been presented before, the higher the intensity. Looking at the frequency distributions, we see a similar overall shape to that found in Hintzman (1988), with the amplitudes decreasing and the variance increasing with higher frequency. The details are different, probably because the setup here is different from Hintzman (1988): we start off here with a 'blank slate' of random vectors in memory before adding each probe to populate memory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall simulation function (with discrepancy encoding)\n\nsim_intensity_once_de <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n\n  # Starting state secondary memory\nmemory <- t(replicate(n = 60, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3))))\n\n# Caluclating intensities and storing each probe\nintensity_vector <- numeric(length = nrow(probe_matrix))\nprobe_discrep_cor <- numeric(length = nrow(probe_matrix))\n  for(i in 1:nrow(probe_matrix)){\n    current_echo <- get_echo(probe_matrix[i, ], memory, output = \"both\")\n    intensity_vector[i] <- current_echo[[1]]\n    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))\n    current_content <- current_echo[[2]]/max(current_echo[[2]])\n    discrep <- probe_matrix[i, ] - current_content\n    learned_trace <- discrep * learning_filter\n    memory[i, ] <- learned_trace\n    current_cor <- cor(probe_matrix[i, ], discrep)\n    probe_discrep_cor[i] <- current_cor\n  }\nintensity_df <- data.frame(intensity_value = intensity_vector)\ndiscrep_df = data.frame(discrep_cor = probe_discrep_cor)\noutput_final <- list(intensity = intensity_df, discrep_cor = discrep_df, memory = memory)\nreturn(output_final)\n}\n\nsim_intensity_multiple_de <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  full_intensity_df <- data.frame(matrix(0, nrow = sum((1:max_num_of_copies)*4), ncol = n_of_sim))\n  full_discrep_cor_df <- data.frame(matrix(0, nrow = sum((1:max_num_of_copies)*4), ncol = n_of_sim))\n  for(i in 1:n_of_sim) {\n    output_list <- sim_intensity_once_de(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n    full_intensity_df[ , i] <- output_list[[1]]\n    full_discrep_cor_df[ , i] <- output_list[[2]]\n  }\n  full_df_list <- list(intensity = full_intensity_df, dicrep_cor = full_discrep_cor_df)\n  return(full_df_list)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_dfs_with_de <- sim_intensity_multiple_de(1000)\nintensity_with_de <- raw_dfs_with_de[[1]]\ndiscrep_cor_with_de <- raw_dfs_with_de[[2]]\nby_probe_with_de <- data.frame(probe_no = 1:60, intensity = rowMeans(intensity_with_de), discrep_cor = rowMeans(discrep_cor_with_de))\nggplot(by_probe_with_de, aes(x = probe_no, y = intensity)) + geom_line() + scale_x_continuous(breaks = seq(1, 60, 2)) + geom_vline(xintercept = c(5, 13, 25, 41), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(by_probe_with_de, aes(x = probe_no, y = discrep_cor)) + geom_line() + scale_x_continuous(breaks = seq(1, 60, 2)) + geom_vline(xintercept = c(5, 13, 25, 41), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nby_freq_with_de <- data.frame(frequency = rep(c(1,2,3,4,5), each = 4), intensity_with_de) %>% group_by(frequency) %>% summarize_all(mean) %>% pivot_longer(!frequency, names_to = \"Drop\", values_to = \"Intensity\") %>% select(-Drop)\nggplot(by_freq_with_de, aes(x = Intensity, color = factor(frequency))) + geom_density(show.legend = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n:::\n\n\nThis result doesn't make sense. It could be that the discrepancy encoding in MINERVA-AL applies only to associative learning, where two or more events are paired and you can elicit expectations for an event through \"partial return\", as Semon would have put it, by presenting one of the paired events. In the frequency judgment task, however, the probe presented is not paired with anything, and so there is no expectation to be had. Each probe is a \"full pattern\" with no 0s to show that a \"missing event\" is anticipated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall simulation function (using Collins et al. 2020's encoding function)\n\nsim_intensity_collins <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value_max = 1.0, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n\n  # Starting state secondary memory\nmemory <- t(replicate(n = 60, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3))))\n\n# Caluclating intensities and storing each probe\nintensity_vector <- numeric(length = nrow(probe_matrix))\n  for(i in 1:nrow(probe_matrix)){\n    current_echo <- get_echo(probe_matrix[i, ], memory, output = \"both\")\n    intensity_vector[i] <- current_echo[[1]]\n    l_value <- l_value_max * (1 - 1/(1 + exp(1)^(-12 * current_echo[[1]] + 2)))\n    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))\n    learned_trace <- probe_matrix[i, ] * learning_filter\n    memory[i, ] <- learned_trace\n  }\nintensity_df <- data.frame(intensity_value = intensity_vector)\nreturn(intensity_df)\n}\n\nsim_intensity_multiple_collins <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value_max = 1.0, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  full_intensity_df <- data.frame(matrix(0, nrow = sum((1:max_num_of_copies)*4), ncol = n_of_sim))\n  for(i in 1:n_of_sim) {\n    current_int <- sim_intensity_collins(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value_max = l_value_max, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n    full_intensity_df[ , i] <- current_int\n  }\n  return(full_intensity_df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_df_collins <- sim_intensity_multiple_collins(1000)\n\nby_probe_collins <- data.frame(probe_no = 1:60, intensity = rowMeans(raw_df_collins))\nggplot(by_probe_collins, aes(x = probe_no, y = intensity)) + geom_line() + scale_x_continuous(breaks = seq(1, 60, 2)) + geom_vline(xintercept = c(5, 13, 25, 41), color = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\nby_freq_collins <- data.frame(frequency = rep(c(1,2,3,4,5), each = 4), raw_df_collins) %>% group_by(frequency) %>% summarize_all(mean) %>% pivot_longer(!frequency, names_to = \"Drop\", values_to = \"Intensity\") %>% select(-Drop)\nggplot(by_freq_collins, aes(x = Intensity, color = factor(frequency))) + geom_density(show.legend = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\nThe encoding function in Collins et al. (2020) makes more sense, even more sense than the no-DE simulations! The initial spike from frequency = 0 to frequency = 1 makes sense, and then subsequent increases in intensity become more and more muted as frequency increases i.e. the increase from frequency = 1 to frequency = 2 is smaller, and so on. At some point, uf we were to simulate frequencies higher than 5, we might arrive at a plateau, where the presentation of a high frequency probe does not lead to a noticeable increase in the intensity of the echo. Since familiarity (i.e. echo intensity) is inversely proportional to L, the rate at which a well-learned probe is encoded to memory diminishes with frequency, just as Matt predicted.\n\nSo where does this leave discrepancy encoding as laid out in MINVERVA-AL (Jamieson et al., 2012)? As laid out in Collins et al., the retrieval process occurs both at the encoding stage and at the retrieval stage. Maybe the MINERVA-AL discrepancy encoding function happens at the retrieval stage AFTER the Collins et al encoding function? This way, you get the best of both worlds: a copy of the actual item (with varying fidelity depending on L) that occurs at the encoding stage AND a record of expectancy information that occurs at the retrieval stage.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}