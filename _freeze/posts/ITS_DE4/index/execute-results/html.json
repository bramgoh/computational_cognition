{
  "hash": "c42a53c4d793bba7daa3320fcbd6f7bb",
  "result": {
    "markdown": "---\ntitle: \"Exploring discrepancy encoding with ITS (Part 4)\"\nauthor: \"Bram Goh\"\ndate: \"2023-04-21\"\ncategories: [code]\nimage: \"ALredlines.png\"\n---\n\n\n# One-hot encoding words in ITS and applying discrepancy encoding\n\nWe identified a difficulty with combining ITS and MINERVA-AL: in ITS, sentence vectors are formed by adding word vectors together, and two or more word vectors can have non-zero elements in the same vector columns (e.g. A = \\[1, 1, 0, 1\\], B = \\[1, 0, 1, 1\\]. Thus, values in the sentence vectors can exceed the -1 to 1 range. In contrast, in MINERVA-AL, compound stimuli is represented by adding basic components with non-overlapping vector columns (e.g. A = \\[1, 1, 0, 0\\], B = \\[0, 0, 1, 1\\]). Also, MINERVA-AL always normalizes the echo to the -1 to 1 range before computing the probe-echo discrepancy. Hence, in last week's exercise, the probes (sentence vectors) had values way larger in magnitude than the echoes, resulting in the discrepancy vectors being strange.\n\nOne way to mitigate this issue is to normalize the sentence vectors so that they also fall within the -1 to 1 range. Another method is to apply something similar to one-hot encoding to ensure the word vectors don't have overlapping vector columns, so sentences always remain within the -1 to 1 range.\n\nThe goal of this exercise is to repeat last week's exercise while apply one-hot encoding-style representations for word vectors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(RsemanticLibrarian)\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'data.table'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    transpose\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nn_words <- 130\nband_width <- 10\n\n# Setup of word-sentence co-occurence matrix\nband_matrix <- Matrix::bandSparse(n = n_words,k=0:(band_width-1))\nband_matrix <- as.matrix(band_matrix)*1\n\n# ITS setup (each word appears in the same number of sentences)\ndictionary_words <- 1:n_words\n\nsentence_ids <- list()\nfor(i in 1:ncol(band_matrix)){\n  sentence_ids[[i]] <- which(band_matrix[,i] == 1)\n}\nsentence_ids <- sentence_ids[11:119]\n\nwords_to_plot <- c(11:110)\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  return(word_meaning)\n}\n\n# Function to create environment vectors with non-overlapping columns (modified from RsemanticLibrarian::sl_create_riv)\nno_overlap_riv <- function(dimensions, no_of_words, sparsity = 10){\n  if(sparsity %% 2 != 0) stop(\"sparsity must be even integer\")\n  temp_matrix <- matrix(0,ncol=dimensions,nrow=no_of_words)\n  for(i in 1:no_of_words){\n    temp_matrix[i, (i*10):(i*10+sparsity-1)] <- sample(c(rep(1, sparsity/2), rep(-1, sparsity/2)))\n  }\n  return(temp_matrix)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines <- lsa::cosine(t(band_matrix[words_to_plot,]))\nsecond_order_cosines <- lsa::cosine(t(first_order_cosines))\n```\n:::\n\n\n## Comparing learning over 15 representations of sentences\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_sent_freq <- 15\nn_of_sim <- 10\n```\n:::\n\n\n### No discrepancy encoding (L = 1.0, i.e. point of comparison)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_first_1.0 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_1.0 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory <- prior_memory\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_1.0 <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_1.0),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_1.0),c(second_order_cosines))^2\n  }\nR2_first_1.0[i, ] <- R2_first\nR2_second_1.0[i, ] <- R2_second\n}\n```\n:::\n\n\n### No discrepancy encoding (L = .3)\n\nFirst, let's visualize how the semantic vector similarities change as sentences are presented with higher frequency.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_0.3 <- list()\n\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\nprior_memory_test <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory <- prior_memory_test\n\nfor(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .3, .3))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_list_0.3[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:length(cosines_list_0.3)){\n  corrplot::corrplot(cosines_list_0.3[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-15.png){width=672}\n:::\n:::\n\n\nNow, for the actual simulations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_first_0.3 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_0.3 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory_0.3 <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .3, .3))\n    }\n    its_memory_0.3 <- rbind(its_memory_0.3, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_0.3,\n                                                tau=3)\n  cosines_ITS_0.3 <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_0.3),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_0.3),c(second_order_cosines))^2\n  }\nR2_first_0.3[i, ] <- R2_first\nR2_second_0.3[i, ] <- R2_second\n}\n```\n:::\n\n\n### No discrepancy encoding (L = .5)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_first_0.5 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_0.5 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory_0.5 <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .5, .5))\n    }\n    its_memory_0.5 <- rbind(its_memory_0.5, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_0.5,\n                                                tau=3)\n  cosines_ITS_0.5 <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_0.5),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_0.5),c(second_order_cosines))^2\n  }\nR2_first_0.5[i, ] <- R2_first\nR2_second_0.5[i, ] <- R2_second\n}\n```\n:::\n\n\n### No discrepancy encoding (L = .7)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_first_0.7 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_0.7 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory_0.7 <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .7, .7))\n    }\n    its_memory_0.7 <- rbind(its_memory_0.7, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_0.7,\n                                                tau=3)\n  cosines_ITS_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_0.7),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_0.7),c(second_order_cosines))^2\n  }\nR2_first_0.7[i, ] <- R2_first\nR2_second_0.7[i, ] <- R2_second\n}\n```\n:::\n\n\n### MINERVA-AL discrepancy encoding (L = 1.0)\n\nLet's visualize the progression of ITS encoding by visualizing the semantic vector similarities. First, let's examine the similarities when the sentences are in the original order.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_AL <- list()\n\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nfull_norm_echoes <- c()\n\nintensities_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL <- prior_memory_test\n\nfor(j in 1:max_sent_freq){\n  intensities <- numeric(length = length(sentence_ids))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  intensities_matrix[j, ] <- intensities\n  full_norm_echoes <- rbind(full_norm_echoes, normalized_echoes)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:length(cosines_list_AL)){\n  corrplot::corrplot(cosines_list_AL[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-15.png){width=672}\n:::\n:::\n\n\nHow can we explain the red lines? The red lines are visible from the first iteration and get clearer with increasing sentence frequency. They're are also well-defined and sharp, instead of fading out. Each semantic vector has 2 semantic vectors it is negatively related to: one that is 10 words before it, and one that is 10 words after it. The magnitude of the red lines also follows a regular pattern, growing stronger and weaker in regular cycles.\n\nLet's also plot the intensities to get a better idea of whether what's being stored is still related to the original sentences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintensities_df <- data.frame(sent_freq = 1:15, intensities_matrix)\nintensities_df <- intensities_df %>% pivot_longer(-sent_freq, names_to = \"sentence\", names_prefix = \"X\", names_transform = list(sentence = as.numeric), values_to = \"intensities\")\nintensities_sliver <- intensities_df %>% filter(sent_freq %in% c(1, 5, 10, 15))\nggplot(intensities_sliver, aes(x = sentence, y = intensities, group = sent_freq, color = sent_freq)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nNot really sure what to make of the peaks and valleys across the sentences, but at least intensity increases with sentence frequency, which is what we expect.\n\nNow let's see the similarities when the sentences are randomly reordered every time they are presented.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_AL <- list()\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n  \nits_memory_AL <- prior_memory_test\n\nfor(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids_reordered)) {\n    current_sentence <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    echo <- colSums(as.numeric(cosine_x_to_m(current_sentence, its_memory_AL) ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:length(cosines_list_AL)){\n  corrplot::corrplot(cosines_list_AL[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-15.png){width=672}\n:::\n:::\n\n\nThe reordering of the sentences affects the regular pattern of the magnitudes of the red regions. Here, the blue areas also disappear with increasing sentence frequency.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_first_AL <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_AL <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory_AL <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids_reordered)) {\n    current_sentence <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    echo <- colSums(as.numeric(cosine_x_to_m(current_sentence, its_memory_AL) ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_ITS_AL <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_AL),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_AL),c(second_order_cosines))^2\n  }\nR2_first_AL[i, ] <- R2_first\nR2_second_AL[i, ] <- R2_second\n}\n```\n:::\n\n\n### Plotting the graphs for comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R-squared wrt first-order similarity\n\nR2_first_no_de <- data.frame(l_value = c(1.0, 0.7, 0.5, 0.3), rbind(colMeans(R2_first_1.0), colMeans(R2_first_0.7), colMeans(R2_first_0.5), colMeans(R2_first_0.3)))\ncolnames(R2_first_no_de) <- c(\"l_value\", 1:max_sent_freq)\nR2_first_no_de <- R2_first_no_de %>% pivot_longer(-l_value, names_to = \"sent_freq\", values_to = \"R2_first_order\")\n\nR2_first_de <- data.frame(sent_freq = 1:max_sent_freq, colMeans(R2_first_AL))\ncolnames(R2_first_de) <- c(\"sent_freq\", \"R2_first_order\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + geom_line(data = R2_first_no_de, aes(x = factor(sent_freq, level = 1:15), y = R2_first_order, group = as.factor(l_value), color = as.factor(l_value))) + geom_line(data = R2_first_de, aes(x = factor(sent_freq, level = 1:15), y = R2_first_order, group = 1), linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# R-squared wrt second-order similarity\n\nR2_second_no_de <- data.frame(l_value = c(1.0, 0.7, 0.5, 0.3), rbind(colMeans(R2_second_1.0), colMeans(R2_second_0.7), colMeans(R2_second_0.5), colMeans(R2_second_0.3)))\ncolnames(R2_second_no_de) <- c(\"l_value\", 1:max_sent_freq)\nR2_second_no_de <- R2_second_no_de %>% pivot_longer(-l_value, names_to = \"sent_freq\", values_to = \"R2_second_order\")\n\nR2_second_de <- data.frame(sent_freq = 1:max_sent_freq, colMeans(R2_second_AL))\ncolnames(R2_second_de) <- c(\"sent_freq\", \"R2_second_order\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + geom_line(data = R2_second_no_de, aes(x = factor(sent_freq, level = 1:15), y = R2_second_order, group = as.factor(l_value), color = as.factor(l_value))) + geom_line(data = R2_second_de, aes(x = factor(sent_freq, level = 1:15), y = R2_second_order, group = 1), linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nThe discrepancy encoding is still hitting a ceiling early on before declining, never reaching the values for when there is no discrepancy encoding. This is the case for the R-squared with respect to both first- and second-order similarities.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}