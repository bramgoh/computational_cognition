{
  "hash": "3c165bcfd403d1ddc85d683444230c33",
  "result": {
    "markdown": "---\ntitle: \"Hintzman's (1988) MINERVA (Part 1)\"\nauthor: \"Bram Goh\"\ndate: \"2023-02-14\"\ncategories: [code]\nimage: \"echo_intensity_amended.png\"\n---\n\n\n# Moving on to Hintzman (1988)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nset.seed(30)\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations_3 <- function(probe, mem) {\n  \n  as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))^3)\n}\n\n# Item generation function (borrowed from Matt)\n\ngenerate_item <- function(item_size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = item_size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# Item matrix (original, before applying learning rate) function\n\ngen_item_matrix <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3)) {\n  item_matrix <- t(replicate(n = matrix_size, generate_item(item_size = item_size, prob = prob)))\n  return(item_matrix)\n}\n\n# Form probe matrix i.e. item_matrix + 4 more random items\n\ngen_probes <- function(item_matrix, prob = c(1/3, 1/3, 1/3), max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  random_items <- t(replicate(n = num_of_traces_per_freq, generate_item(item_size = ncol(item_matrix), prob = prob)))\n  probe_matrix <- rbind(random_items, item_matrix)\n  return(probe_matrix)\n}\n\n# Form secondary memory -- create encoded matrix (i.e. apply learning rate) and input varying frequencies of items\n\ngen_secondary_mem <- function(item_matrix, l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n \n  freq_multiplier <- c()\n  for (i in 1:max_num_of_copies) {\n    current_multiplier <- rep(i, num_of_traces_per_freq)\n    freq_multiplier <- c(freq_multiplier, current_multiplier)\n  }\n  secondary_memory <- c()\n  for(j in 1:length(freq_multiplier)) {\n    current_rows <- matrix(rep(item_matrix[j, ], freq_multiplier[j]), nrow = freq_multiplier[j], ncol = ncol(item_matrix), byrow = TRUE)\n    secondary_memory <- rbind(secondary_memory, current_rows)\n  }\n   learning_matrix <- t(replicate(n = nrow(secondary_memory), sample(c(0,1), size = ncol(secondary_memory), prob = c(1 - l_value, l_value), replace = TRUE)))\n  encoded_memory <- secondary_memory * learning_matrix\n return(encoded_memory)\n}\n\n\n\n# Calculate activations for multiple probes\n\ncalc_activs_for_mult_probes <- function(probe_matrix, secondary_memory) {\n  activations_matrix <- c()\n  for(i in 1:nrow(probe_matrix)) {\n    current_activs <- get_activations_3(probe_matrix[i, ], secondary_memory)\n    activations_matrix <- rbind(activations_matrix, current_activs)\n  }\n  return(activations_matrix)\n}\n\n# Convert activations matrix to transformed intensity matrix ready for plotting\n\nconvert_to_intensity_mat <- function(activations_matrix, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  intensity_vector <- rowSums(activations_matrix)\n  intensity_matrix <- matrix(intensity_vector, nrow = max_num_of_copies + 1, ncol = num_of_traces_per_freq, byrow = TRUE)\n  return(intensity_matrix)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall simulation function\n\nsim_intensity_once <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  secondary_memory <- gen_secondary_mem(item_matrix, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  activations_matrix <- calc_activs_for_mult_probes(probe_matrix, secondary_memory)\n  intensity_matrix <- convert_to_intensity_mat(activations_matrix, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  return(intensity_matrix)\n}\n\nsim_intensity_multiple <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  raw_intensity_matrix <- c()\n  for(i in 1:n_of_sim) {\n    temp_intensity <- sim_intensity_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n    raw_intensity_matrix <- cbind(raw_intensity_matrix, temp_intensity)\n  }\n  row_names <- as.data.frame(0:max_num_of_copies)\n  names(row_names) <- \"Frequency\"\n  intensity_df <- bind_cols(row_names, data.frame(raw_intensity_matrix)) %>%\n      pivot_longer(!Frequency, names_to = \"Drop\", values_to = \"Intensity\") %>% select(\"Frequency\", \"Intensity\")\n  return(intensity_df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_intensity <- sim_intensity_multiple(1000)\nggplot(df_intensity, aes(x = Intensity, color = factor(Frequency))) + geom_density(show.legend = TRUE) + xlim(-1, 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 4 rows containing non-finite values (stat_density).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nFinally, I managed to reproduce the graph. I had gotten the general shape earlier (see below) but the mean values were too small and the graphs weren't spread out enough. It turns out that I was misunderstanding the L value. The L value affects **sampling rate** (i.e. how many features are selected to be copied versus nullified), not **trace quality** (i.e. the actual number of features in memory that are different from others). Thus, I had assumed that all copies of the same original item in secondary memory would i) have the same same number of features learnt, ii) have the exact same features. I was wrong on both counts.\n\nI fixed this by creating the copies first **before** applying the learning rate, instead of the other way around. This way, the learning rate applies noise, so even among copies of the same item, there is variation in the number of features changes. This is due to the fact that, for some traces, feature values that were originally 0 may be selected to be nullified (resulting in essentially no change), whereas for other traces, feature values that were originally 1 or -1 may be selected to be nullified (resulting in a meaningful change).\n\n![](echo_intensity.png){width=\"426\"}\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}