{
  "hash": "e8c691f5eae23b98349cd91fe2354193",
  "result": {
    "markdown": "---\ntitle: \"Exploring discrepancy encoding with ITS (Part 5)\"\nauthor: \"Bram Goh\"\ndate: \"2023-04-28\"\ncategories: [code]\nimage: \"learning_rate_comp.png\"\n---\n\n\n# Let's try one-hot encoding again...\n\nMy R-squared values were peaking around .2, while Matt's were peaking around .6. After some tinkering, I realized that this is at least partially because my RIVs were much sparser (10000 columns, sparsity = 8) than his (1000 columns, sparsity = 100). Using his code, I ran it once with each of the RIVs and found that the sparser RIVs led to lower R-squared values.\n\nThis week, I am going to use less sparse RIVs and do the following:\n\n-   Examine how different learning rates affect the reconstruction, intensity and R-squared values\n\n-   Without reordering the sentences, plot the sentence vectors, echoes, and discrepancies to visualize better what traces are being stored, and how that affects what happens when the same sentence is presented again in the next epochs.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(RsemanticLibrarian)\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'data.table'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    transpose\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(lsa)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: SnowballC\n```\n:::\n\n```{.r .cell-code}\nn_words <- 130\nband_width <- 10\n\n# Setup of word-sentence co-occurence matrix\nband_matrix <- Matrix::bandSparse(n = n_words,k=0:(band_width-1))\nband_matrix <- as.matrix(band_matrix)*1\n\n# ITS setup (each word appears in the same number of sentences)\ndictionary_words <- 1:n_words\n\nsentence_ids <- list()\nfor(i in 1:ncol(band_matrix)){\n  sentence_ids[[i]] <- which(band_matrix[,i] == 1)\n}\nsentence_ids <- sentence_ids[10:118]\n\nwords_to_plot <- c(10:109)\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  return(word_meaning)\n}\n\n# Function to create environment vectors with non-overlapping columns (modified from RsemanticLibrarian::sl_create_riv) - only uses 1s (no -1 values)\nno_overlap_riv <- function(dimensions, no_of_words, sparsity = 10){\n  if(sparsity %% 2 != 0) stop(\"sparsity must be even integer\")\n  temp_matrix <- matrix(0,ncol=dimensions,nrow=no_of_words)\n  for(i in 1:no_of_words){\n    temp_matrix[i, (i*10):(i*10+sparsity-1)] <- sample(c(rep(1, sparsity/2), rep(1, sparsity/2)))\n  }\n  return(temp_matrix)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines <- lsa::cosine(t(band_matrix[words_to_plot,]))\nsecond_order_cosines <- lsa::cosine(t(first_order_cosines))\n\nmax_sent_freq <- 10\n```\n:::\n\n\n## Without reordering, no learning rate applied (L = 1.0)\n\n### No discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\n\nenvironment_sub <- no_overlap_riv(1500, length(dictionary_words))\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory <- prior_memory\n\nintensities_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, current_sentence)\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS),c(second_order_cosines))^2\n  intensities_matrix[j, ] <- intensities\n  reconst_matrix[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_AL <- list()\n\nR2_first_AL <- numeric(length = max_sent_freq)\nR2_second_AL <- numeric(length = max_sent_freq)\n\nenvironment_sub <- no_overlap_riv(1500, length(dictionary_words))\n\nsentence_matrix <- c()\n\nnorm_echoes_matrix <- c()\n\nintensities_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- echo/max(abs(echo))\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  sentence_matrix <- rbind(sentence_matrix, sentences)\n  norm_echoes_matrix <- rbind(norm_echoes_matrix, normalized_echoes)\n  intensities_matrix_AL[j, ] <- intensities\n  reconst_matrix_AL[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n  R2_first_AL[j] <- cor(c(cosines_list_AL[[j]]),c(first_order_cosines))^2\n  R2_second_AL[j] <- cor(c(cosines_list_AL[[j]]),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot::corrplot(cosines_list_AL[[10]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Comparing no discrepancy encoding to discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix), rowMeans(intensities_matrix), R2_first, R2_second)\ncolnames(no_de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_perfect <- no_de_df_perfect %>% pivot_longer(c(first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL), rowMeans(intensities_matrix_AL), R2_first_AL, R2_second_AL)\ncolnames(de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"first_order_R2\", \"second_order_R2\")\nde_df_perfect <- de_df_perfect %>% pivot_longer(c(first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_perfect <- bind_rows(no_de_df_perfect, de_df_perfect)\nggplot(compare_df_perfect, aes(x = sent_freq, y = reconstruction, color = condition)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(compare_df_perfect, aes(x = sent_freq, y = intensity, color = condition)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(compare_df_perfect, aes(x = sent_freq, y = R_squared, color = order_of_sim)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n:::\n\n\n## Without reordering, applying learning rate of .7\n\n### No discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL <- .7\n\nR2_first_0.7 <- numeric(length = max_sent_freq)\nR2_second_0.7 <- numeric(length = max_sent_freq)\n\nenvironment_sub <- no_overlap_riv(1500, length(dictionary_words))\n\nits_memory <- prior_memory\n\nintensities_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, (current_sentence*(rbinom(length(current_sentence), 1, L))))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_first_0.7[j] <- cor(c(cosines_ITS_0.7),c(first_order_cosines))^2\n  R2_second_0.7[j] <- cor(c(cosines_ITS_0.7),c(second_order_cosines))^2\n  intensities_matrix_0.7[j, ] <- intensities\n  reconst_matrix_0.7[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_first_AL_0.7 <- numeric(length = max_sent_freq)\nR2_second_AL_0.7 <- numeric(length = max_sent_freq)\n\nsentence_matrix_0.7 <- c()\n\nnorm_echoes_matrix_0.7 <- c()\n\nintensities_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL_0.7 <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL_0.7)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL_0.7))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- echo/max(abs(echo))\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL_0.7 <- rbind(its_memory_AL_0.7, ((current_sentence - echo_norm)*(rbinom(length(current_sentence), 1, L))))\n    }\n  sentence_matrix_0.7 <- rbind(sentence_matrix_0.7, sentences)\n  norm_echoes_matrix_0.7 <- rbind(norm_echoes_matrix_0.7, normalized_echoes)\n  intensities_matrix_AL_0.7[j, ] <- intensities\n  reconst_matrix_AL_0.7[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL_0.7,\n                                                tau=3)\n  cosines_AL_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_first_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(first_order_cosines))^2\n  R2_second_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### Comparing no discrepancy encoding to discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix_0.7), rowMeans(intensities_matrix_0.7), R2_first_0.7, R2_second_0.7)\ncolnames(no_de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_0.7 <- no_de_df_0.7 %>% pivot_longer(c(first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL_0.7), rowMeans(intensities_matrix_AL_0.7), R2_first_AL_0.7, R2_second_AL_0.7)\ncolnames(de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"first_order_R2\", \"second_order_R2\")\nde_df_0.7 <- de_df_0.7 %>% pivot_longer(c(first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_0.7 <- bind_rows(no_de_df_0.7, de_df_0.7)\nggplot(compare_df_0.7, aes(x = sent_freq, y = reconstruction, color = condition)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(compare_df_0.7, aes(x = sent_freq, y = intensity, color = condition)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(compare_df_0.7, aes(x = sent_freq, y = R_squared, color = order_of_sim)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n:::\n\n\n## In summary...\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomplete_df <- bind_rows(compare_df_perfect, compare_df_0.7)\ncomplete_df$learning_rate <- factor(complete_df$learning_rate, levels = c(\"1.0\", \"0.7\"))\nggplot(complete_df, aes(x = sent_freq, y = reconstruction, color = condition, linetype = learning_rate)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(complete_df, aes(x = sent_freq, y = intensity, color = condition, linetype = learning_rate)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(complete_df, aes(x = sent_freq, y = R_squared, color = order_of_sim, linetype = learning_rate)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n:::\n\n\n## Plotting the traces\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiscrep_matrix <- its_memory_AL[101:1190, ]\n\nsent_df <- data.frame(trial_no = 1:nrow(sentence_matrix), type = \"sentence\", sentence_matrix) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\necho_df <- data.frame(trial_no = 1:nrow(norm_echoes_matrix), type = \"echo_norm\", norm_echoes_matrix) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\ndiscrep_df <- data.frame(trial_no = 1:nrow(discrep_matrix), type = \"discrepancy\", discrep_matrix) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\n\ncombined_df <- bind_rows(sent_df, echo_df, discrep_df)\ncombined_df$type <- factor(combined_df$type, levels = c(\"sentence\", \"echo_norm\", \"discrepancy\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndiscrep_matrix_0.7 <- its_memory_AL_0.7[101:1190, ]\n\nsent_df_0.7 <- data.frame(trial_no = 1:nrow(sentence_matrix_0.7), type = \"sentence\", sentence_matrix_0.7) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\necho_df_0.7 <- data.frame(trial_no = 1:nrow(norm_echoes_matrix_0.7), type = \"echo\", norm_echoes_matrix_0.7) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\ndiscrep_df_0.7 <- data.frame(trial_no = 1:nrow(discrep_matrix_0.7), type = \"discrepancy\", discrep_matrix_0.7) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\n\ncombined_df_0.7 <- bind_rows(sent_df_0.7, echo_df_0.7, discrep_df_0.7)\ncombined_df_0.7$type <- factor(combined_df_0.7$type, levels = c(\"sentence\", \"echo\", \"discrepancy\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrial_12 <- combined_df %>% filter(trial_no %in% c(1:5, 110:114))\nggplot(trial_12, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntrial_12_0.7 <- combined_df_0.7 %>% filter(trial_no %in% c(1:5, 110:114))\nggplot(trial_12_0.7, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrial_34 <- combined_df %>% filter(trial_no %in% c(219:223, 328:332))\nggplot(trial_34, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntrial_34_0.7 <- combined_df_0.7 %>% filter(trial_no %in% c(219:223, 328:332))\nggplot(trial_34_0.7, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrial_56 <- combined_df %>% filter(trial_no %in% c(437:441, 546:550))\nggplot(trial_56, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntrial_56_0.7 <- combined_df_0.7 %>% filter(trial_no %in% c(437:441, 546:550))\nggplot(trial_56_0.7, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n:::\n\n\nWhen learning rate is 1, the discrepancy traces becomes almost negligible by the 6th presentation of the sentence. When learning rate is .7, the discrepancy traces require more presentations before becoming almost negligible. Thus, there are more traces stored that are more similar to the sentence, leading to higher activations.\n\nMaybe these higher activations then lead to higher R-squared values?\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}