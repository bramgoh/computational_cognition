{
  "hash": "e961e4b1e4fed6cecad7572e03407fc6",
  "result": {
    "markdown": "---\ntitle: \"Exploring discrepancy encoding with ITS (Part 3)\"\nauthor: \"Bram Goh\"\ndate: \"2023-04-07\"\ncategories: [code]\nimage: \"ALvsnoDE.png\"\n---\n\n\n# Using toy language setup from Sandin et al.'s (2017)\n\nThe goal of this exercise is to use the toy language setup in Sandin et al. (2017) (without actually using the intricacies of random indexing) to:\n\n-   Find the number of sentences required for learning at each L to obtain echoes similar to that of L = 1.0\n\nmost appropriate parameters (tau, L) without discrepancy encoding\n\n## Matt's code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(RsemanticLibrarian)\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'data.table'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    transpose\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nn_words_matt <- 1000\nband_width_matt <- 50\n\n#rows are words, columns are sentences\nband_matrix_matt <- Matrix::bandSparse(n = n_words_matt,k=0:band_width_matt)\nband_matrix_matt <- as.matrix(band_matrix_matt)*1\n\n# the following use functions from Rsemanticlibrarian\n\n# get all unique words from training text\ndictionary_words_matt <- 1:n_words_matt\n\n# create a list of sentences by index in dictionary\nsentence_ids_matt <- list()\nfor(i in 1:ncol(band_matrix_matt)){\n  sentence_ids_matt[[i]] <- which(band_matrix_matt[,i] == 1)\n}\n\n#create random environment vectors for each word in dictionary\nenvironment_matt <- sl_create_riv(10000,length(dictionary_words_matt),8)\n\n# make ITS sentence memory\n# for each sentence, sum the environment vectors for the words in the sentence\nits_memory_matt <- matrix(0,ncol=dim(environment_matt)[2],nrow=length(sentence_ids_matt))\nfor(i in 1:length(sentence_ids_matt)){\n  if(is.null(nrow(environment_matt[sentence_ids_matt[[i]],]))){\n    its_memory_matt[i,] <- environment_matt[sentence_ids_matt[[i]],]\n  } else {\n    its_memory_matt[i,] <- colSums(environment_matt[sentence_ids_matt[[i]],])\n  }\n  \n}\n\nwords_to_plot_matt <- c(1:100)\n\n# function to generate echoes for a list of words\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  \n  return(word_meaning)\n  \n}\n\nword_semantic_vectors_matt <- subset_semantic_vectors(words_to_plot_matt,                                                dictionary_words_matt,\n    environment_matt,\n      its_memory_matt,\n          tau=3)\n\ncosine_matt <- lsa::cosine(t(word_semantic_vectors_matt))\n\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines_matt <- lsa::cosine(t(band_matrix_matt[words_to_plot_matt,]))\nsecond_order_cosines_matt <- lsa::cosine(t(first_order_cosines_matt))\n\nmatt_R2_first <- cor(c(cosine_matt),c(first_order_cosines_matt))^2\nmatt_R2_second <- cor(c(cosine_matt),c(second_order_cosines_matt))^2\n```\n:::\n\n\n## Making minor changes and comparing to Matt's code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_words <- 130\nband_width <- 10\n\n# Setup of word-sentence co-occurence matrix\nband_matrix <- Matrix::bandSparse(n = n_words,k=0:(band_width-1))\nband_matrix <- as.matrix(band_matrix)*1\n\n# ITS setup (each word appears in the same number of sentences)\ndictionary_words <- 1:n_words\n\nsentence_ids <- list()\nfor(i in 1:ncol(band_matrix)){\n  sentence_ids[[i]] <- which(band_matrix[,i] == 1)\n}\nsentence_ids <- sentence_ids[11:119]\nenvironment <- sl_create_riv(10000,length(dictionary_words),8)\n\nwords_to_plot <- c(11:110)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines <- lsa::cosine(t(band_matrix[words_to_plot,]))\nsecond_order_cosines <- lsa::cosine(t(first_order_cosines))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(environment), replace = TRUE)))\n\nits_memory_1 <- rbind(prior_memory, matrix(0,ncol=dim(environment)[2],nrow=length(sentence_ids)))\nfor(i in 1:length(sentence_ids)){\n  if(is.null(nrow(environment[sentence_ids[[i]],]))){\n    its_memory_1[4+i,] <- environment[sentence_ids[[i]],]\n  } else {\n    its_memory_1[4+i,] <- colSums(environment[sentence_ids[[i]],])\n  }\n}\n\nsemantic_vectors_1 <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment,\n                                                its_memory_1,\n                                                tau=3)\n\ncosine_1 <- lsa::cosine(t(semantic_vectors_1))\n\ncorrplot::corrplot(cosine_1,\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_first_1 <- cor(c(cosine_1),c(first_order_cosines))^2\nR2_first_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9425725\n```\n:::\n\n```{.r .cell-code}\nmatt_R2_first\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9578603\n```\n:::\n\n```{.r .cell-code}\nR2_second_1 <- cor(c(cosine_1),c(second_order_cosines))^2\nR2_second_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9978066\n```\n:::\n\n```{.r .cell-code}\nmatt_R2_second\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9681356\n```\n:::\n:::\n\n\nI've changed three things in Matt's code: i) changing the bandwidth from 11 to 10; iii) making sure each sentence has the same number of words (i.e. 10); ii) including prior memory (i.e. noise) into ITS memory. Our first-order R-squared values are similar, but my second-order R-squared is slightly higher?\n\n## Comparing the number of sentences required for adequate learning at different L\n\nUsing the R-squared between ITS and the second-order word-word similarities as the metric, we can find out, given a L value (e.g. L = .3, .5, .7), how many sentences a subject needs to obtain learning comparable to when L = 1.0.\n\n### No discrepancy encoding (L = 1.0, i.e. point of comparison)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_sent_freq <- 15\n\nn_of_sim <- 10\n\n# L = 1.0\n\nR2_full_1.0 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_1.0 <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_1.0),c(second_order_cosines))^2\n  }\nR2_full_1.0[i, ] <- R2_second\n}\n```\n:::\n\n\n### No discrepancy encoding (L = .3)\n\nFirst, let's visualize how the word-sentence similarities change as sentences are presented with higher frequency.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_0.3 <- list()\n\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nits_memory <- prior_memory\n\nfor(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .3, .3))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_list_0.3[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:length(cosines_list_0.3)){\n  corrplot::corrplot(cosines_list_0.3[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-15.png){width=672}\n:::\n:::\n\n\nNow, for the actual simulations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_full_0.3 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .3, .3))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.3 <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_0.3),c(second_order_cosines))^2\n  }\nR2_full_0.3[i, ] <- R2_second\n}\n```\n:::\n\n\n### No discrepancy encoding (L = .5)\n\nVisualizing the word-sentence similarities with increasing sentence frequency:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_0.5 <- list()\n\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .5, .5))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_list_0.5[[j]] <- lsa::cosine(t(semantic_vectors))\n  }\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:length(cosines_list_0.5)){\n  corrplot::corrplot(cosines_list_0.5[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-15.png){width=672}\n:::\n:::\n\n\nActual simulation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_full_0.5 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .5, .5))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.5 <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_0.5),c(second_order_cosines))^2\n  }\nR2_full_0.5[i, ] <- R2_second\n}\n```\n:::\n\n\n### No discrepancy encoding (L = .7)\n\nVisualizing word-sentence similarities with increasing sentence frequency:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_0.7 <- list()\n\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .7, .7))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_list_0.7[[j]] <- lsa::cosine(t(semantic_vectors))\n  }\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:length(cosines_list_0.7)){\n  corrplot::corrplot(cosines_list_0.7[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-15.png){width=672}\n:::\n:::\n\n\nActual simulation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_full_0.7 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .7, .7))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_0.7),c(second_order_cosines))^2\n  }\nR2_full_0.7[i, ] <- R2_second\n}\n```\n:::\n\n\n### MINERVA-AL discrepancy encoding (L = 1.0)\n\nLet's visualize the progression of ITS encoding by visualizing the word-sentence similarities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_AL <- list()\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n  \nits_memory_AL <- prior_memory\n\nfor(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids_reordered)) {\n    current_sentence <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    echo <- colSums(as.numeric(cosine_x_to_m(current_sentence, its_memory_AL) ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(i in 1:length(cosines_list_AL)){\n  corrplot::corrplot(cosines_list_AL[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-15.png){width=672}\n:::\n:::\n\n\nWord-sentence similarities seem to stabilize around the 7th presentation of the sentences. Now, to run the actual simulations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_full_de <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory_AL <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids_reordered)) {\n    current_sentence <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    echo <- colSums(as.numeric(cosine_x_to_m(current_sentence, its_memory_AL) ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_ITS_AL <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_AL),c(second_order_cosines))^2\n  }\nR2_full_de[i, ] <- R2_second\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nits_memory_AL[1:10, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]       [,2]        [,3]       [,4]       [,5]\n [1,]  0.0000000 -1.0000000  1.00000000  1.0000000  0.0000000\n [2,] -1.0000000 -1.0000000 -1.00000000  1.0000000  1.0000000\n [3,] -1.0000000 -1.0000000  1.00000000  0.0000000 -1.0000000\n [4,]  1.0000000  1.0000000 -1.00000000  1.0000000 -1.0000000\n [5,] -0.9977807 -1.0000000  0.58611904  0.2091598 -0.5838998\n [6,]  0.6407729  0.3494726  0.02382425  0.4831534 -0.4895270\n [7,]  0.2735105  0.4661442 -0.99262772  0.6106153 -0.2800206\n [8,] -0.6453965 -0.5849472  0.54819815 -0.6710448  0.6492927\n [9,]  0.3771347  0.2623084 -0.59621311  0.2836291  0.7073388\n[10,] -0.1322865 -0.2228952  0.47443002 -0.2947606  0.1353827\n```\n:::\n\n```{.r .cell-code}\nits_memory_AL[1629:1639, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               [,1]          [,2]          [,3]          [,4]          [,5]\n [1,] -1.264035e-04 -3.462184e-05  1.011630e-04 -1.083267e-04 -2.263957e-04\n [2,]  3.710836e-05  2.076570e-05 -2.219678e-06  3.243088e-05 -3.216009e-05\n [3,]  1.062101e-05  3.316479e-05 -1.720831e-05 -2.593322e-05 -1.451305e-05\n [4,]  2.593332e-04  1.438617e-04  7.729729e-07  1.967663e-04 -1.925258e-04\n [5,] -8.150816e-05 -6.375385e-05  2.501117e-05 -2.393398e-05  8.336514e-06\n [6,]  5.451425e-05  3.212950e-05 -6.159582e-06  4.448722e-05 -4.163051e-05\n [7,]  4.718266e-04  5.000087e-04 -3.298992e-04 -1.171870e-04  3.096171e-04\n [8,] -1.563369e-04 -1.373852e-04  7.035068e-05 -1.849167e-05 -2.318766e-05\n [9,]  9.120338e-06  3.859228e-05  2.639440e-05 -6.061685e-05 -5.856319e-05\n[10,]  1.778721e-05  3.230424e-05  2.561530e-06 -3.144276e-05 -1.809289e-05\n[11,]  2.849881e-04  1.335925e-04  9.879837e-05  1.526164e-04 -1.822352e-04\n```\n:::\n:::\n\n\nThe MINERVA-AL discrepancy encoding code seems to be working well. The first few traces encoded have much higher values than the last few traces encoded (when each sentence had already been presented around 14 times before).\n\n### Plotting the graphs for comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_no_de <- data.frame(l_value = c(1.0, 0.7, 0.5, 0.3), rbind(colMeans(R2_full_1.0), colMeans(R2_full_0.7), colMeans(R2_full_0.5), colMeans(R2_full_0.3))) \ncolnames(R2_no_de) <- c(\"l_value\", 1:max_sent_freq)\nR2_no_de <- R2_no_de %>% pivot_longer(-l_value, names_to = \"sent_freq\", values_to = \"R2_second_order\")\n\nR2_de <- data.frame(sent_freq = 1:max_sent_freq, colMeans(R2_full_de))\ncolnames(R2_de) <- c(\"sent_freq\", \"R2_second_order\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + geom_line(data = R2_no_de, aes(x = factor(sent_freq, level = 1:15), y = R2_second_order, group = as.factor(l_value), color = as.factor(l_value))) + geom_line(data = R2_de, aes(x = factor(sent_freq, level = 1:15), y = R2_second_order, group = 1), linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nDoes this mean that, with discrepancy encoding, learning can never reach the same level as that without discrepancy encoding? Also, does this mean that discrepancy encoding even performs worse than no discrepancy encoding at a low learning rate (e.g. L = 0.3)?\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}