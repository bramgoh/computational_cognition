{
  "hash": "b27449cd96eab8dd16ccc0f6d26b4757",
  "result": {
    "markdown": "---\ntitle: \"Exploring discrepancy encoding with ITS (Part 6)\"\nauthor: \"Bram Goh\"\ndate: \"2023-05-05\"\ncategories: [code]\nimage: \"\"\n---\n\n\n# Using RIVs instead of one-hot encoding\n\nUsing one-hot encoding, we found that ITS without discrepancy encoding produced semantic word vectors that \"pull in\" elements from adjacent words, in line with the distributional hypothesis (i.e. word contexts make up the semantic meaning of the word). ITS vanillla also had lower reconstruction and higher intensity values (perhaps akin to lower accuracy with higher confidence in memory recall for individual words). In contrast, we found that ITS with MINERVA-AL discrepancy encoding produced semantic word vectors that exclusively comprised of the elements for the target word while inhibiting the most proximal elements of adjacent words. ITS-DE also had higher reconstruction and lower intensity values (perhaps akin to higher accuracy with lower confidence in memory recall for individual words).\n\nThus, it could be that ITS vanilla models pattern completion processes, while ITS-DE models pattern separation processes. This opens up possibilities for designing an experiment with human participants to compare the model's performance with human performance.\n\nThis week, I will replace the one-hot encoding with RIVs to confirm whether the above results are an artifact of the one-hot encoding procedure or whether there really is something here. Also, I will examine the zero-order correlations (correlation between semantic vector and actual word environmental vector). Additionally, I will randomize the order of the sentences presented.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(RsemanticLibrarian)\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'data.table'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    transpose\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(lsa)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: SnowballC\n```\n:::\n\n```{.r .cell-code}\nn_words <- 130\nband_width <- 10\n\n# Setup of word-sentence co-occurence matrix\nband_matrix <- Matrix::bandSparse(n = n_words,k=0:(band_width-1))\nband_matrix <- as.matrix(band_matrix)*1\n\n# ITS setup (each word appears in the same number of sentences)\ndictionary_words <- 1:n_words\n\nsentence_ids <- list()\nfor(i in 1:ncol(band_matrix)){\n  sentence_ids[[i]] <- which(band_matrix[,i] == 1)\n}\nsentence_ids <- sentence_ids[10:118]\n\nwords_to_plot <- c(10:109)\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  return(word_meaning)\n}\n\n# Function to create environment vectors with non-overlapping columns (modified from RsemanticLibrarian::sl_create_riv) - only uses 1s (no -1 values)\nno_overlap_riv <- function(dimensions, no_of_words, sparsity = 10){\n  if(sparsity %% 2 != 0) stop(\"sparsity must be even integer\")\n  temp_matrix <- matrix(0,ncol=dimensions,nrow=no_of_words)\n  for(i in 1:no_of_words){\n    temp_matrix[i, (i*10):(i*10+sparsity-1)] <- sample(c(rep(1, sparsity/2), rep(1, sparsity/2)))\n  }\n  return(temp_matrix)\n}\n\nrectify_vector <- function(x){\n  x[x > 1] <- 1\n  x[x < -1] <- -1\n  return(x)\n}\n\nnorm_vector <- function(x){\n  x <- x/(max(abs(x)))\n  return(x)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines <- cosine(t(band_matrix[words_to_plot,]))\nsecond_order_cosines <- cosine(t(first_order_cosines))\n\nmax_sent_freq <- 10\n```\n:::\n\n\n## No learning rate applied (L = 1.0)\n\n### No discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_zero <- numeric(length = max_sent_freq)\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\n\nenvironment_sub <- sl_create_riv(1500, length(dictionary_words), 10)\nzero_order_cosines <- cosine(t(environment_sub[words_to_plot, ]))\n\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory <- prior_memory\n\nintensities_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, current_sentence)\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS <- lsa::cosine(t(semantic_vectors))\n  R2_zero[j] <- cor(c(cosines_ITS), c(zero_order_cosines))^2\n  R2_first[j] <- cor(c(cosines_ITS),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS),c(second_order_cosines))^2\n  intensities_matrix[j, ] <- intensities\n  reconst_matrix[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_AL <- list()\n\nR2_zero_AL <- numeric(length = max_sent_freq)\nR2_first_AL <- numeric(length = max_sent_freq)\nR2_second_AL <- numeric(length = max_sent_freq)\n\nsentence_matrix <- c()\n\nnorm_echoes_matrix <- c()\n\nintensities_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- rectify_vector(echo)\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  sentence_matrix <- rbind(sentence_matrix, sentences)\n  norm_echoes_matrix <- rbind(norm_echoes_matrix, normalized_echoes)\n  intensities_matrix_AL[j, ] <- intensities\n  reconst_matrix_AL[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n  R2_zero_AL[j] <- cor(c(cosines_list_AL[[j]]),c(zero_order_cosines))^2\n  R2_first_AL[j] <- cor(c(cosines_list_AL[[j]]),c(first_order_cosines))^2\n  R2_second_AL[j] <- cor(c(cosines_list_AL[[j]]),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot::corrplot(cosines_list_AL[[10]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Comparing no discrepancy encoding to discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix), rowMeans(intensities_matrix), R2_zero, R2_first, R2_second)\ncolnames(no_de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_perfect <- no_de_df_perfect %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL), rowMeans(intensities_matrix_AL), R2_zero_AL, R2_first_AL, R2_second_AL)\ncolnames(de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nde_df_perfect <- de_df_perfect %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_perfect <- bind_rows(no_de_df_perfect, de_df_perfect)\nggplot(compare_df_perfect, aes(x = sent_freq, y = reconstruction, color = condition)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(compare_df_perfect, aes(x = sent_freq, y = intensity, color = condition)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(compare_df_perfect, aes(x = sent_freq, y = R_squared, color = order_of_sim)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n:::\n\n\n## Applying learning rate of .7\n\n### No discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL <- .7\n\nR2_zero_0.7 <- numeric(length = max_sent_freq)\nR2_first_0.7 <- numeric(length = max_sent_freq)\nR2_second_0.7 <- numeric(length = max_sent_freq)\n\nits_memory <- prior_memory\n\nintensities_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, (current_sentence*(rbinom(length(current_sentence), 1, L))))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_0.7[j] <- cor(c(cosines_ITS_0.7),c(zero_order_cosines))^2\n  R2_first_0.7[j] <- cor(c(cosines_ITS_0.7),c(first_order_cosines))^2\n  R2_second_0.7[j] <- cor(c(cosines_ITS_0.7),c(second_order_cosines))^2\n  intensities_matrix_0.7[j, ] <- intensities\n  reconst_matrix_0.7[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_zero_AL_0.7 <- numeric(length = max_sent_freq)\nR2_first_AL_0.7 <- numeric(length = max_sent_freq)\nR2_second_AL_0.7 <- numeric(length = max_sent_freq)\n\nsentence_matrix_0.7 <- c()\n\nnorm_echoes_matrix_0.7 <- c()\n\nintensities_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL_0.7 <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL_0.7)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL_0.7))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- rectify_vector(echo)\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL_0.7 <- rbind(its_memory_AL_0.7, ((current_sentence - echo_norm)*(rbinom(length(current_sentence), 1, L))))\n    }\n  sentence_matrix_0.7 <- rbind(sentence_matrix_0.7, sentences)\n  norm_echoes_matrix_0.7 <- rbind(norm_echoes_matrix_0.7, normalized_echoes)\n  intensities_matrix_AL_0.7[j, ] <- intensities\n  reconst_matrix_AL_0.7[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL_0.7,\n                                                tau=3)\n  cosines_AL_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(zero_order_cosines))^2\n  R2_first_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(first_order_cosines))^2\n  R2_second_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### Comparing no discrepancy encoding to discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix_0.7), rowMeans(intensities_matrix_0.7), R2_zero_0.7, R2_first_0.7, R2_second_0.7)\ncolnames(no_de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_0.7 <- no_de_df_0.7 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL_0.7), rowMeans(intensities_matrix_AL_0.7), R2_zero_AL_0.7, R2_first_AL_0.7, R2_second_AL_0.7)\ncolnames(de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nde_df_0.7 <- de_df_0.7 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_0.7 <- bind_rows(no_de_df_0.7, de_df_0.7)\nggplot(compare_df_0.7, aes(x = sent_freq, y = reconstruction, color = condition)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(compare_df_0.7, aes(x = sent_freq, y = intensity, color = condition)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(compare_df_0.7, aes(x = sent_freq, y = R_squared, color = order_of_sim)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n:::\n\n\n## In summary...\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomplete_df <- bind_rows(compare_df_perfect, compare_df_0.7)\ncomplete_df$learning_rate <- factor(complete_df$learning_rate, levels = c(\"1.0\", \"0.7\"))\nggplot(complete_df, aes(x = sent_freq, y = reconstruction, color = condition, linetype = learning_rate)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(complete_df, aes(x = sent_freq, y = intensity, color = condition, linetype = learning_rate)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(complete_df, aes(x = sent_freq, y = R_squared, color = order_of_sim, linetype = learning_rate)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n:::\n\n\nThe zero-order correlations have a negative, almost mirror-like relationship with the first- and second-order correlations. This seems different from the almost-parallel curves we were getting with one-hot encoding?\n\nReordering the sentences leads to lower zero-order correlations in ITS-DE.\n\nAlso, the decision to rectify or normalize the target sentence and the echo matters. In Matt's simulations, we have chosen to rectify the target sentence and normalize the echo. Here, I've attempted to rectify both sentence and echo (to keep the way we handle values consistent). As a result, the intensity values seem to plateau (and even start to dip with increasing frequency of presentation) when L = 1, and increases very gradually when L = 0.7.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}