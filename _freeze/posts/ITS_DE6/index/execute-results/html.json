{
  "hash": "2117ab3ecbb44974d3bb794ee7bf8908",
  "result": {
    "markdown": "---\ntitle: \"Exploring discrepancy encoding with ITS (Part 6)\"\nauthor: \"Bram Goh\"\ndate: \"2023-05-05\"\ncategories: [code]\nimage: \"\"\n---\n\n\n# Using RIVs instead of one-hot encoding\n\nUsing one-hot encoding, we found that ITS without discrepancy encoding produced semantic word vectors that \"pull in\" elements from adjacent words, in line with the distributional hypothesis (i.e. word contexts make up the semantic meaning of the word). ITS vanillla also had lower reconstruction and higher intensity values (perhaps akin to lower accuracy with higher confidence in memory recall for individual words). In contrast, we found that ITS with MINERVA-AL discrepancy encoding produced semantic word vectors that exclusively comprised of the elements for the target word while inhibiting the most proximal elements of adjacent words. ITS-DE also had higher reconstruction and lower intensity values (perhaps akin to higher accuracy with lower confidence in memory recall for individual words).\n\nThus, it could be that ITS vanilla models pattern completion processes, while ITS-DE models pattern separation processes. This opens up possibilities for designing an experiment with human participants to compare the model's performance with human performance.\n\nThis week, I will replace the one-hot encoding with RIVs to confirm whether the above results are an artifact of the one-hot encoding procedure or whether there really is something here. Also, I will examine the zero-order correlations (correlation between semantic vector and actual word environmental vector). Additionally, I will randomize the order of the sentences presented.\n\nMost importantly, I will be comparing the rectifying versus normalization procedures for both the target sentence and the echo. In previous simulations, we have chosen to rectify the target sentence and normalize the echo, but perhaps a consistent approach to both sentence and echo is more defensible?\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(RsemanticLibrarian)\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'data.table'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    transpose\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(lsa)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: SnowballC\n```\n:::\n\n```{.r .cell-code}\nn_words <- 130\nband_width <- 10\n\n# Setup of word-sentence co-occurence matrix\nband_matrix <- Matrix::bandSparse(n = n_words,k=0:(band_width-1))\nband_matrix <- as.matrix(band_matrix)*1\n\n# ITS setup (each word appears in the same number of sentences)\ndictionary_words <- 1:n_words\n\nsentence_ids <- list()\nfor(i in 1:ncol(band_matrix)){\n  sentence_ids[[i]] <- which(band_matrix[,i] == 1)\n}\nsentence_ids <- sentence_ids[10:118]\n\nwords_to_plot <- c(10:109)\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  return(word_meaning)\n}\n\n# Function to create environment vectors with non-overlapping columns (modified from RsemanticLibrarian::sl_create_riv) - only uses 1s (no -1 values)\nno_overlap_riv <- function(dimensions, no_of_words, sparsity = 10){\n  if(sparsity %% 2 != 0) stop(\"sparsity must be even integer\")\n  temp_matrix <- matrix(0,ncol=dimensions,nrow=no_of_words)\n  for(i in 1:no_of_words){\n    temp_matrix[i, (i*10):(i*10+sparsity-1)] <- sample(c(rep(1, sparsity/2), rep(1, sparsity/2)))\n  }\n  return(temp_matrix)\n}\n\nrectify_vector <- function(x){\n  x[x > 1] <- 1\n  x[x < -1] <- -1\n  return(x)\n}\n\nnorm_vector <- function(x){\n  x <- x/(max(abs(x)))\n  return(x)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines <- cosine(t(band_matrix[words_to_plot,]))\nsecond_order_cosines <- cosine(t(first_order_cosines))\n\nenvironment_sub <- sl_create_riv(1500, length(dictionary_words), 10)\nzero_order_cosines <- cosine(t(environment_sub[words_to_plot, ]))\n\nmax_sent_freq <- 10\n```\n:::\n\n\n## Rectifying procedure\n\n### No discrepancy encoding (L = 1.0)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_zero <- numeric(length = max_sent_freq)\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\n\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory <- prior_memory\n\nintensities_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, current_sentence)\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS <- lsa::cosine(t(semantic_vectors))\n  R2_zero[j] <- cor(c(cosines_ITS), c(zero_order_cosines))^2\n  R2_first[j] <- cor(c(cosines_ITS),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS),c(second_order_cosines))^2\n  intensities_matrix[j, ] <- intensities\n  reconst_matrix[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding (L = 1.0)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_AL <- list()\n\nR2_zero_AL <- numeric(length = max_sent_freq)\nR2_first_AL <- numeric(length = max_sent_freq)\nR2_second_AL <- numeric(length = max_sent_freq)\n\nsentence_matrix <- c()\n\nnorm_echoes_matrix <- c()\n\nintensities_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- rectify_vector(echo)\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  sentence_matrix <- rbind(sentence_matrix, sentences)\n  norm_echoes_matrix <- rbind(norm_echoes_matrix, normalized_echoes)\n  intensities_matrix_AL[j, ] <- intensities\n  reconst_matrix_AL[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n  R2_zero_AL[j] <- cor(c(cosines_list_AL[[j]]),c(zero_order_cosines))^2\n  R2_first_AL[j] <- cor(c(cosines_list_AL[[j]]),c(first_order_cosines))^2\n  R2_second_AL[j] <- cor(c(cosines_list_AL[[j]]),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot::corrplot(cosines_list_AL[[10]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix), rowMeans(intensities_matrix), R2_zero, R2_first, R2_second)\ncolnames(no_de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_perfect <- no_de_df_perfect %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL), rowMeans(intensities_matrix_AL), R2_zero_AL, R2_first_AL, R2_second_AL)\ncolnames(de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nde_df_perfect <- de_df_perfect %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_perfect <- bind_rows(no_de_df_perfect, de_df_perfect)\n```\n:::\n\n\n### No discrepancy encoding (L = 0.7)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL <- .7\n\nR2_zero_0.7 <- numeric(length = max_sent_freq)\nR2_first_0.7 <- numeric(length = max_sent_freq)\nR2_second_0.7 <- numeric(length = max_sent_freq)\n\nits_memory <- prior_memory\n\nintensities_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, (current_sentence*(rbinom(length(current_sentence), 1, L))))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_0.7[j] <- cor(c(cosines_ITS_0.7),c(zero_order_cosines))^2\n  R2_first_0.7[j] <- cor(c(cosines_ITS_0.7),c(first_order_cosines))^2\n  R2_second_0.7[j] <- cor(c(cosines_ITS_0.7),c(second_order_cosines))^2\n  intensities_matrix_0.7[j, ] <- intensities\n  reconst_matrix_0.7[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding (L = 0.7)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_zero_AL_0.7 <- numeric(length = max_sent_freq)\nR2_first_AL_0.7 <- numeric(length = max_sent_freq)\nR2_second_AL_0.7 <- numeric(length = max_sent_freq)\n\nsentence_matrix_0.7 <- c()\n\nnorm_echoes_matrix_0.7 <- c()\n\nintensities_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL_0.7 <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL_0.7)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL_0.7))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- rectify_vector(echo)\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL_0.7 <- rbind(its_memory_AL_0.7, ((current_sentence - echo_norm)*(rbinom(length(current_sentence), 1, L))))\n    }\n  sentence_matrix_0.7 <- rbind(sentence_matrix_0.7, sentences)\n  norm_echoes_matrix_0.7 <- rbind(norm_echoes_matrix_0.7, normalized_echoes)\n  intensities_matrix_AL_0.7[j, ] <- intensities\n  reconst_matrix_AL_0.7[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL_0.7,\n                                                tau=3)\n  cosines_AL_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(zero_order_cosines))^2\n  R2_first_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(first_order_cosines))^2\n  R2_second_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix_0.7), rowMeans(intensities_matrix_0.7), R2_zero_0.7, R2_first_0.7, R2_second_0.7)\ncolnames(no_de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_0.7 <- no_de_df_0.7 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL_0.7), rowMeans(intensities_matrix_AL_0.7), R2_zero_AL_0.7, R2_first_AL_0.7, R2_second_AL_0.7)\ncolnames(de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nde_df_0.7 <- de_df_0.7 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_0.7 <- bind_rows(no_de_df_0.7, de_df_0.7)\n```\n:::\n\n\n### No discrepancy encoding (L = 0.4)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL <- .4\n\nR2_zero_0.4 <- numeric(length = max_sent_freq)\nR2_first_0.4 <- numeric(length = max_sent_freq)\nR2_second_0.4 <- numeric(length = max_sent_freq)\n\nits_memory <- prior_memory\n\nintensities_matrix_0.4 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_0.4 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, (current_sentence*(rbinom(length(current_sentence), 1, L))))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.4 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_0.4[j] <- cor(c(cosines_ITS_0.4),c(zero_order_cosines))^2\n  R2_first_0.4[j] <- cor(c(cosines_ITS_0.4),c(first_order_cosines))^2\n  R2_second_0.4[j] <- cor(c(cosines_ITS_0.4),c(second_order_cosines))^2\n  intensities_matrix_0.4[j, ] <- intensities\n  reconst_matrix_0.4[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding (L = 0.4)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_zero_AL_0.4 <- numeric(length = max_sent_freq)\nR2_first_AL_0.4 <- numeric(length = max_sent_freq)\nR2_second_AL_0.4 <- numeric(length = max_sent_freq)\n\nsentence_matrix_0.4 <- c()\n\nnorm_echoes_matrix_0.4 <- c()\n\nintensities_matrix_AL_0.4 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL_0.4 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL_0.4 <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- rectify_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL_0.4)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL_0.4))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- rectify_vector(echo)\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL_0.4 <- rbind(its_memory_AL_0.4, ((current_sentence - echo_norm)*(rbinom(length(current_sentence), 1, L))))\n    }\n  sentence_matrix_0.4 <- rbind(sentence_matrix_0.4, sentences)\n  norm_echoes_matrix_0.4 <- rbind(norm_echoes_matrix_0.4, normalized_echoes)\n  intensities_matrix_AL_0.4[j, ] <- intensities\n  reconst_matrix_AL_0.4[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL_0.4,\n                                                tau=3)\n  cosines_AL_0.4 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_AL_0.4[j] <- cor(c(cosines_AL_0.4),c(zero_order_cosines))^2\n  R2_first_AL_0.4[j] <- cor(c(cosines_AL_0.4),c(first_order_cosines))^2\n  R2_second_AL_0.4[j] <- cor(c(cosines_AL_0.4),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_0.4 <- data.frame(learning_rate = \"0.4\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix_0.4), rowMeans(intensities_matrix_0.4), R2_zero_0.4, R2_first_0.4, R2_second_0.4)\ncolnames(no_de_df_0.4) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_0.4 <- no_de_df_0.4 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_0.4 <- data.frame(learning_rate = \"0.4\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL_0.4), rowMeans(intensities_matrix_AL_0.4), R2_zero_AL_0.4, R2_first_AL_0.4, R2_second_AL_0.4)\ncolnames(de_df_0.4) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nde_df_0.4 <- de_df_0.4 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_0.4 <- bind_rows(no_de_df_0.4, de_df_0.4)\n```\n:::\n\n\n### Summary for rectifying procedure\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomplete_df <- bind_rows(compare_df_perfect, compare_df_0.7, compare_df_0.4)\ncomplete_df$learning_rate <- factor(complete_df$learning_rate, levels = c(\"1.0\", \"0.7\", \"0.4\"))\nggplot(complete_df, aes(x = sent_freq, y = reconstruction, color = learning_rate)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(complete_df, aes(x = sent_freq, y = intensity, color = learning_rate)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(complete_df, aes(x = sent_freq, y = R_squared, color = order_of_sim, linetype = condition)) + geom_line() + facet_wrap(vars(learning_rate))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n:::\n\n\nReordering the sentences leads to lower zero-order correlations in ITS-DE.\n\nWith ITS-DE, the intensity values seem to plateau (and even start to dip with increasing frequency of presentation) when L = 1, and as L decreases the rate of increase grows.\n\nThe zero-order correlations have a negative, almost mirror-like relationship with the first- and second-order correlations. This seems different from the almost-parallel curves we were getting before. Is this due to removing the one-hot encoding procedure or due to rectifying both sentence and echo?\n\n## Normalization procedure\n\n### No discrepancy encoding (L = 1.0)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_zero <- numeric(length = max_sent_freq)\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\n\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory <- prior_memory\n\nintensities_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- norm_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, current_sentence)\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS <- lsa::cosine(t(semantic_vectors))\n  R2_zero[j] <- cor(c(cosines_ITS), c(zero_order_cosines))^2\n  R2_first[j] <- cor(c(cosines_ITS),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS),c(second_order_cosines))^2\n  intensities_matrix[j, ] <- intensities\n  reconst_matrix[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding (L = 1.0)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncosines_list_AL <- list()\n\nR2_zero_AL <- numeric(length = max_sent_freq)\nR2_first_AL <- numeric(length = max_sent_freq)\nR2_second_AL <- numeric(length = max_sent_freq)\n\nsentence_matrix <- c()\n\nnorm_echoes_matrix <- c()\n\nintensities_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- norm_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- norm_vector(echo)\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  sentence_matrix <- rbind(sentence_matrix, sentences)\n  norm_echoes_matrix <- rbind(norm_echoes_matrix, normalized_echoes)\n  intensities_matrix_AL[j, ] <- intensities\n  reconst_matrix_AL[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n  R2_zero_AL[j] <- cor(c(cosines_list_AL[[j]]),c(zero_order_cosines))^2\n  R2_first_AL[j] <- cor(c(cosines_list_AL[[j]]),c(first_order_cosines))^2\n  R2_second_AL[j] <- cor(c(cosines_list_AL[[j]]),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrplot::corrplot(cosines_list_AL[[10]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThis is quite different from what we were getting with the rectifying procedure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix), rowMeans(intensities_matrix), R2_zero, R2_first, R2_second)\ncolnames(no_de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_perfect <- no_de_df_perfect %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL), rowMeans(intensities_matrix_AL), R2_zero_AL, R2_first_AL, R2_second_AL)\ncolnames(de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nde_df_perfect <- de_df_perfect %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_perfect <- bind_rows(no_de_df_perfect, de_df_perfect)\n```\n:::\n\n\n### No discrepancy encoding (L = 0.7)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL <- .7\n\nR2_zero_0.7 <- numeric(length = max_sent_freq)\nR2_first_0.7 <- numeric(length = max_sent_freq)\nR2_second_0.7 <- numeric(length = max_sent_freq)\n\nits_memory <- prior_memory\n\nintensities_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- norm_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, (current_sentence*(rbinom(length(current_sentence), 1, L))))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_0.7[j] <- cor(c(cosines_ITS_0.7),c(zero_order_cosines))^2\n  R2_first_0.7[j] <- cor(c(cosines_ITS_0.7),c(first_order_cosines))^2\n  R2_second_0.7[j] <- cor(c(cosines_ITS_0.7),c(second_order_cosines))^2\n  intensities_matrix_0.7[j, ] <- intensities\n  reconst_matrix_0.7[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding (L = 0.7)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_zero_AL_0.7 <- numeric(length = max_sent_freq)\nR2_first_AL_0.7 <- numeric(length = max_sent_freq)\nR2_second_AL_0.7 <- numeric(length = max_sent_freq)\n\nsentence_matrix_0.7 <- c()\n\nnorm_echoes_matrix_0.7 <- c()\n\nintensities_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL_0.7 <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- norm_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL_0.7)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL_0.7))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- norm_vector(echo)\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL_0.7 <- rbind(its_memory_AL_0.7, ((current_sentence - echo_norm)*(rbinom(length(current_sentence), 1, L))))\n    }\n  sentence_matrix_0.7 <- rbind(sentence_matrix_0.7, sentences)\n  norm_echoes_matrix_0.7 <- rbind(norm_echoes_matrix_0.7, normalized_echoes)\n  intensities_matrix_AL_0.7[j, ] <- intensities\n  reconst_matrix_AL_0.7[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL_0.7,\n                                                tau=3)\n  cosines_AL_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(zero_order_cosines))^2\n  R2_first_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(first_order_cosines))^2\n  R2_second_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix_0.7), rowMeans(intensities_matrix_0.7), R2_zero_0.7, R2_first_0.7, R2_second_0.7)\ncolnames(no_de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_0.7 <- no_de_df_0.7 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL_0.7), rowMeans(intensities_matrix_AL_0.7), R2_zero_AL_0.7, R2_first_AL_0.7, R2_second_AL_0.7)\ncolnames(de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nde_df_0.7 <- de_df_0.7 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_0.7 <- bind_rows(no_de_df_0.7, de_df_0.7)\n```\n:::\n\n\n### No discrepancy encoding (L = 0.4)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL <- .4\n\nR2_zero_0.4 <- numeric(length = max_sent_freq)\nR2_first_0.4 <- numeric(length = max_sent_freq)\nR2_second_0.4 <- numeric(length = max_sent_freq)\n\nits_memory <- prior_memory\n\nintensities_matrix_0.4 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_0.4 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- norm_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, (current_sentence*(rbinom(length(current_sentence), 1, L))))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.4 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_0.4[j] <- cor(c(cosines_ITS_0.4),c(zero_order_cosines))^2\n  R2_first_0.4[j] <- cor(c(cosines_ITS_0.4),c(first_order_cosines))^2\n  R2_second_0.4[j] <- cor(c(cosines_ITS_0.4),c(second_order_cosines))^2\n  intensities_matrix_0.4[j, ] <- intensities\n  reconst_matrix_0.4[j, ] <- reconstruction\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n\n### MINERVA-AL discrepancy encoding (L = 0.4)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR2_zero_AL_0.4 <- numeric(length = max_sent_freq)\nR2_first_AL_0.4 <- numeric(length = max_sent_freq)\nR2_second_AL_0.4 <- numeric(length = max_sent_freq)\n\nsentence_matrix_0.4 <- c()\n\nnorm_echoes_matrix_0.4 <- c()\n\nintensities_matrix_AL_0.4 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL_0.4 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL_0.4 <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- norm_vector(colSums(environment_sub[sentence_ids_reordered[[k]],]))\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL_0.4)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL_0.4))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- norm_vector(echo)\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL_0.4 <- rbind(its_memory_AL_0.4, ((current_sentence - echo_norm)*(rbinom(length(current_sentence), 1, L))))\n    }\n  sentence_matrix_0.4 <- rbind(sentence_matrix_0.4, sentences)\n  norm_echoes_matrix_0.4 <- rbind(norm_echoes_matrix_0.4, normalized_echoes)\n  intensities_matrix_AL_0.4[j, ] <- intensities\n  reconst_matrix_AL_0.4[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL_0.4,\n                                                tau=3)\n  cosines_AL_0.4 <- lsa::cosine(t(semantic_vectors))\n  R2_zero_AL_0.4[j] <- cor(c(cosines_AL_0.4),c(zero_order_cosines))^2\n  R2_first_AL_0.4[j] <- cor(c(cosines_AL_0.4),c(first_order_cosines))^2\n  R2_second_AL_0.4[j] <- cor(c(cosines_AL_0.4),c(second_order_cosines))^2\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nno_de_df_0.4 <- data.frame(learning_rate = \"0.4\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix_0.4), rowMeans(intensities_matrix_0.4), R2_zero_0.4, R2_first_0.4, R2_second_0.4)\ncolnames(no_de_df_0.4) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_0.4 <- no_de_df_0.4 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_0.4 <- data.frame(learning_rate = \"0.4\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL_0.4), rowMeans(intensities_matrix_AL_0.4), R2_zero_AL_0.4, R2_first_AL_0.4, R2_second_AL_0.4)\ncolnames(de_df_0.4) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"zero_order_R2\", \"first_order_R2\", \"second_order_R2\")\nde_df_0.4 <- de_df_0.4 %>% pivot_longer(c(zero_order_R2, first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_0.4 <- bind_rows(no_de_df_0.4, de_df_0.4)\n```\n:::\n\n\n### Summary for rectifying procedure\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomplete_df <- bind_rows(compare_df_perfect, compare_df_0.7, compare_df_0.4)\ncomplete_df$learning_rate <- factor(complete_df$learning_rate, levels = c(\"1.0\", \"0.7\", \"0.4\"))\nggplot(complete_df, aes(x = sent_freq, y = reconstruction, color = learning_rate)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(complete_df, aes(x = sent_freq, y = intensity, color = learning_rate)) + geom_line() + facet_wrap(vars(condition))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(complete_df, aes(x = sent_freq, y = R_squared, color = order_of_sim, linetype = condition)) + geom_line() + facet_wrap(vars(learning_rate))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-3.png){width=672}\n:::\n:::\n\n\nWith normalization, reconstruction in ITS-DE [increases]{.underline} with decreasing L and is overall lower than that in ITS vanilla. This is in contrast with the results in the rectifying procedure, as reconstruction there decreases with lower L and is overall higher than that in ITS vanilla.\n\nIntensity values in the normalization procedure are the same regardless of L and rises minimally as sentence frequency increases.\n\nInstead of the mirror-effect between zero- and first-/second-order similarities, all the R-squared values are directionally in tandem, similar to what we were getting last week with the one-hot encoding procedure. Generally, they all increase with decreasing L.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}