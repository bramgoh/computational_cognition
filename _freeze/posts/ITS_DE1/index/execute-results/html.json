{
  "hash": "818b3686c19279d569c056fa78f689be",
  "result": {
    "markdown": "---\ntitle: \"Exploring discrepancy encoding with ITS (Part 1)\"\nauthor: \"Bram Goh\"\ndate: \"2023-03-31\"\ncategories: [code]\nimage: \"toy_no_de.png\"\n---\n\n\n# Implementing discrepancy encoding in a toy language\n\nGoals for this exercise:\n\n-   Replicate toy language involving the homonym \"break\" from Jamieson et al. (2018)\n\n-   Implement discrepancy encoding in Jamieson et al.'s (2018) toy language\n\n## Replicating \"break\" toy language (without discrepancy encoding)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RsemanticLibrarian)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(lsa)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: SnowballC\n```\n:::\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Create dictionary (modified from RsemanticLibrarian)\n\ncorpus_dictionary <- function(words){\n  neat_words <- words %>%\n                as.character() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                qdapRegex::rm_white_lead_trail() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                unique()\n  return(neat_words)\n}\n\n# Functions to ensure strings only contain words (modified from RsemanticLibrarian)\nclean <- function(words){\n  if(length(words) == 1) {\n    neat_words <- words %>%\n      as.character() %>%\n      strsplit(split=\" \") %>%\n      unlist() %>%\n      qdapRegex::rm_white_lead_trail() %>%\n      strsplit(split=\" \") %>%\n      unlist() \n    return(neat_words)\n  }\n}\n\nclean_vector <- function(words){\n  return(lapply(unlist(strsplit(words,split=\"[.]\")), clean))\n}\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    echo <- get_echo(probe, sentence_memory, tau = tau, output = \"echo\", type = \"cosine\")\n    word_meaning[i, ] <- echo\n  }\n  row.names(word_meaning) <- strings\n  \n  return(word_meaning)\n}\n\n# Find top n similar words in terms of semantic meaning\nfind_similar_words <- function(meaning_matrix, dictionary, word_to_find, n) {\n  similarities <- cosine_x_to_m(meaning_matrix[which(dictionary$word == word_to_find), ], meaning_matrix)\nsimilarities_df <- data.frame(dictionary, similarities = similarities)\nsim_top <- similarities_df %>% arrange(desc(similarities)) %>% slice(1:n)\nreturn(sim_top)\n}\n```\n:::\n\n\n### Setting up toy language (with 1000 sentences)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl_value <- 0.7\n\nnoun_human <- c(\"man\", \"woman\")\nverb_vehicle <- c(\"stop\", \"break\")\nverb_dinner <- c(\"smash\", \"break\")\nverb_news <- c(\"report\", \"break\")\nnoun_vehicle <- c(\"car\", \"truck\")\nnoun_dinner <- c(\"plate\", \"glass\")\nnoun_news <- c(\"story\", \"news\")\n\nsentences <- data.frame(sentence_no = 1:1000, sentence = NA)\nsentence_frames <- c(\"vehicle\", \"dinner\", \"news\")\nfor(i in 1:nrow(sentences)) {\n  current_frame <- sample(sentence_frames, 1)\n  if(current_frame == \"vehicle\"){\n  sentences$sentence[i] <- paste(sample(noun_human, 1), sample(verb_vehicle, 1), sample(noun_vehicle, 1))\n  } else if(current_frame == \"dinner\"){\n  sentences$sentence[i] <- paste(sample(noun_human, 1), sample(verb_dinner, 1), sample(noun_dinner, 1))\n  } else {\n  sentences$sentence[i] <- paste(sample(noun_human, 1), sample(verb_news, 1), sample(noun_news, 1))\n  }\n}\n\ndictionary <- unique(c(noun_human, verb_vehicle, verb_dinner, verb_news, noun_vehicle, noun_dinner, noun_news))\nclean_sentences <- clean_vector(sentences$sentence)\nsentence_ids <- sl_word_ids_sentences(clean_sentences, dictionary)\nenv_vectors <- sl_create_riv(20000, length(dictionary), 16)\n\n# Setting up prior memory (cannot start with other three-word sentences, as they are too sparse. Have to start with less sparse memory to avoid NaN values). Then encoding sentences without discrepancy encoding (also applying L value)\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(env_vectors), replace = TRUE)))\ntoy_memory <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(sentence_ids)))\nfor(i in 1:length(sentence_ids)){\n  current_sentence <- colSums(env_vectors[sentence_ids[[i]], ])\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))\n  toy_memory[4+i, ] <- current_sentence*learning_vector\n}\n\n# Get echoes (semantic meaning vectors)\nmeaning <- subset_semantic_vectors(dictionary, dictionary, env_vectors, toy_memory, tau = 3)\n\ncosine_toy <- cosine(t(meaning))\n\ncorrplot::corrplot(cosine_toy,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmds_toy <- cmdscale(1-cosine_toy) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_toy) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_toy, aes(x = Dim1, y = Dim2, label = dictionary)) + geom_text() + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nEven with just 1000 sentences (much fewer than the 20,000 in Jamieson et al.), I was able to reproduce the MDS solution.\n\nI have a question about what the distances mean. They don't seem to measure \"direct\" co-occurence, since man/woman, car/truck, glass/plate and news/story never appear in the same sentences together. Do they measure a higher-order similarity, since for e.g. stop only occurs with car/truck?\n\n## Implementing MINERVA-AL discrepancy encoding in \"break\" toy language\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntau_de <- 3\n\n# Setting up prior memory and discrepancy encoding (with L value)\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(env_vectors), replace = TRUE)))\ntoy_memory_de <- rbind(prior_memory, matrix(0, nrow = length(sentence_ids), ncol = ncol(env_vectors)))\n\nfor(i in 1:length(sentence_ids)){\n  current_sentence <- colSums(env_vectors[sentence_ids[[i]], ])\n  echo <- get_echo(current_sentence, toy_memory_de, tau = tau_de, output = \"echo\", type = \"cosine\")\n  echo_norm <- echo/max(abs(echo))\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))\n  discrep <- current_sentence - echo_norm\n  toy_memory_de[4+i, ] <- discrep * learning_vector\n  }\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get echoes (semantic meaning vectors)\nmeaning_de <- subset_semantic_vectors(dictionary, dictionary, env_vectors, toy_memory_de, tau = 3)\n\ncosine_toy_de <- cosine(t(meaning_de))\n\ncorrplot::corrplot(cosine_toy_de,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nClustering is less obvious.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmds_toy_de <- cmdscale(1-cosine_toy_de) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_toy_de) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_toy_de, aes(x = Dim1, y = Dim2, label = dictionary)) + geom_text() + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe distinction between some of the clusters is lost!\n\n## Implementing Collins et al.'s (2020) discrepancy encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl_max <- 1.0\n\n# Setting up prior memory and discrepancy encoding (with L value)\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(env_vectors), replace = TRUE)))\ntoy_memory_collins <- rbind(prior_memory, matrix(0, nrow = length(sentence_ids), ncol = ncol(env_vectors)))\n\nfor(i in 1:length(sentence_ids)){\n  current_sentence <- colSums(env_vectors[sentence_ids[[i]], ])\n  current_intensity <- get_echo(current_sentence, toy_memory_collins, tau = tau_de, output = \"intensity\", type = \"cosine\")\n    l_collins <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_collins, l_collins))\n  toy_memory_collins[4+i, ] <- current_sentence * learning_vector\n  }\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get echoes (semantic meaning vectors)\nmeaning_collins <- subset_semantic_vectors(dictionary, dictionary, env_vectors, toy_memory_collins, tau = 3)\n\ncosine_toy_collins <- cosine(t(meaning_collins))\n\ncorrplot::corrplot(cosine_toy_collins,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThere seems to be clustering!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotly)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'plotly'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:graphics':\n\n    layout\n```\n:::\n\n```{.r .cell-code}\nmds_toy_collins <- cmdscale(1-cosine_toy_collins) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_toy_collins) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_toy_collins, aes(x = Dim1, y = Dim2, label = dictionary)) + geom_text() + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe words are clustered differently but are still in discernible clusters. Stop, report and smash seem to be roughly equidistant from break, as in the original ITS diagram. However, as opposed to the ITS diagram, man and woman are far from each other. Perhaps a higher-order dimensionality (e.g. 3D) plot would make this more sensible?\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}