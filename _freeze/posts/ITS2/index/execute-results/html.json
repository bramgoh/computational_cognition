{
  "hash": "eefc376a64e4b584d50f82dbcdb85e0a",
  "result": {
    "markdown": "---\ntitle: \"ITS (part 2)\"\nauthor: \"Bram Goh\"\ndate: \"2023-03-24\"\ncategories: [code]\nimage: \"matt_mds.png\"\n---\n\n\n# Replicating taxonomic structure from Jamieson et al. (2018)\n\nThe goal is to replicate the top panel of Figure 8 in Jamieson et al. (2018): the two-dimensional MDS solutions for various words from different categories (shown in Table 2 in the paper).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RsemanticLibrarian)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'NLP'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'data.table'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    transpose\n```\n:::\n\n```{.r .cell-code}\nlibrary(lsa)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: SnowballC\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(RsemanticLibrarian::cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Function to replace words with their dictionary ids\nreplace_word_with_id <- function(sentence_df, dict_df, max_sent_length){\n  sentence_id_matrix <- c()\n  for(i in 1:length(sentence_df$sentence)){\n    current_sent <- rep(0, max_sent_length)\n    words_list <- tokenize_words(sentence_df$sentence[i])\n    words <- words_list[[1]]\n    for(j in 1:length(words)){\n      current_id <- which(dict_df$word == words[j])\n      if(length(current_id) == 0){\n        current_id <- 0\n      }\n      current_sent[j] <- current_id\n    }\n    sentence_id_matrix <- rbind(sentence_id_matrix, current_sent)\n  }\n  return(sentence_id_matrix)\n}\n\n# Function to generate random environment vectors for each word in dictionary\nmake_env_vectors <- function(dict_df, length, sparsity) {\n  environment_matrix <- matrix(0, nrow = nrow(dict_df), ncol = length)\n  for(i in 1:nrow(dict_df)){\n    current <- rep(0, length)\n    positions <- sample(1:length, sparsity)\n    ones_vector <- c(rep(1, sparsity/2), rep(-1, sparsity/2))\n    ordered_ones_vector <- sample(ones_vector, sparsity, replace = FALSE)\n    for(j in 1:length(positions)){\n      current[positions[j]] <- ordered_ones_vector[j]\n    }\n    environment_matrix[i, ] <- current\n  }\n  return(environment_matrix)\n}\n\n# Function to add up environment vectors for all words in a sentence\nmake_sentence_vectors <- function(compiled_word_ids, env_matrix) {\n  sentence_ids <- matrix(0, nrow = nrow(compiled_word_ids), ncol = ncol(env_matrix))\n  for(i in 1:nrow(compiled_word_ids)){\n    current_word_ids <- compiled_word_ids[i, ]\n    current_sentence <- rep(0, ncol(env_matrix))\n    for(j in 1:length(current_word_ids)){\n      if(current_word_ids[j] == 0){\n        temp_vector <- rep(0, ncol(env_matrix))\n      } else {\n      temp_vector <- env_matrix[current_word_ids[j], ]\n      }\n      current_sentence <- current_sentence + temp_vector\n  }\n  sentence_ids[i, ] <- current_sentence\n  }\n  return(sentence_ids)\n}\n\n# Function to generate semantic meaning vectors for each word\nmake_meaning_vectors <- function(env_matrix, sentence_memory){\n  meaning_matrix <- matrix(0, nrow = nrow(env_matrix), ncol = ncol(env_matrix))\n  for(i in 1:nrow(env_matrix)){\n    meaning_matrix[i, ] <- get_echo(env_matrix[i, ], sentence_memory, output = \"echo\", type = \"cosine\")\n  }\n  return(meaning_matrix)\n}\n\n# Find top n similar words in terms of semantic meaning\nfind_similar_words <- function(meaning_matrix, dictionary, word_to_find, n) {\n  similarities <- cosine_x_to_m(meaning_matrix[which(dictionary$word == word_to_find), ], meaning_matrix)\nsimilarities_df <- data.frame(dictionary, similarities = similarities)\nsim_top <- similarities_df %>% arrange(desc(similarities)) %>% slice(1:n)\nreturn(sim_top)\n}\n```\n:::\n\n\nUnfortunately, my computer doesn't have sufficient processing power to run 10,000 TASA sentences with vectors 10,000 elements long (sparsity = 8). I am going to try using only the first 1000 sentences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nTASA_full <- read.table(\"tasaDocsPara.txt\", \n                          sep = \"\\t\", \n                          fill = FALSE, \n                          strip.white = TRUE)\n\nJonesMewhort_Figure3 <- c(\"financial\",\"savings\",\"finance\",\"pay\",\"invested\",\n                          \"loaned\",\"borrow\",\"lend\",\"invest\",\"investments\",\n                          \"bank\",\"spend\",\"save\",\"astronomy\",\"physics\",\n                          \"chemistry\",\"psychology\",\"biology\",\"scientific\",\n                          \"mathematics\",\"technology\",\"scientists\",\"science\",\n                          \"research\",\"sports\",\"team\",\"teams\",\"football\",\n                          \"coach\",\"sport\",\"players\",\"baseball\",\"soccer\",\n                          \"tennis\",\"basketball\")\n```\n:::\n\n\n## Shrink corpus size (= 1000), maintain vector length\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraining_TASA <- data.frame(sentence = TASA_full$V1[1:1000])\n\nTASA_dict <- training_TASA %>% unnest_tokens(output = \"word\", input = sentence, token = \"words\") %>% unique()\n\nTASA_ids <- replace_word_with_id(training_TASA, TASA_dict, 300)\n\nenv_vectors_TASA <- make_env_vectors(TASA_dict, 10000, 8)\n\nsent_vectors_TASA <- make_sentence_vectors(TASA_ids, env_vectors_TASA)\n\nmemory_TASA <- sent_vectors_TASA\n\ndict_env_df <- data.frame(TASA_dict, env_vectors_TASA) \n\nenv_its <- dict_env_df[which(TASA_dict$word %in% JonesMewhort_Figure3), ]\n\nenv_its_vecs <- env_its %>% select(-word) %>% as.matrix()\n\nits_meaning_vecs <- make_meaning_vectors(env_its_vecs, memory_TASA)\nrownames(its_meaning_vecs) <- env_its$word\n\ncosine_table_bram <- lsa::cosine(t(its_meaning_vecs))\n\ncorrplot::corrplot(cosine_table_bram,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWords from the same categories aren't always related to each other. Something's wrong, possibly because of the smaller corpus size?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nits_mds <- its_meaning_vecs %>% dist() %>% cmdscale() %>% as_tibble(.name_repair = \"minimal\")\ncolnames(its_mds) <- c(\"Dim1\", \"Dim2\")\n\nggplot(its_mds, aes(x = Dim1, y = Dim2, label = rownames(its_meaning_vecs))) + geom_point() + geom_text()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThis doesn't make sense.\n\n## Trying Matt's code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmatt_training <- TASA_full$V1[1:10000]\n\ncorpus_dictionary <- function(words){\n  neat_words <- words %>%\n                as.character() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                qdapRegex::rm_white_lead_trail() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                unique()\n  return(neat_words)\n}\n\ndictionary <- corpus_dictionary(matt_training)\n\nclean <- function(words){\n  if(length(words) == 1) {\n    neat_words <- words %>%\n      as.character() %>%\n      strsplit(split=\" \") %>%\n      unlist() %>%\n      qdapRegex::rm_white_lead_trail() %>%\n      strsplit(split=\" \") %>%\n      unlist() \n    return(neat_words)\n  }\n}\n\nclean_vector <- function(words){\n  return(lapply(unlist(strsplit(words,split=\"[.]\")),clean))\n}\n\nsentences_list <- clean_vector(matt_training)\n\nword_ids <- sl_word_ids_sentences(sentences_list, dictionary)\n\nenvironments <- sl_create_riv(10000, length(dictionary), 8)\n\nits_memory <- matrix(0,ncol=dim(environments)[2],nrow=length(word_ids))\nfor(i in 1:length(sentences_list)){\n  its_memory[i,] <- colSums(environments[word_ids[[i]],])\n}\n\n# function to generate echoes for a list of words\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  \n  return(word_meaning)\n  \n}\n\nJonesMewhort_vectors <- subset_semantic_vectors(JonesMewhort_Figure3,\n                                                dictionary,\n                                                environments,\n                                                its_memory,\n                                                tau=3)\n\ncosine_table <- lsa::cosine(t(JonesMewhort_vectors))\n\ncorrplot::corrplot(cosine_table,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThe correlation matrix makes sense. Words from the different categories are more or less correlated with others in the same category.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmatt_mds <- JonesMewhort_vectors %>% dist() %>% cmdscale() %>% as_tibble(.name_repair = \"minimal\")\ncolnames(matt_mds) <- c(\"Dim1\", \"Dim2\")\n\nggplot(matt_mds, aes(x = Dim1, y = Dim2, label = rownames(its_meaning_vecs))) + geom_point() + geom_text()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis doesn't make sense! Could it be because I used a different MDS function?\n\n## Comparing my functions with Matt's\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Redoing Matt's process with just the first 1000 sentences\n\nmatt_training_test <- TASA_full$V1[1:1000]\n\ndictionary_test <- corpus_dictionary(matt_training_test)\n\nsentences_list_test <- clean_vector(matt_training_test)\n\nword_ids_test <- sl_word_ids_sentences(sentences_list_test, dictionary_test)\n\nenvironments_test <- sl_create_riv(10000, length(dictionary_test), 8)\n\nits_memory_test <- matrix(0,ncol=dim(environments_test)[2],nrow=length(word_ids_test))\nfor(i in 1:length(sentences_list_test)){\n  its_memory_test[i,] <- colSums(environments_test[word_ids_test[[i]],])\n}\n\nJones_vector_test <- subset_semantic_vectors(JonesMewhort_Figure3, dictionary_test, environments_test, its_memory_test, tau = 3) \n\n# Comparing dictionaries\n\nall(TASA_dict$word == dictionary_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\n# Comparing sentences replaced with IDs (choosing a random sentence to compare i.e. sentence 19)\n\nall(TASA_ids[19, 1:length(word_ids_test[[19]])] == word_ids_test[[19]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\n# Comparing sentence memory (using the same environment vectors i.e. Matt's)\n\ntest_using_matt_env <- make_sentence_vectors(TASA_ids, environments_test)\n\nall(test_using_matt_env == its_memory_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\n# Comparing semantic meaning vectors\n\ntest_env_its <- environments_test[which(TASA_dict$word %in% JonesMewhort_Figure3), ]\n\ntest_meaning_vecs <- make_meaning_vectors(test_env_its, test_using_matt_env)\n\nall(test_meaning_vecs == JonesMewhort_vectors)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\nOur functions that create semantic meaning vectors differ. Looking at the code for each function, however, I cannot see why the two would diverge.\n\n## Looking at Randy's code\n\nAs laid out in the paper, his environment vectors don't contain integers (1s, 0s and -1s) but a randomly sampled value with mean 0 and variance 1/(vector length), although the variance is actually 1/sqrt(vector length) in the code. Not sure how this compares to using only the three integer values and taking into account sparsity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsemantic_vecs <- function (probe_words, env_vectors, memory, tau = 3) {\n  semantic_vectors <- matrix(0, length(probe_words), ncol(memory))\n  for (i in 1:length(probe_words)) {\n    probe <- unlist(strsplit(probe_words[i], split=\"/\"))\n    if (all(probe %in% rownames(env_vectors)) == TRUE) {\n      for (j in 1:nrow(memory)) {\n        A <- 1.0\n        for (k in 1:length(probe)) {\n          A <- A * cosine_randy(env_vectors[probe[k],], memory[j,])**tau\n        }\n        semantic_vectors[i,] <- semantic_vectors[i,] + A * memory[j,]\n      }\n    }\n  }\n  rownames(semantic_vectors) <- probe_words\n  semantic_vectors <- semantic_vectors[abs(rowSums(semantic_vectors)) != 0,]\n  return(semantic_vectors)\n}\n```\n:::\n\n\nI don't understand this function for making semantic vectors. Why is A multiplied by the memory again afterwards and why is A cumulative over different probes?\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}