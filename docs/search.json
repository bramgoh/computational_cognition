[
  {
    "objectID": "posts/Hintzman1/index.html",
    "href": "posts/Hintzman1/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 1)",
    "section": "",
    "text": "#Generating category prototypes and exemplars\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\nvalues <- c(rep(-1, 50), rep(1, 50))\nprotoA <- sample(values, 23, replace = TRUE)\nprotoB <- sample(values, 23, replace = TRUE)\nprotoC <- sample(values, 23, replace = TRUE)\nfeat_index <- 11:23\n\ndistort_low <- function(x) {\n  change_index <- sample(feat_index, 2)\n  x_new <- x\n  x_new[change_index[1]] <- x_new[change_index[1]] * -1\n  x_new[change_index[2]] <- x_new[change_index[2]] * -1\n  return(x_new)\n}\ndistort_high <- function(x) {\n  change_index <- sample(feat_index, 4)\n  x_new <- x\n  x_new[change_index[1]] <- x_new[change_index[1]] * -1\n  x_new[change_index[2]] <- x_new[change_index[2]] * -1\n  x_new[change_index[3]] <- x_new[change_index[3]] * -1\n  x_new[change_index[4]] <- x_new[change_index[4]] * -1\n  return(x_new)\n}\n\nlow_A1 <- distort_low(protoA)\nlow_A2 <- distort_low(protoA)\nlow_A3 <- distort_low(protoA)\nlow_B1 <- distort_low(protoB)\nlow_B2 <- distort_low(protoB)\nlow_B3 <- distort_low(protoB)\nlow_B4 <- distort_low(protoB)\nlow_B5 <- distort_low(protoB)\nlow_B6 <- distort_low(protoB)\nlow_C1 <- distort_low(protoC)\nlow_C2 <- distort_low(protoC)\nlow_C3 <- distort_low(protoC)\nlow_C4 <- distort_low(protoC)\nlow_C5 <- distort_low(protoC)\nlow_C6 <- distort_low(protoC)\nlow_C7 <- distort_low(protoC)\nlow_C8 <- distort_low(protoC) \nlow_C9 <- distort_low(protoC)\n\nlow_C_full <- rbind(low_C1, low_C2, low_C3, low_C4, low_C5, low_C6, low_C7, low_C8, low_C9)\n\nhigh_A1 <- distort_high(protoA)\nhigh_B1 <- distort_high(protoB)\nhigh_C1 <- distort_high(protoC)\n\nThe code works, but is very inelegant and requires a lot of copy and pasting. There has to be a way to automate this with a function so that I can simulate this whole thing 20 times.\n\nindexC_high <- seq(1, 23, 1)\nfor(i in 1:9) {\n  temp <- distort_high(protoC)\n  indexC_high <- rbind(indexC_high, temp)\n}\n\nThis is faster, but still requires specifying the variable to store it in, the number of exemplars, the function and the prototype. Attempts to create a function to automate this failed, as the dataframe returned (e.g. indexC_high) was unchanged."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I’m a millennial and technology is not my forte.\nIn spite of that, here is my attempt at a blog record of things I’ve learned. Here’s to jumping off the deep end…"
  },
  {
    "objectID": "posts/TechSetUp/index.html",
    "href": "posts/TechSetUp/index.html",
    "title": "Setting up the Techymabobs",
    "section": "",
    "text": "Matt introduced me to a ton of new software to download and familiarize myself with.\nQuarto blog\nI updated both R and RStudio, and the Quarto blog works as intended. However, remember to NOT rename the index.qmd etc. files, as that interferes with the seamless rendering process and the blog will not be updated as you render.\nDragging and dropping images still does not work. It would be really convenient if this could be rectified.\nSometime in the near future, I need to refresh my Rmarkdown code wrt to stylistic features, so I can do away with the default blog visuals.\nZotero\nI downloaded Zotero and the Safari plugin, and it seems to work fine. It syncs with R and with Microsoft Word, and my brief tests don’t raise any issues.\nI do have to note that most articles downloaded from Google Scholar do not contain DOI information, so this is something I still have to add in manually.\nGithub Desktop\nDownloading this was the easy part. Committing to changes and publishing/pushing was also not too difficult to figure out.\nThe only roadblock I’ve faced thus far is obtaining the blog URL. After setting the output directory to docs, I get a message saying that the pages failed to be built? I’m not sure how to fix this.\nI’ve noticed also that, while I’ve downloaded Github Desktop, I can’t seem to get the “Create a git repository” option when I start a new project in RStudio. An hour of Googling and troubleshooting has gotten me nowhere, so I’m stumped for now."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Cognition",
    "section": "",
    "text": "A blog record of a computational cognition journey\n\n\n\n\n\n\n\n\n  \n\n\n\n\nExploring discrepancy encoding with ITS (Part 5)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nExploring discrepancy encoding with ITS (Part 4)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nExploring discrepancy encoding with ITS (Part 3)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nBram Goh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring discrepancy encoding with ITS (Part 2)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nExploring discrepancy encoding with ITS (Part 1)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nITS (part 2)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nBram Goh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nITS (part 1)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nDiscrepancy Encoding in MINERVA (Part 1)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nA MINERVA model for attention (Part 2)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nA MINERVA model for attention\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nHintzman’s (1988) MINERVA (Part 2)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nHintzman’s (1988) MINERVA (Part 1)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nBram Goh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproducing Hintzman’s MINERVA (Part 5)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nReproducing Hintzman’s MINERVA (Part 4)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nReproducing Hintzman’s MINERVA (Part 3)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nReproducing Hintzman’s MINERVA (Part 2)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nReproducing Hintzman’s MINERVA (Part 1)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nSetting up the Techymabobs\n\n\n\n\n\n\n\ntech\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nBram Goh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Hintzman2/index.html",
    "href": "posts/Hintzman2/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 2)",
    "section": "",
    "text": "Generating traces and calculating the echo\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\nnum_feat <- 13\nnum_name <- 10\nnum_trace <- num_name + num_feat\nvalues <- c(rep(1, 50), rep(-1, 50))\n\n# Generating exemplars with distortions\n\ngen_exemp <- function(num_exemp, num_distort) {\n  \n  name <- sample(values, num_name)\n  name_copies <- matrix(rep(name, num_exemp), nrow = num_exemp, ncol = num_name, byrow = TRUE)\n  \n  feat_raw <- t(replicate(num_exemp, sample(values, num_feat)))\n  feat_filter <- t(replicate(n = num_exemp, sample(c(rep(1, num_feat - num_distort), rep(-1, num_distort)), num_feat)))\n  \n  feat_final <- feat_raw * feat_filter\n\n  exemp <- cbind(name_copies, feat_final)\n  \n  return(exemp)\n}\n\n\nempty_feat <- rep(0, num_feat)\n\nabstraction <- function(n1, n2, n3, num_distort) {\n\n# Generating traces and probes\na <- gen_exemp(n1, num_distort)\nprobe_a <- c(a[1, 1:num_name], empty_feat)\nb <- gen_exemp(n2, num_distort)\nprobe_b <- c(b[1, 1:num_name], empty_feat)\nc <- gen_exemp(n3, num_distort)\nprobe_c <- c(c[1, 1:num_name], empty_feat)\n  \nsm <- rbind(a, b, c)\n  \n# Echo activation function\necho_activation <- function(probe, sec_mem) {\n  e_int <- c()\n      for(i in 1:nrow(sec_mem)){\n          n_rel <- 0\n          temp_sim <- 0\n              for(j in 1:num_trace){\n                   current <- probe[j] * sec_mem[i, j]\n                   temp_sim <- current + temp_sim\n                          if(probe[j] != 0 & sec_mem[i, j] != 0) {\n                                     n_rel <- n_rel + 1\n      }\n}\ntrace_sim <- temp_sim/n_rel\ntrace_act <- trace_sim^3\ne_int <- c(e_int, trace_act)\n      }\n  return(e_int)\n}\n\nactivs_a <- echo_activation(probe_a, sm)\nactivs_b <- echo_activation(probe_b, sm)\nactivs_c <- echo_activation(probe_c, sm)\n\n# Echo content function\n\necho_content <- function(acts, sec_mem) {\n  e_cont <- c()\n  for(j in 1:num_trace){\n    temp_cont <- 0\n    for(i in 1:nrow(sec_mem)){\n      current <- acts[i] * sec_mem[i, j]\n      temp_cont <- current + temp_cont\n    }\n    e_cont <- c(e_cont, temp_cont)\n  }\n  return(e_cont)\n}\n\n# Calculating echo intensity and probe-echo correlations\necho_a <- round(echo_content(activs_a, sm), 3)\ncor_a <- cor(probe_a, echo_a)\nint_a <- sum(activs_a)\necho_b <- round(echo_content(activs_b, sm), 3)\ncor_b <- cor(probe_b, echo_b)\nint_b <- sum(activs_b)\necho_c <- round(echo_content(activs_c, sm), 3)\ncor_c <- cor(probe_c, echo_c)\nint_c <- sum(activs_c)\n\ndf <- data.frame(corr = c(cor_a, cor_b, cor_c), intensity = c(int_a, int_b, int_c))\nrownames(df) <- c(\"catA\", \"catB\", \"catC\")\nreturn(df)\n}\n\nI am close to replicating the Abstraction exercise in Hintzman (1986). I’ve made a mistake, however, as I am supposed to calculate prototype-echo correlations, not probe-echo correlations. I’ll need to rectify this, as well as amend the code to allow for 20 simulations, so that I can calculate mean prototype-echo correlations for the 3-, 6-, and 9-exemplar categories."
  },
  {
    "objectID": "posts/Hintzman3/index.html",
    "href": "posts/Hintzman3/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 3)",
    "section": "",
    "text": "Improving on the Abstraction exercise code and plotting prototypes and echoes\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\nnum_feat <- 13\nnum_name <- 10\nnum_trace <- num_name + num_feat\nvalues <- c(rep(1, 50), rep(-1, 50))\n\nempty_feat <- rep(0, num_feat)\n\nabstraction <- function(num_a, num_b, num_c, num_distort) {\n\n# Generating prototype\n  \ngen_proto <- function() {\n   proto <- sample(values, num_trace, replace = TRUE)\n   return(proto)\n}\n\nproto_a <- gen_proto()\nproto_b <- gen_proto()\nproto_c <- gen_proto()\n\n# Generating exemplars with distortions\n\ngen_exemp <- function(proto, num_exemp, num_distort) {\n  \n  proto_copies <- matrix(rep(proto, num_exemp), nrow = num_exemp, ncol = num_trace, byrow = TRUE)\n  \n  name_matrix_ones <- matrix(1, nrow = num_exemp, ncol = num_name)\n  feat_matrix_distort <- t(replicate(n = num_exemp, sample(c(rep(1, num_feat - num_distort), rep(-1, num_distort)), num_feat)))\n  distort_filter <- cbind(name_matrix_ones, feat_matrix_distort)\n\n  exemp <- proto_copies * distort_filter\n  return(exemp)\n}\n  \n# Generating traces and probes\na <- gen_exemp(proto_a, num_a, num_distort)\nprobe_a <- c(a[1, 1:num_name], empty_feat)\nb <- gen_exemp(proto_b, num_b, num_distort)\nprobe_b <- c(b[1, 1:num_name], empty_feat)\nc <- gen_exemp(proto_c, num_c, num_distort)\nprobe_c <- c(c[1, 1:num_name], empty_feat)\n  \nsm <- rbind(a, b, c)\n  \n# Echo activation function\necho_activation <- function(probe, sec_mem) {\n  e_activs <- c()\n      for(i in 1:nrow(sec_mem)){\n          n_rel <- 0\n          temp_sim <- 0\n              for(j in 1:num_trace){\n                   current <- probe[j] * sec_mem[i, j]\n                   temp_sim <- current + temp_sim\n                          if(probe[j] != 0 & sec_mem[i, j] != 0) {\n                                     n_rel <- n_rel + 1\n      }\n}\ntrace_sim <- temp_sim/n_rel\ntrace_act <- trace_sim^3\ne_activs <- c(e_activs, trace_act)\n      }\n  return(e_activs)\n}\n\nactivs_a <- echo_activation(probe_a, sm)\nactivs_b <- echo_activation(probe_b, sm)\nactivs_c <- echo_activation(probe_c, sm)\n\n# Echo content function\n\necho_content <- function(acts, sec_mem) {\n  e_cont <- c()\n  for(j in 1:num_trace){\n    temp_cont <- 0\n    for(i in 1:nrow(sec_mem)){\n      current <- acts[i] * sec_mem[i, j]\n      temp_cont <- current + temp_cont\n    }\n    e_cont <- c(e_cont, temp_cont)\n  }\n  return(e_cont)\n}\n\n# Calculating echo intensity and probe-echo correlations\necho_a <- round(echo_content(activs_a, sm), 3)\ncor_a <- cor(proto_a[(num_name + 1):num_trace], echo_a[(num_name + 1):num_trace])\nint_a <- sum(activs_a)\necho_b <- round(echo_content(activs_b, sm), 3)\ncor_b <- cor(proto_b[(num_name + 1):num_trace], echo_b[(num_name + 1):num_trace])\nint_b <- sum(activs_b)\necho_c <- round(echo_content(activs_c, sm), 3)\ncor_c <- cor(proto_c[(num_name + 1):num_trace], echo_c[(num_name + 1):num_trace])\nint_c <- sum(activs_c)\n\noutput_mat <- rbind(proto_a, echo_a, proto_b, echo_b, proto_c, echo_c)\nreturn(output_mat)\n}\n\nI edited the correlation code to reflect only the correlation for the 13 stimulus features (excluding the 10 name features; clearly, I misunderstood Hintzman the first time). Also, I changed the output to a matrix with the probes and echoes for the 3 categories. However, why do the echoes have values greater than 1 and less than -1? That’s not what Hintzman got.\n\nreplicate(20, abstraction(3,6,9,4))\n\n, , 1\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]  [,9]  [,10]\nproto_a -1.000  1.000  1.000  1.000 -1.000  1.000 -1.000 -1.000 1.000 -1.000\necho_a  -3.048  2.952  2.952  3.048 -3.048  3.048 -3.048 -2.952 2.952 -3.048\nproto_b  1.000  1.000  1.000 -1.000  1.000 -1.000  1.000 -1.000 1.000  1.000\necho_b   6.600  6.552  6.552 -6.600  6.600 -6.600  5.448 -6.552 5.400  5.448\nproto_c -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000  1.000 1.000  1.000\necho_c  -9.384 -9.384 -9.384  9.384 -9.384  9.384  8.616  9.384 8.616  8.616\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17] [,18] [,19]  [,20]\nproto_a -1.000 -1.000  1.000  1.000 -1.000 -1.000  1.000   1.0 1.000 -1.000\necho_a  -1.016  1.032  1.032  2.968  1.048 -3.000  1.016  -1.0 0.984 -0.984\nproto_b -1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000  -1.0 1.000 -1.000\necho_b   2.328 -4.072 -4.328  3.784 -5.816 -0.296 -1.816   0.2 1.544 -1.800\nproto_c -1.000  1.000  1.000 -1.000 -1.000  1.000 -1.000  -1.0 1.000 -1.000\necho_c  -5.128  1.256  5.256  2.744 -2.616  5.000 -2.872  -3.0 6.872 -2.872\n         [,21]  [,22]  [,23]\nproto_a  1.000  1.000  1.000\necho_a   1.000  3.032  3.032\nproto_b -1.000 -1.000 -1.000\necho_b   0.312 -4.216 -4.344\nproto_c -1.000  1.000  1.000\necho_c  -5.000  3.256  5.256\n\n, , 2\n\n         [,1]  [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]  [,9]  [,10]\nproto_a 1.000 1.000  1.000 -1.000 -1.000 -1.000  1.000 -1.000 1.000 -1.000\necho_a  3.072 3.072  2.928 -2.928 -3.072 -3.072  2.928 -2.928 3.072 -3.072\nproto_b 1.000 1.000 -1.000  1.000 -1.000  1.000  1.000  1.000 1.000  1.000\necho_b  6.576 6.576 -6.576  6.576 -6.576  5.424  5.424  6.576 6.576  5.424\nproto_c 1.000 1.000 -1.000  1.000 -1.000 -1.000 -1.000  1.000 1.000 -1.000\necho_c  9.408 9.408 -9.360  9.360 -9.408 -8.640 -8.592  9.360 9.408 -8.640\n         [,11] [,12]  [,13]  [,14]  [,15] [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.00 -1.000  1.000  1.000 -1.00  1.000  1.000 -1.000 -1.000\necho_a  -1.008 -0.96 -2.960  2.976  0.992 -0.96  0.944 -0.944 -1.024 -2.960\nproto_b  1.000  1.00  1.000  1.000  1.000 -1.00  1.000 -1.000 -1.000 -1.000\necho_b   3.936  2.32  2.320 -0.192  3.936  2.32  3.552 -1.552 -0.192  0.320\nproto_c  1.000  1.00  1.000 -1.000 -1.000  1.00 -1.000  1.000  1.000  1.000\necho_c  -0.752  5.12  5.104 -2.976 -0.736  5.12 -6.736  6.864 -3.008  4.976\n         [,21]  [,22]  [,23]\nproto_a -1.000  1.000  1.000\necho_a  -0.960  0.976 -0.976\nproto_b -1.000  1.000 -1.000\necho_b  -3.680  5.808 -3.808\nproto_c  1.000 -1.000  1.000\necho_c   4.736 -2.608  2.736\n\n, , 3\n\n           [,1]    [,2]    [,3]   [,4]   [,5]    [,6]    [,7]   [,8]    [,9]\nproto_a  -1.000  -1.000  -1.000 -1.000  1.000  -1.000  -1.000  1.000  -1.000\necho_a   -7.608  -7.608  -7.608  1.608  7.608  -7.608  -7.608  7.608  -7.608\nproto_b   1.000   1.000  -1.000  1.000  1.000  -1.000   1.000  1.000   1.000\necho_b    5.928   5.928  -6.072  6.072  6.072  -6.072   5.928  6.072   5.928\nproto_c  -1.000  -1.000  -1.000  1.000  1.000  -1.000  -1.000  1.000  -1.000\necho_c  -10.488 -10.488 -10.584  7.512 10.584 -10.584 -10.488 10.584 -10.488\n         [,10]  [,11]  [,12]  [,13]  [,14] [,15]  [,16]  [,17]  [,18]  [,19]\nproto_a  1.000 -1.000 -1.000 -1.000 -1.000 1.000  1.000  1.000  1.000 -1.000\necho_a   7.608 -4.584  2.536  0.536  0.488 4.584  1.464  1.560  3.512  0.536\nproto_b  1.000  1.000 -1.000 -1.000 -1.000 1.000 -1.000 -1.000 -1.000  1.000\necho_b   6.072  3.944 -3.976 -1.976 -2.008 6.056 -0.024 -1.960 -1.992  0.024\nproto_c  1.000 -1.000  1.000  1.000 -1.000 1.000 -1.000  1.000  1.000  1.000\necho_c  10.584 -7.480  3.480  2.472 -0.504 7.560 -1.464  4.472  2.520  2.488\n         [,20] [,21]  [,22]  [,23]\nproto_a  1.000 -1.00 -1.000 -1.000\necho_a   3.512 -0.44  1.560 -2.536\nproto_b  1.000  1.00  1.000  1.000\necho_b   2.008  2.04  2.040  1.976\nproto_c -1.000  1.00  1.000 -1.000\necho_c   2.552  3.48  4.504 -3.496\n\n, , 4\n\n          [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000 -1.000  1.000  1.000 1.000 -1.000 -1.000  1.000 -1.000 -1.000\necho_a   3.624 -3.624  3.624  3.624 2.376 -2.376 -3.624  2.472 -3.528 -3.528\nproto_b -1.000  1.000 -1.000 -1.000 1.000 -1.000  1.000 -1.000 -1.000 -1.000\necho_b  -6.600  6.600 -6.600 -6.600 6.552 -6.552  6.600 -5.448 -5.400 -5.400\nproto_c -1.000  1.000 -1.000 -1.000 1.000 -1.000  1.000  1.000  1.000  1.000\necho_c  -9.576  9.576 -9.576 -9.576 9.192 -9.192  9.576  8.424  8.808  8.808\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000  1.000  1.000  1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000\necho_a  -2.584  1.320 -1.464  3.192 -2.808 -3.336  3.208 -3.336 -2.952 -2.456\nproto_b  1.000 -1.000  1.000  1.000  1.000  1.000 -1.000  1.000  1.000  1.000\necho_b   3.576 -0.328  2.456 -0.216 -0.168  2.344 -2.216  2.344  1.960  3.448\nproto_c -1.000  1.000  1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000\necho_c  -6.552 -5.064  7.192 -3.192 -2.808  5.320 -3.320  5.320 -0.680 -8.552\n         [,21] [,22]  [,23]\nproto_a  1.000 1.000 -1.000\necho_a  -1.112 0.392  2.952\nproto_b  1.000 1.000 -1.000\necho_b   6.072 4.568 -1.960\nproto_c -1.000 1.000 -1.000\necho_c   1.448 9.192  0.680\n\n, , 5\n\n         [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\nproto_a 1.000  1.000  1.000 -1.000 -1.000 -1.000 1.000 -1.000 -1.000 -1.000\necho_a  3.000  3.000  3.000 -3.000 -3.000 -3.000 3.000 -3.000 -3.000 -3.000\nproto_b 1.000  1.000  1.000  1.000  1.000 -1.000 1.000  1.000  1.000  1.000\necho_b  6.072  5.928  5.928  5.928  5.928 -6.072 6.072  6.072  6.072  6.072\nproto_c 1.000 -1.000 -1.000 -1.000 -1.000 -1.000 1.000  1.000  1.000  1.000\necho_c  9.048 -8.952 -8.952 -8.952 -8.952 -9.048 9.048  9.048  9.048  9.048\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17] [,18]  [,19]  [,20]\nproto_a -1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000  1.00 -1.000  1.000\necho_a  -3.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000  3.00 -3.000  3.000\nproto_b  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000  1.00  1.000 -1.000\necho_b   1.992 -3.992  3.960  6.024  0.024  0.056 -1.960 -0.04  4.024 -4.024\nproto_c  1.000  1.000 -1.000  1.000  1.000  1.000  1.000 -1.00  1.000 -1.000\necho_c  -0.984  0.968 -4.968  3.048  3.000  7.000  4.984 -5.00  3.032 -3.032\n         [,21] [,22]  [,23]\nproto_a -1.000 -1.00  1.000\necho_a  -1.000 -1.00  3.000\nproto_b  1.000  1.00 -1.000\necho_b   4.056  0.04 -0.008\nproto_c  1.000  1.00  1.000\necho_c   7.032  5.00 -1.000\n\n, , 6\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9] [,10]\nproto_a -1.000 -1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.000 -1.000 1.000\necho_a  -2.928 -2.928 -3.072 -3.072  3.072 -2.928 -3.072  2.928 -3.072 3.072\nproto_b -1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000  1.000 1.000\necho_b  -6.000  6.000 -6.000  6.000 -6.000 -6.000 -6.000 -6.000  6.000 6.000\nproto_c  1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000 1.000\necho_c   8.976  8.976 -9.024 -9.024  9.024  8.976 -9.024 -8.976 -9.024 9.024\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000  1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000  1.000 -1.000\necho_a   0.992 -1.024 -2.992 -1.024 -2.992  0.992  0.960  2.976 -0.944 -0.928\nproto_b  1.000 -1.000  1.000  1.000  1.000  1.000 -1.000  1.000  1.000 -1.000\necho_b   0.000 -6.000  4.000  0.000  4.000  4.000 -2.000  4.000  4.000  2.000\nproto_c -1.000 -1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000\necho_c  -0.992 -3.008  0.976 -3.008  0.976 -0.992 -4.992 -2.976  6.992  8.992\n         [,21]  [,22]  [,23]\nproto_a -1.000 -1.000  1.000\necho_a  -2.928 -0.960  1.008\nproto_b -1.000 -1.000 -1.000\necho_b  -2.000  2.000 -4.000\nproto_c  1.000  1.000  1.000\necho_c   8.976  4.992  1.008\n\n, , 7\n\n          [,1]  [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000 1.000  1.000 -1.000  1.000 -1.000  1.000 -1.000  1.000  1.000\necho_a   2.688 3.456  3.312 -3.456  3.312 -3.456  2.688 -3.312  2.688  3.312\nproto_b -1.000 1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_b  -6.384 5.616  6.768 -5.616  6.768 -5.616 -6.384 -6.768 -6.384  6.768\nproto_c  1.000 1.000 -1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000 -1.000\necho_c   9.408 8.640 -9.360 -8.640 -9.360 -8.640  9.408  9.360  9.408 -9.360\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18] [,19]  [,20]\nproto_a -1.000  1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000 1.000  1.000\necho_a  -2.592  1.088 -1.040 -0.672 -0.864 -1.280 -2.736  3.040 1.168 -1.264\nproto_b  1.000  1.000  1.000  1.000  1.000 -1.000  1.000 -1.000 1.000 -1.000\necho_b   5.616  2.384  0.256  3.360  1.872 -3.872  3.744 -0.128 1.744 -4.000\nproto_c  1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000 1.000 -1.000\necho_c   2.592 -5.120 -5.008  8.736  0.864 -2.752  0.720  5.024 4.880 -0.752\n         [,21]  [,22]  [,23]\nproto_a  1.000 -1.000 -1.000\necho_a   2.784 -0.992 -1.152\nproto_b -1.000  1.000  1.000\necho_b  -4.128 -0.128 -1.872\nproto_c  1.000  1.000 -1.000\necho_c   5.280  0.992 -2.880\n\n, , 8\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000  1.000  1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000\necho_a  -3.120  3.120  3.120  3.120  2.880 -3.024 -3.024 -2.880 -2.976 -2.976\nproto_b -1.000  1.000  1.000  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000\necho_b  -6.096  6.096  6.096  6.096 -6.048  5.904  5.904  6.048 -5.952 -5.952\nproto_c  1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000 -1.000\necho_c   9.072 -9.072 -9.072 -9.072  9.024  8.976  8.976 -9.024 -8.928 -8.928\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_a   1.056 -3.008  3.056  3.008 -2.992 -0.976 -0.992 -0.944 -0.960 -1.040\nproto_b  1.000  1.000  1.000  1.000  1.000 -1.000  1.000  1.000  1.000  1.000\necho_b   4.032  1.952  2.064  2.016  1.968 -1.968  3.968  4.016  2.016 -0.048\nproto_c -1.000  1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000\necho_c  -3.040  3.008 -5.040  0.960  1.008 -4.976  2.976 -3.024 -3.008  5.008\n         [,21]  [,22]  [,23]\nproto_a -1.000  1.000  1.000\necho_a  -2.960  2.928  1.056\nproto_b  1.000 -1.000  1.000\necho_b   2.000 -0.048  4.032\nproto_c -1.000  1.000 -1.000\necho_c  -2.992  8.976 -3.040\n\n, , 9\n\n          [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000  1.000  1.000 -1.000 1.000  1.000  1.000 -1.000 -1.000  1.000\necho_a   3.120  3.120  2.976 -3.120 2.880  3.024  3.024 -2.880 -3.120  2.976\nproto_b -1.000 -1.000 -1.000  1.000 1.000  1.000  1.000 -1.000  1.000 -1.000\necho_b  -6.096 -6.096 -5.952  6.096 6.048  5.904  5.904 -6.048  6.096 -5.952\nproto_c -1.000 -1.000  1.000  1.000 1.000 -1.000 -1.000 -1.000  1.000  1.000\necho_c  -9.072 -9.072  8.928  9.072 9.024 -8.976 -8.976 -9.024  9.072  8.928\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19] [,20]\nproto_a  1.000  1.000 -1.000 -1.000  1.000  1.000  1.000  1.000  1.000 1.000\necho_a   0.992  2.976 -0.944 -0.896  1.024 -0.992  2.976  2.992  3.008 0.992\nproto_b  1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000  1.000 1.000\necho_b   3.968 -1.984 -0.048 -6.048 -2.016  1.984  5.952 -3.984 -0.032 0.000\nproto_c  1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000 -1.000 1.000\necho_c  -2.976  4.960 -6.992 -7.040 -1.024 -2.976 -2.976  4.944 -1.024 0.992\n         [,21]  [,22]  [,23]\nproto_a  1.000  1.000 -1.000\necho_a   1.040 -3.056 -1.056\nproto_b -1.000  1.000  1.000\necho_b   1.936  2.064  4.032\nproto_c -1.000  1.000  1.000\necho_c  -6.992  5.040  3.040\n\n, , 10\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9] [,10]\nproto_a -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000 1.000\necho_a  -2.928 -2.928  3.072 -3.072  3.072 -3.072 -3.072  3.072  2.928 2.928\nproto_b  1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000 1.000\necho_b   6.576  6.576 -5.424 -6.576  6.576 -6.576 -6.576 -5.424 -6.576 5.424\nproto_c -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000 1.000\necho_c  -9.360 -9.360 -8.640  9.408 -9.408  9.408  9.408 -8.640  9.360 8.592\n         [,11]  [,12] [,13]  [,14]  [,15]  [,16]  [,17]  [,18] [,19]  [,20]\nproto_a -1.000 -1.000 -1.00  1.000 -1.000  1.000  1.000  1.000 1.000 -1.000\necho_a  -2.992 -1.024 -2.96 -2.944 -0.944  1.008  0.976  1.040 2.944 -0.960\nproto_b  1.000  1.000  1.00  1.000  1.000 -1.000 -1.000  1.000 1.000 -1.000\necho_b   6.064  1.808  6.32  0.448  4.448 -1.936 -2.192  2.320 3.552  4.320\nproto_c -1.000  1.000 -1.00 -1.000 -1.000  1.000  1.000 -1.000 1.000 -1.000\necho_c  -1.360  2.880 -5.36 -6.976 -7.248 -0.880  3.120 -5.136 6.720 -5.248\n         [,21]  [,22]  [,23]\nproto_a  1.000 -1.000 -1.000\necho_a  -1.024 -1.008 -2.992\nproto_b -1.000  1.000 -1.000\necho_b  -2.192  3.936  0.064\nproto_c  1.000 -1.000 -1.000\necho_c   3.136  0.752 -0.976\n\n, , 11\n\n          [,1]   [,2]   [,3]  [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000  1.000  1.000 1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000\necho_a  -2.424  3.576  3.576 3.576  3.576 -3.576 -3.576  2.424 -3.576 -2.424\nproto_b  1.000 -1.000 -1.000 1.000 -1.000 -1.000 -1.000  1.000  1.000 -1.000\necho_b   5.928 -6.072 -6.072 5.928 -6.072 -5.928 -5.928  6.072  6.072 -6.072\nproto_c  1.000  1.000  1.000 1.000  1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_c   8.760  9.240  9.240 9.144  9.240 -9.144 -9.144 -8.856 -9.240  8.856\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18] [,19]  [,20]\nproto_a -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000 -1.000  1.00  1.000\necho_a  -3.064  2.808  0.552 -0.936 -2.936 -2.808  0.808  0.808  1.32  1.448\nproto_b  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000  1.000 -1.00  1.000\necho_b  -1.992 -3.976  6.056  3.992  1.992 -6.024 -1.976  2.024 -2.04 -0.056\nproto_c -1.000 -1.000 -1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.00  1.000\necho_c  -1.176 -2.776 -6.984  0.904  0.792  2.856 -2.920 -2.952  5.08  7.064\n         [,21]  [,22] [,23]\nproto_a -1.000 -1.000 1.000\necho_a   0.808  1.576 1.192\nproto_b  1.000 -1.000 1.000\necho_b   2.024 -0.072 1.976\nproto_c -1.000  1.000 1.000\necho_c  -2.952  9.064 3.048\n\n, , 12\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 -1.000  1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000\necho_a  -3.048 -2.952  3.048 -3.048 -2.952  3.048 -3.048  2.952 -3.048 -2.952\nproto_b -1.000  1.000  1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000\necho_b  -6.024  5.976  6.024 -6.024  5.976  6.024 -6.024 -5.976 -6.024  5.976\nproto_c  1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_c   9.000 -9.000 -9.000 -9.000 -9.000 -9.000 -9.000 -9.000 -9.000  9.000\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a  1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000\necho_a   2.968 -1.032 -1.016 -0.984 -1.048 -2.968 -3.000  3.000  1.016 -3.000\nproto_b -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000 -1.000\necho_b  -3.976 -4.008 -2.008  1.992 -6.008  3.976 -0.024  0.024  2.008 -0.024\nproto_c  1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000\necho_c  -1.000  3.000 -1.000 -3.000  7.000  1.000 -3.000 -3.000 -1.000 -5.000\n         [,21]  [,22]  [,23]\nproto_a -1.000  1.000 -1.000\necho_a  -1.032  0.968 -1.016\nproto_b -1.000 -1.000  1.000\necho_b  -4.008 -3.992 -2.008\nproto_c -1.000  1.000 -1.000\necho_c  -7.000  7.000 -5.000\n\n, , 13\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000  1.000  1.000  1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000\necho_a   3.384  3.384  3.384  3.384  2.616  2.616 -3.384 -2.616 -3.384 -3.384\nproto_b -1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000 -1.000  1.000  1.000\necho_b  -6.120 -6.120 -6.264 -6.264  5.880  5.880  6.264 -5.736  6.264  6.120\nproto_c -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_c  -8.952 -8.952  9.048  9.048 -9.048 -9.048 -9.048 -8.952 -9.048  8.952\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000  1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.000 -1.000\necho_a  -3.256 -1.128 -1.256 -3.000 -3.256  3.384 -0.872 -1.128  1.128 -1.128\nproto_b  1.000  1.000  1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000\necho_b   4.152  2.056  4.024  0.216  4.168 -6.232 -1.928  2.056 -2.024  2.024\nproto_c  1.000 -1.000  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000  1.000\necho_c   4.968  0.984  4.968 -3.000  2.968  5.048 -0.984  0.984 -4.984  4.984\n         [,21]  [,22]  [,23]\nproto_a -1.000  1.000 -1.000\necho_a  -0.744  1.000  0.872\nproto_b -1.000  1.000  1.000\necho_b  -3.976 -0.104  1.992\nproto_c  1.000  1.000 -1.000\necho_c   5.032  5.000 -7.016\n\n, , 14\n\n          [,1]   [,2]   [,3]    [,4]   [,5]    [,6]    [,7]  [,8]   [,9]\nproto_a  1.000 -1.000 -1.000  -1.000  1.000  -1.000  -1.000 1.000  1.000\necho_a   1.056 -4.944 -4.944  -4.944  1.056  -4.944  -4.944 4.944  4.944\nproto_b  1.000 -1.000 -1.000   1.000  1.000   1.000   1.000 1.000 -1.000\necho_b   6.576 -5.424 -5.424   6.576  6.576   6.576   6.576 5.424 -6.576\nproto_c -1.000 -1.000 -1.000  -1.000 -1.000  -1.000  -1.000 1.000  1.000\necho_c  -8.736 -9.264 -9.264 -10.032 -8.736 -10.032 -10.032 9.264 10.032\n          [,10]  [,11] [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18] [,19]\nproto_a  -1.000  1.000 1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.00\necho_a   -4.944  1.216 0.080  1.488 -0.784 -0.352  1.216  0.512  0.080 -1.92\nproto_b   1.000 -1.000 1.000 -1.000  1.000 -1.000  1.000 -1.000  1.000 -1.00\necho_b    6.576 -4.064 3.680  0.448  3.936 -6.192  1.936 -4.448  1.680 -2.32\nproto_c  -1.000  1.000 1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000  1.00\necho_c  -10.032  1.472 4.528 -6.352  0.528  3.168  1.088  7.040  4.656  4.48\n         [,20]  [,21]  [,22]  [,23]\nproto_a -1.000 -1.000  1.000  1.000\necho_a  -4.512 -3.216 -2.080 -2.080\nproto_b  1.000 -1.000 -1.000  1.000\necho_b   0.448 -3.936  0.320 -1.680\nproto_c -1.000 -1.000 -1.000 -1.000\necho_c  -7.648 -1.392 -5.216 -5.088\n\n, , 15\n\n          [,1]  [,2]   [,3]   [,4]  [,5]  [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 1.000  1.000 -1.000 1.000 1.000  1.000  1.000 -1.000  1.000\necho_a  -3.024 3.120  2.976 -3.120 3.120 3.120  2.976  2.880 -2.880  3.024\nproto_b  1.000 1.000  1.000 -1.000 1.000 1.000  1.000 -1.000  1.000 -1.000\necho_b   5.904 6.096  5.952 -6.096 6.096 6.096  5.952 -6.048  6.048 -5.904\nproto_c -1.000 1.000 -1.000 -1.000 1.000 1.000 -1.000 -1.000  1.000  1.000\necho_c  -8.976 9.072 -8.928 -9.072 9.072 9.072 -8.928 -9.024  9.024  8.976\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000\necho_a   0.944 -1.040 -0.992  1.008  3.008  0.912 -3.024  0.944 -2.992  1.072\nproto_b -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000  1.000  1.000\necho_b  -2.032 -4.016  5.952  0.016  5.984 -4.048 -4.016 -4.016 -3.984  0.080\nproto_c -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000\necho_c  -5.008 -1.040 -4.960  1.008 -4.928 -7.024  0.944 -3.024  4.944  9.008\n         [,21]  [,22]  [,23]\nproto_a  1.000 -1.000  1.000\necho_a   3.024 -1.024  2.960\nproto_b  1.000 -1.000 -1.000\necho_b   2.032 -2.016 -0.016\nproto_c -1.000 -1.000 -1.000\necho_c   1.040 -1.024 -4.976\n\n, , 16\n\n          [,1]  [,2]   [,3]   [,4]   [,5]  [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 1.000 -1.000 -1.000  1.000 1.000  1.000 -1.000  1.000 -1.000\necho_a  -2.952 3.048 -3.048 -3.048  2.952 3.048  3.048 -2.952  2.952 -3.048\nproto_b  1.000 1.000 -1.000 -1.000 -1.000 1.000  1.000  1.000 -1.000 -1.000\necho_b   5.976 6.024 -6.024 -6.024 -5.976 6.024  6.024  5.976 -5.976 -6.024\nproto_c  1.000 1.000 -1.000  1.000  1.000 1.000 -1.000  1.000  1.000  1.000\necho_c   9.000 9.000 -9.000  9.000  9.000 9.000 -9.000  9.000  9.000  9.000\n        [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a 1.000 -1.000  1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000\necho_a  3.000 -2.984 -0.984 -1.016 -0.968  1.000 -1.016  1.032 -3.000  0.968\nproto_b 1.000  1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000\necho_b  0.024  1.976  1.992 -2.008  3.992  0.008 -2.008  4.008 -0.024 -3.992\nproto_c 1.000 -1.000  1.000  1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000\necho_c  3.000 -5.000  5.000  3.000  5.000 -3.000  3.000 -3.000 -1.000  3.000\n        [,21]  [,22]  [,23]\nproto_a 1.000  1.000  1.000\necho_a  3.032  2.968  0.984\nproto_b 1.000 -1.000 -1.000\necho_b  4.024 -3.976 -1.992\nproto_c 1.000 -1.000 -1.000\necho_c  1.000 -5.000 -5.000\n\n, , 17\n\n          [,1]  [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000 1.000  1.000  1.000  1.000  1.000 -1.000  1.000  1.000 -1.000\necho_a   3.048 2.952  2.952  2.952  3.048  3.048 -3.048  2.952  3.048 -3.048\nproto_b -1.000 1.000  1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000  1.000\necho_b  -6.600 5.400  6.552  6.552 -6.600 -6.600  6.600  6.552 -5.448  5.448\nproto_c  1.000 1.000 -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000\necho_c   9.384 8.616 -9.384 -9.384  9.384  9.384 -9.384 -9.384 -8.616  8.616\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16] [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.000  1.000 -1.000  1.000 -1.000 1.000 -1.000 -1.000 -1.000\necho_a   0.968  1.000  1.032  0.968  0.968 -3.000 2.968 -0.984 -3.048 -1.000\nproto_b -1.000  1.000 -1.000  1.000  1.000  1.000 1.000 -1.000  1.000  1.000\necho_b   3.800  0.568 -4.072  3.800  4.056  0.088 3.784 -2.440  5.576  0.072\nproto_c  1.000 -1.000 -1.000 -1.000 -1.000 -1.000 1.000  1.000  1.000  1.000\necho_c   2.744 -9.000  1.256  2.744 -1.256 -1.000 2.744  7.128  6.616 -1.000\n         [,21]  [,22]  [,23]\nproto_a  1.000  1.000 -1.000\necho_a  -1.048  3.016 -3.016\nproto_b  1.000 -1.000  1.000\necho_b   6.456 -1.704  2.472\nproto_c -1.000 -1.000 -1.000\necho_c  -7.384 -4.872 -7.128\n\n, , 18\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 -1.000 -1.000  1.000 -1.000  1.000 1.000  1.000 -1.000  1.000\necho_a  -2.616 -3.384 -3.384  3.384 -3.384  3.384 2.616  3.384 -3.384  2.616\nproto_b -1.000  1.000  1.000 -1.000  1.000 -1.000 1.000 -1.000  1.000  1.000\necho_b  -5.880  6.264  6.120 -6.264  6.120 -6.120 5.880 -6.264  6.264  5.736\nproto_c -1.000  1.000 -1.000 -1.000 -1.000  1.000 1.000 -1.000  1.000 -1.000\necho_c  -9.048  9.048 -8.952 -9.048 -8.952  8.952 9.048 -9.048  9.048 -8.952\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000 -1.000\necho_a  -2.872 -1.256  0.616 -0.872 -0.744  3.000  1.000 -1.384  1.128 -3.000\nproto_b -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000 -1.000\necho_b  -1.864  4.072  5.976 -2.008 -3.928 -0.248 -0.008  6.088 -2.072  0.136\nproto_c -1.000  1.000  1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000\necho_c  -7.016  1.032  5.048 -9.016  0.968 -7.000  7.000  3.048 -1.016 -7.000\n         [,21]  [,22]  [,23]\nproto_a  1.000  1.000  1.000\necho_a   1.000 -1.128  3.384\nproto_b -1.000  1.000 -1.000\necho_b  -0.072  2.072 -6.216\nproto_c  1.000  1.000 -1.000\necho_c  -1.000  1.016 -3.048\n\n, , 19\n\n          [,1]  [,2]  [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 1.000 1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000\necho_a  -1.104 4.896 4.896 -4.896  4.992  4.992 -4.992 -4.896 -1.104  4.992\nproto_b  1.000 1.000 1.000 -1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000\necho_b   6.096 6.048 6.048 -6.048 -5.952 -5.952  5.952 -6.048  6.096 -5.952\nproto_c  1.000 1.000 1.000 -1.000  1.000  1.000 -1.000 -1.000  1.000  1.000\necho_c   8.400 9.696 9.696 -9.696  9.600  9.600 -9.600 -9.696  8.400  9.600\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17] [,18]  [,19]  [,20]\nproto_a -1.000  1.000 -1.000  1.000  1.000 -1.000 -1.000 1.000  1.000  1.000\necho_a  -2.736  0.368 -1.632  1.664 -0.064 -1.664  1.904 4.928  2.800  0.336\nproto_b -1.000 -1.000 -1.000 -1.000 -1.000  1.000  1.000 1.000 -1.000  1.000\necho_b  -5.968 -2.032 -2.016 -1.984 -2.048  1.984  1.936 2.048 -2.032  1.968\nproto_c  1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.000 1.000 -1.000 -1.000\necho_c   0.304 -2.800 -3.232  3.200 -4.800 -3.200 -4.336 9.664 -0.368 -2.768\n         [,21]  [,22] [,23]\nproto_a  1.000  1.000 1.000\necho_a   0.352  0.368 1.616\nproto_b -1.000 -1.000 1.000\necho_b  -0.032 -2.032 4.016\nproto_c -1.000 -1.000 1.000\necho_c  -2.784 -2.800 3.248\n\n, , 20\n\n          [,1]    [,2]   [,3]    [,4]   [,5]   [,6]    [,7]    [,8]   [,9]\nproto_a  1.000   1.000 -1.000   1.000  1.000 -1.000  -1.000  -1.000 -1.000\necho_a   3.024   3.120 -3.120   3.120  2.880 -3.120  -2.880  -2.880 -3.120\nproto_b -1.000   1.000 -1.000   1.000 -1.000 -1.000   1.000   1.000 -1.000\necho_b  -4.032   7.968 -7.968   7.968 -7.920 -7.968   7.920   7.920 -7.968\nproto_c -1.000  -1.000  1.000  -1.000  1.000  1.000  -1.000  -1.000  1.000\necho_c  -7.728 -10.320 10.320 -10.320 10.272 10.320 -10.272 -10.272 10.320\n         [,10]  [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]\nproto_a -1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000\necho_a  -2.976 -1.056 -0.944 -1.040 -1.024 -2.912  3.056  2.992  2.992  2.992\nproto_b -1.000 -1.000  1.000  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000\necho_b  -4.080 -1.520  3.072 -2.656 -3.792  5.488  3.104 -3.328 -1.760 -4.896\nproto_c -1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000\necho_c  -7.680  7.008 -5.424  3.440 -0.128 -7.840 -5.456 -2.160 -0.592 -3.728\n         [,20]  [,21]  [,22]  [,23]\nproto_a -1.000  1.000 -1.000  1.000\necho_a  -2.992  3.008  0.912 -0.944\nproto_b  1.000 -1.000 -1.000  1.000\necho_b   4.896 -1.328 -5.504  4.640\nproto_c -1.000 -1.000  1.000 -1.000\necho_c   3.728 -2.592  7.856 -3.856\n\n\nUsing the replicate function allows me to simulate multiple participants easily.\nNext, I need to plot the prototypes against the echoes in a “histogram”. After several failed attempts at doing so with ggplot, I finally came to the conclusion that I had to use the basic R plotting package. An example of the code is below.\n\ngraph_in <- abstraction(3, 6, 9, 4)\npar = (mfrow = c (1, 2))\nbarplot(graph_in[5, ], main = \"proto_a\")\n\n\n\nbarplot(graph_in[6, ], main = \"echo_a\")\n\n\n\n\nThe code works, though it seems to indicate that the name features are a little different in the echo than in the probe. That might make sense, since the probe would not be highly similar to traces from a different category, which could affect the “purity” of the echo.\nI haven’t figured out a way to write a function to easily churn out the 6 graphs. Functions only return 1 output, and I’m not sure the basic plotting package in R allows for saving a graph containing multiple layers as a variable."
  },
  {
    "objectID": "posts/Hintzman4/index.html",
    "href": "posts/Hintzman4/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 4)",
    "section": "",
    "text": "Generalizing the code for more flexibility and brief thoughts on cosine similarity\nFirst, I need to clean up my code: name variables such that I don’t forget what they mean and define functions outside of other functions. Second, I need to generalize the code so that the number of categories is not hard coded .\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\nnum_of_pattern_feat <- 13\nnum_of_name_feat <- 10\nnum_of_trace_feat <- num_of_name_feat + num_of_pattern_feat\nfeat_values <- c(rep(1, 50), rep(-1, 50))\nnum_to_distort <- 4\n\n\n\n\n# Generating prototype function\n  \ngen_proto <- function() {\n   proto <- sample(feat_values, num_of_trace_feat, replace = TRUE)\n   return(proto)\n}\n\n# Generating prototype copies --> yields proto_copies_matrix\n\ngen_proto_copies <- function(input_vector) {\n  full_proto_copies_matrix <- c()\n  for(i in input_vector) {\n    proto_single <- gen_proto()\n    proto_copies <- matrix(rep(proto_single, i), nrow = i, ncol = num_of_trace_feat, byrow = TRUE)\n    full_proto_copies_matrix <- rbind(full_proto_copies_matrix, proto_copies)\n  }\n  return(full_proto_copies_matrix)\n}\n\n# Generating traces in secondary memory with distortion --> yields memory_matrix\n\ngen_secondary_memory <- function(input_vector, proto_copies_matrix) {\n  total_num_traces_in_memory <- sum(input_vector)\n  matrix_of_ones_for_name <- matrix(1, nrow = total_num_traces_in_memory, ncol = num_of_name_feat)\n  matrix_of_pattern_distort <- t(replicate(n = total_num_traces_in_memory, sample(c(rep(1, num_of_pattern_feat - num_to_distort), rep(-1, num_to_distort)), num_of_pattern_feat)))\n  distort_filter <- cbind(matrix_of_ones_for_name, matrix_of_pattern_distort)\n  distorted_memories <- proto_copies_matrix * distort_filter\nreturn(distorted_memories)\n}\n  \n# Extracting unique prototypes (one for each category) --> yields proto_matrix\n\nextract_unique_proto <- function(input_vector, proto_copies_matrix) {\n  full_unique_proto_matrix <- c()\n  row_counter <- 0\n  for(i in input_vector) {\n    proto_current <- proto_copies_matrix[row_counter + 1, ]\n    full_unique_proto_matrix <- rbind(full_unique_proto_matrix, proto_current)\n    row_counter <- row_counter + i\n  }\nreturn(full_unique_proto_matrix)\n}\n\n# Generating probes from prototypes --> yields probe_matrix\n\ngen_probes_from_proto <- function(proto_matrix) {\n  matrix_of_ones_for_name <- matrix(1, nrow = nrow(proto_matrix), ncol = num_of_name_feat)\n  matrix_of_zeroes_for_pattern <- matrix(0, nrow = nrow(proto_matrix), ncol = num_of_pattern_feat)\n  probe_filter <- cbind(matrix_of_ones_for_name, matrix_of_zeroes_for_pattern)\n  probe_unique_matrix <- proto_matrix * probe_filter\n  return(probe_unique_matrix)\n}\n\n# Echo activation function --> yields activations_matrix\ncalc_echo_activations <- function(probe_matrix, memory_matrix) {\nfull_probe_activations_matrix <- c()\n  for(probe in 1:nrow(probe_matrix)) {\n    all_activations_for_each_probe <- c()\n    for(memory in 1:nrow(memory_matrix)) {\n      num_of_relevant_features <- 0\n      similarity_temp <- 0\n      for(feat in 1:num_of_trace_feat) {\n        current_product <- probe_matrix[probe, feat] * memory_matrix[memory, feat]\n        similarity_temp <- similarity_temp + current_product\n          if(probe_matrix[probe, feat] != 0 & memory_matrix[memory, feat] != 0) {\n            num_of_relevant_features <- num_of_relevant_features + 1\n          }\n    }\n    trace_similarity <- similarity_temp/num_of_relevant_features\n    trace_activation <- trace_similarity ^ 3\n    all_activations_for_each_probe <- c(all_activations_for_each_probe, trace_activation)\n  }\nfull_probe_activations_matrix <- rbind(full_probe_activations_matrix, all_activations_for_each_probe)\n}\nreturn(full_probe_activations_matrix)\n}\n\n# Echo intensity function --> yields intensity_matrix\ncalc_echo_intensity <- function(activations_matrix) {\n  full_intensity_matrix <- c()\n  for(probe in 1:nrow(activations_matrix)) {\n    echo_intensity_for_probe <- sum(activations_matrix[probe, ])\n    full_intensity_matrix <- c(full_intensity_matrix, echo_intensity_for_probe) \n  }\n  return(full_intensity_matrix)\n}\n\n# Echo content function --> yields content_matrix\n\ncalc_echo_content <- function(activations_matrix, memory_matrix) {\n  full_echo_content_matrix <- c()\n  for(probe in 1:nrow(activations_matrix)) {\n    echo_content_for_each_probe <- c()\n    for(feat in 1:num_of_trace_feat){\n      content_temp <- 0\n        for(memory in 1:nrow(memory_matrix)) {\n          current_product <- activations_matrix[probe, memory] * memory_matrix[memory, feat]\n          content_temp <- content_temp + current_product\n        }\n      echo_content_for_each_probe <- c(echo_content_for_each_probe, content_temp)\n    }\n    full_echo_content_matrix <- rbind(full_echo_content_matrix, echo_content_for_each_probe)\n  }\n  return(full_echo_content_matrix)\n}\n\n# Calculating prototype-echo correlation --> yields correlation_matrix\n\ncalc_proto_echo_corr <- function(proto_matrix, content_matrix) {\n  full_correlation_matrix <- c()\n  for(proto in 1:nrow(proto_matrix)) {\n    correlation_current <- cor(proto_matrix[proto, ], content_matrix[proto, ])\n    full_correlation_matrix <- c(full_correlation_matrix, correlation_current)\n  }\n  return(full_correlation_matrix)\n}\n\n\nsimulate_name_as_probe_calc_corr <- function(input_vector) {\n  proto_copies_matrix <- gen_proto_copies(input_vector)\n  memory_matrix <- gen_secondary_memory(input_vector, proto_copies_matrix)\n  proto_matrix <- extract_unique_proto(input_vector, proto_copies_matrix)\n  probe_matrix <- gen_probes_from_proto(proto_matrix)\n  activations_matrix <- calc_echo_activations(probe_matrix, memory_matrix)\n  content_matrix <- calc_echo_content(activations_matrix, memory_matrix)\n  correlation_matrix <- calc_proto_echo_corr(proto_matrix, content_matrix)\n  return(correlation_matrix)\n}\n\n\nsimulate_name_as_probe_calc_corr(c(3, 6, 9))\n\n[1] 0.8021890 0.8395375 0.8793400\n\n\nFinally, after much troubleshooting, it finally seems to work as intended. Time to simulate 20 subjects as Hintzman did and compare the mean prototype-echo correlations.\n\nmy_corr <- t(replicate(20, simulate_name_as_probe_calc_corr(c(3,6,9))))\ncorr_means <- c(mean(my_corr[ ,1]), mean(my_corr[ , 2]), mean(my_corr[ , 3]))\ncorr_sds <- c(sd(my_corr[ ,1]), sd(my_corr[ , 2]), sd(my_corr[ , 3]))\ncorr_means\n\n[1] 0.7777373 0.8415486 0.8586056\n\ncorr_sds\n\n[1] 0.04042487 0.02404416 0.01930278\n\n\nInteresting that my mean prototype-echo correlations are higher than Hintzman’s, while my standard deviations are smaller. I’ll need to verify these values with Matt.\nAs an aside, we had a conversation about cosine similarity and how it doesn’t capture differences in vector length, only in the difference in degree.\n\nWhat implications are there for memory? Hintzman conceptualizes memories as -1s or 1s, similar to how computers are able to code complex information into 0s and 1s. Thus, could vector length be already a built-in consideration, as one of the many features?\nGiven the bar chart diagram Matt drew, where the two bar charts are identical in their positive and negative direction figuration, but differ in the length of the bar, that makes me recall an explanation of correlation similarity that I learnt in class. Do both cosine and correlation similarity not capture vector length?"
  },
  {
    "objectID": "posts/Hintzman5/index.html",
    "href": "posts/Hintzman5/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 5)",
    "section": "",
    "text": "Attempting to replicate the schema-abstraction task\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\n\n# Initial parameters to set \n\nnum_of_pattern_feat <- 13\nnum_of_name_feat <- 10\nnum_of_trace_feat <- num_of_name_feat + num_of_pattern_feat\nfeat_values <- c(rep(1, 50), rep(-1, 50))\nnum_to_distort <- 4\n\n# Generating prototype function\n  \ngen_proto <- function() {\n   proto <- sample(feat_values, num_of_trace_feat, replace = TRUE)\n   return(proto)\n}\n\n# Generating prototype copies --> yields proto_copies_matrix\n\ngen_proto_copies <- function(input_vector) {\n  full_proto_copies_matrix <- c()\n  for(i in input_vector) {\n    proto_single <- gen_proto()\n    proto_copies <- matrix(rep(proto_single, i), nrow = i, ncol = num_of_trace_feat, byrow = TRUE)\n    full_proto_copies_matrix <- rbind(full_proto_copies_matrix, proto_copies)\n  }\n  return(full_proto_copies_matrix)\n}\n\n# Generating traces in secondary memory with distortion --> yields memory_matrix\n\ngen_secondary_memory <- function(input_vector, proto_copies_matrix) {\n  total_num_traces_in_memory <- sum(input_vector)\n  matrix_of_ones_for_name <- matrix(1, nrow = total_num_traces_in_memory, ncol = num_of_name_feat)\n  matrix_of_pattern_distort <- t(replicate(n = total_num_traces_in_memory, sample(c(rep(1, num_of_pattern_feat - num_to_distort), rep(-1, num_to_distort)), num_of_pattern_feat)))\n  distort_filter <- cbind(matrix_of_ones_for_name, matrix_of_pattern_distort)\n  distorted_memories <- proto_copies_matrix * distort_filter\nreturn(distorted_memories)\n}\n  \n# Extracting unique prototypes (one for each category) --> yields proto_matrix\n\nextract_unique_proto <- function(input_vector, proto_copies_matrix) {\n  full_unique_proto_matrix <- c()\n  row_counter <- 0\n  for(i in input_vector) {\n    proto_current <- proto_copies_matrix[row_counter + 1, ]\n    full_unique_proto_matrix <- rbind(full_unique_proto_matrix, proto_current)\n    row_counter <- row_counter + i\n  }\nreturn(full_unique_proto_matrix)\n}\n\n# Generating probes from prototypes --> yields probe_matrix\n\ngen_probes_from_proto <- function(proto_matrix) {\n  matrix_of_ones_for_name <- matrix(1, nrow = nrow(proto_matrix), ncol = num_of_name_feat)\n  matrix_of_zeroes_for_pattern <- matrix(0, nrow = nrow(proto_matrix), ncol = num_of_pattern_feat)\n  probe_filter <- cbind(matrix_of_ones_for_name, matrix_of_zeroes_for_pattern)\n  probe_unique_matrix <- proto_matrix * probe_filter\n  return(probe_unique_matrix)\n}\n\n# Echo activation function --> yields activations_matrix\ncalc_echo_activations <- function(probe_matrix, memory_matrix) {\nfull_probe_activations_matrix <- c()\n  for(probe in 1:nrow(probe_matrix)) {\n    all_activations_for_each_probe <- c()\n    for(memory in 1:nrow(memory_matrix)) {\n      num_of_relevant_features <- 0\n      similarity_temp <- 0\n      for(feat in 1:num_of_trace_feat) {\n        current_product <- probe_matrix[probe, feat] * memory_matrix[memory, feat]\n        similarity_temp <- similarity_temp + current_product\n          if(probe_matrix[probe, feat] != 0 & memory_matrix[memory, feat] != 0) {\n            num_of_relevant_features <- num_of_relevant_features + 1\n          }\n    }\n    trace_similarity <- similarity_temp/num_of_relevant_features\n    trace_activation <- trace_similarity ^ 3\n    all_activations_for_each_probe <- c(all_activations_for_each_probe, trace_activation)\n  }\nfull_probe_activations_matrix <- rbind(full_probe_activations_matrix, all_activations_for_each_probe)\n}\nreturn(full_probe_activations_matrix)\n}\n\n# Echo intensity function --> yields intensity_matrix\ncalc_echo_intensity <- function(activations_matrix) {\n  full_intensity_matrix <- c()\n  for(probe in 1:nrow(activations_matrix)) {\n    echo_intensity_for_probe <- sum(activations_matrix[probe, ])\n    full_intensity_matrix <- c(full_intensity_matrix, echo_intensity_for_probe) \n  }\n  return(full_intensity_matrix)\n}\n\n# Echo content function --> yields content_matrix\n\ncalc_echo_content <- function(activations_matrix, memory_matrix) {\n  full_echo_content_matrix <- c()\n  for(probe in 1:nrow(activations_matrix)) {\n    echo_content_for_each_probe <- c()\n    for(feat in 1:num_of_trace_feat){\n      content_temp <- 0\n        for(memory in 1:nrow(memory_matrix)) {\n          current_product <- activations_matrix[probe, memory] * memory_matrix[memory, feat]\n          content_temp <- content_temp + current_product\n        }\n      echo_content_for_each_probe <- c(echo_content_for_each_probe, content_temp)\n    }\n    full_echo_content_matrix <- rbind(full_echo_content_matrix, echo_content_for_each_probe)\n  }\n  return(full_echo_content_matrix)\n}\n\n# Calculating prototype-echo correlation --> yields correlation_matrix\n\ncalc_proto_echo_corr <- function(proto_matrix, content_matrix) {\n  full_correlation_matrix <- c()\n  for(proto in 1:nrow(proto_matrix)) {\n    correlation_current <- cor(proto_matrix[proto, ], content_matrix[proto, ])\n    full_correlation_matrix <- c(full_correlation_matrix, correlation_current)\n  }\n  return(full_correlation_matrix)\n}\n\n\n# Makes all name features in secondary memory empty --> yields nameless_memory_matrix\nremove_name_feat_from_memory <- function(memory_matrix) {\n  zero_matrix <- matrix(0, nrow = nrow(memory_matrix), ncol = num_of_name_feat)\n  one_matrix <- matrix(1, nrow = nrow(memory_matrix), ncol = num_of_pattern_feat)\n  remove_name_filter <- cbind(zero_matrix, one_matrix)\n  memory_matrix_without_name <- memory_matrix * remove_name_filter\n  return(memory_matrix_without_name)\n}\n\n# Generates old exemplars already in secondary memory (probe a)\nextract_old_exemplars <- function(input_vector, nameless_memory_matrix) {\n  full_old_exemp_matrix <- c()\n  row_counter <- 0\n  for(i in input_vector) {\n    old_exemp_current <- nameless_memory_matrix[row_counter + 1, ]\n    full_old_exemp_matrix <- rbind(full_old_exemp_matrix, old_exemp_current)\n    row_counter <- row_counter + i\n  }\n  return(full_old_exemp_matrix)\n}\n\n# Extracts prototype matrix with empty name features (probe b) --> yields nameless_proto_matrix\n\nextract_unique_proto_without_names <- function(input_vector, proto_copies_matrix) {\n  \n  unique_protos <- extract_unique_proto(input_vector, proto_copies_matrix)\n  zero_matrix <- matrix(0, nrow = nrow(unique_protos), ncol = num_of_name_feat)\n  one_matrix <- matrix(1, nrow = nrow(unique_protos), ncol = num_of_pattern_feat)\n  remove_name_filter <- cbind(zero_matrix, one_matrix)\n  protos_without_name <- unique_protos * remove_name_filter\n  return(protos_without_name)\n}\n\n# Generates new low-distorted exemplars (probe c)\ngen_new_low_distort_exemp <- function(nameless_proto_matrix) {\n  \n  vector_of_ones_for_name <- rep(1, num_of_name_feat)\n  vector_of_pattern_distort <- sample(c(rep(1, num_of_pattern_feat- 2), rep(-1, 2)), num_of_pattern_feat)\n  distort_filter <- c(vector_of_ones_for_name, vector_of_pattern_distort)\n  \n  full_new_low_distort_matrix <- c()\n  for(proto in 1:nrow(nameless_proto_matrix)) {\n    current_new_low_distort <- nameless_proto_matrix[proto, ] * distort_filter\n    full_new_low_distort_matrix <- rbind(full_new_low_distort_matrix, current_new_low_distort)\n  }\nreturn(full_new_low_distort_matrix)\n  }\n\n# Generates new high-distorted exemplars (probe d)\ngen_new_high_distort_exemp <- function(nameless_proto_matrix) {\n  \n  vector_of_ones_for_name <- rep(1, num_of_name_feat)\n  vector_of_pattern_distort <- sample(c(rep(1, num_of_pattern_feat- 4), rep(-1, 4)), num_of_pattern_feat)\n  distort_filter <- c(vector_of_ones_for_name, vector_of_pattern_distort)\n  \n  full_new_high_distort_matrix <- c()\n  for(proto in 1:nrow(nameless_proto_matrix)) {\n    current_new_high_distort <- nameless_proto_matrix[proto, ] * distort_filter\n    full_new_high_distort_matrix <- rbind(full_new_high_distort_matrix, current_new_high_distort)\n  }\nreturn(full_new_high_distort_matrix)\n  }\n\n# Generates random pattern (probe e)\ngen_random_patterns <- function(nameless_proto_matrix) {\n  full_random_pattern_matrix <- c()\n  for(proto in 1:nrow(nameless_proto_matrix)) {\n    name_vector <- nameless_proto_matrix[proto, 1:num_of_name_feat]\n    random_pattern_feat_vector <- sample(feat_values, num_of_pattern_feat, replace = TRUE)\n    current_random_vector <- c(name_vector, random_pattern_feat_vector)\n    full_random_pattern_matrix <- rbind(full_random_pattern_matrix, current_random_vector)\n  }\n  return(full_random_pattern_matrix)\n}\n\n# Applies forgetting to traces in secondary memory\nforgetting_cycle <- function(nameless_memory_matrix, f_value) {\n  memory_matrix_post_forgetting <- c()\n  for(trace in 1:nrow(nameless_memory_matrix)) {\n    forget_filter <- rbinom(num_of_trace_feat, 1, 1 - f_value)\n    current_forgotten_memory <- nameless_memory_matrix[trace, ] * forget_filter\n    memory_matrix_post_forgetting <- rbind(memory_matrix_post_forgetting, current_forgotten_memory)\n  }\n  return(memory_matrix_post_forgetting)\n}\n\n# Extracts only name features from prototypes --> yields category_names\nextract_category_names <- function(input_vector, proto_copies_matrix) {\n  unique_protos <- extract_unique_proto(input_vector, proto_copies_matrix)\n  cat_names <- unique_protos[ , 1:num_of_name_feat]\n  return(cat_names)\n}\n\n# Extracts only name features from echo content --> yields echo_names\nextract_echo_content_name_feat <- function(content_matrix) {\n  echo_content_name <- content_matrix[ , 1:num_of_name_feat]\n}\n\n# Computes category name-echo content name correlation for each probe and assigns echo to category --> yields category_assign_vector\nassign_category_for_each_echo <- function(category_names, echo_names) {\ncategory_for_each_echo <- c()\nfor(e_name in 1:nrow(echo_names)) {\n  cat_echo_name_cor <- c()\n  for(c_name in 1:nrow(category_names)) {\n    current_cor <- cor(echo_names[e_name, ], category_names[c_name, ])\n    cat_echo_name_cor <- c(cat_echo_name_cor, current_cor)\n  }\n  if(max(cat_echo_name_cor) < 0) {\n   current_answer <- -9999\n  } else {\n    current_answer_temp <- which(cat_echo_name_cor == max(cat_echo_name_cor))\n    if(length(current_answer_temp) > 1) {\n    current_answer <- sample(current_answer_temp, 1)\n    } else {\n    current_answer <- current_answer_temp\n    }\n  }\n  category_for_each_echo <- c(category_for_each_echo, current_answer)\n}\nreturn(category_for_each_echo)\n}\n\ncheck_assign_accuracy <- function(category_assign_vector, input_vector, num_types_of_probe){\n  accuracy_vector <- category_assign_vector == rep(1:length(input_vector), num_types_of_probe)\n  return(accuracy_vector)\n  }\n\n\nschema_abstraction <- function(input_vector, num_types_of_probe) {\n  proto_copies_matrix <- gen_proto_copies(input_vector)\n  secondary_memory <- gen_secondary_memory(input_vector, proto_copies_matrix)\n  \n  nameless_memory_matrix <- remove_name_feat_from_memory(secondary_memory)\n  \n  old_exemplars <- extract_old_exemplars(input_vector, nameless_memory_matrix)\n  nameless_proto_matrix <- extract_unique_proto_without_names(input_vector, proto_copies_matrix)\n  new_low_distorts <- gen_new_low_distort_exemp(nameless_proto_matrix)\n  new_high_distorts <- gen_new_high_distort_exemp(nameless_proto_matrix)\n  random_patterns <- gen_random_patterns(nameless_proto_matrix)\n  \n  diverse_probes <- rbind(old_exemplars, nameless_proto_matrix, new_low_distorts, new_high_distorts, random_patterns)\n  activations_matrix <- calc_echo_activations(diverse_probes, secondary_memory)\n  content_matrix <- calc_echo_content(activations_matrix, secondary_memory)\n  \n  category_names <- extract_category_names(input_vector, proto_copies_matrix)\n  echo_names <- extract_echo_content_name_feat(content_matrix)\n  category_vector <- assign_category_for_each_echo(category_names, echo_names)\n  accuracy_vector <- check_assign_accuracy(category_vector, input_vector, num_types_of_probe)\n  return(accuracy_vector)\n}\n\nsimulate_schema_abstraction <- function(input_vector, num_types_of_probe, num_of_simulations, column_names) {\n  results_matrix <- t(replicate(num_of_simulations, schema_abstraction(input_vector, num_types_of_probe)))\n  results_df <- data.frame(results_matrix)\n  colnames(results_df) <- column_names\n  return(results_df)\n}\n\n\n# Test simulating 10 subjects\n\ndf_col_names <- c(\"old_exemp_1\", \"old_exemp_2\", \"old_exemp_3\", \"proto_1\", \"proto_2\", \"proto_3\", \"low_dist_1\", \"low_dist_2\", \"low_dist_3\", \"high_dist_1\", \"high_dist_2\", \"high_dist_3\", \"random_1\", \"random_2\", \"random_3\")\nhintz_input <- c(3, 6, 9)\nschema_sim <- simulate_schema_abstraction(hintz_input, 5, 10, df_col_names)\n\nThings I’m unsure about:\n\nThe probes contain empty name features, right?\nThe forgetting affects all features, including name features, right?\nDoes it matter which old exemplar I choose? (I’ve just chosen the first of each category)\n\nThere’s something wrong with the code, especially when the number of simulations gets too high (e.g. 100). It seems the problem lies with the assign_category_for_each_echo function; there could be a function that is occasionally turning up an NA value?"
  },
  {
    "objectID": "posts/Hintzman1988a/index.html",
    "href": "posts/Hintzman1988a/index.html",
    "title": "Hintzman’s (1988) MINERVA (Part 1)",
    "section": "",
    "text": "Moving on to Hintzman (1988)\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations_3 <- function(probe, mem) {\n  \n  as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))^3)\n}\n\n# Item generation function (borrowed from Matt)\n\ngenerate_item <- function(item_size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = item_size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# Item matrix (original, before applying learning rate) function\n\ngen_item_matrix <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3)) {\n  item_matrix <- t(replicate(n = matrix_size, generate_item(item_size = item_size, prob = prob)))\n  return(item_matrix)\n}\n\n# Form probe matrix i.e. item_matrix + 4 more random items\n\ngen_probes <- function(item_matrix, prob = c(1/3, 1/3, 1/3), max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  random_items <- t(replicate(n = num_of_traces_per_freq, generate_item(item_size = ncol(item_matrix), prob = prob)))\n  probe_matrix <- rbind(random_items, item_matrix)\n  return(probe_matrix)\n}\n\n# Form secondary memory -- create encoded matrix (i.e. apply learning rate) and input varying frequencies of items\n\ngen_secondary_mem <- function(item_matrix, l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n \n  freq_multiplier <- c()\n  for (i in 1:max_num_of_copies) {\n    current_multiplier <- rep(i, num_of_traces_per_freq)\n    freq_multiplier <- c(freq_multiplier, current_multiplier)\n  }\n  secondary_memory <- c()\n  for(j in 1:length(freq_multiplier)) {\n    current_rows <- matrix(rep(item_matrix[j, ], freq_multiplier[j]), nrow = freq_multiplier[j], ncol = ncol(item_matrix), byrow = TRUE)\n    secondary_memory <- rbind(secondary_memory, current_rows)\n  }\n   learning_matrix <- t(replicate(n = nrow(secondary_memory), sample(c(0,1), size = ncol(secondary_memory), prob = c(1 - l_value, l_value), replace = TRUE)))\n  encoded_memory <- secondary_memory * learning_matrix\n return(encoded_memory)\n}\n\n\n\n# Calculate activations for multiple probes\n\ncalc_activs_for_mult_probes <- function(probe_matrix, secondary_memory) {\n  activations_matrix <- c()\n  for(i in 1:nrow(probe_matrix)) {\n    current_activs <- get_activations_3(probe_matrix[i, ], secondary_memory)\n    activations_matrix <- rbind(activations_matrix, current_activs)\n  }\n  return(activations_matrix)\n}\n\n# Convert activations matrix to transformed intensity matrix ready for plotting\n\nconvert_to_intensity_mat <- function(activations_matrix, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  intensity_vector <- rowSums(activations_matrix)\n  intensity_matrix <- matrix(intensity_vector, nrow = max_num_of_copies + 1, ncol = num_of_traces_per_freq, byrow = TRUE)\n  return(intensity_matrix)\n}\n\n\n# Overall simulation function\n\nsim_intensity_once <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  secondary_memory <- gen_secondary_mem(item_matrix, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  activations_matrix <- calc_activs_for_mult_probes(probe_matrix, secondary_memory)\n  intensity_matrix <- convert_to_intensity_mat(activations_matrix, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  return(intensity_matrix)\n}\n\nsim_intensity_multiple <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  raw_intensity_matrix <- c()\n  for(i in 1:n_of_sim) {\n    temp_intensity <- sim_intensity_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n    raw_intensity_matrix <- cbind(raw_intensity_matrix, temp_intensity)\n  }\n  row_names <- as.data.frame(0:max_num_of_copies)\n  names(row_names) <- \"Frequency\"\n  intensity_df <- bind_cols(row_names, data.frame(raw_intensity_matrix)) %>%\n      pivot_longer(!Frequency, names_to = \"Drop\", values_to = \"Intensity\") %>% select(\"Frequency\", \"Intensity\")\n  return(intensity_df)\n}\n\n\ndf_intensity <- sim_intensity_multiple(1000)\nggplot(df_intensity, aes(x = Intensity, color = factor(Frequency))) + geom_density(show.legend = TRUE) + xlim(-1, 2)\n\nWarning: Removed 4 rows containing non-finite values (stat_density).\n\n\n\n\n\nFinally, I managed to reproduce the graph. I had gotten the general shape earlier (see below) but the mean values were too small and the graphs weren’t spread out enough. It turns out that I was misunderstanding the L value. The L value affects sampling rate (i.e. how many features are selected to be copied versus nullified), not trace quality (i.e. the actual number of features in memory that are different from others). Thus, I had assumed that all copies of the same original item in secondary memory would i) have the same same number of features learnt, ii) have the exact same features. I was wrong on both counts.\nI fixed this by creating the copies first before applying the learning rate, instead of the other way around. This way, the learning rate applies noise, so even among copies of the same item, there is variation in the number of features changes. This is due to the fact that, for some traces, feature values that were originally 0 may be selected to be nullified (resulting in essentially no change), whereas for other traces, feature values that were originally 1 or -1 may be selected to be nullified (resulting in a meaningful change)."
  },
  {
    "objectID": "posts/Hintzman1988b/index.html",
    "href": "posts/Hintzman1988b/index.html",
    "title": "Hintzman’s (1988) MINERVA (Part 2)",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations_3 <- function(probe, mem) {\n  \n  as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))^3)\n}\n\n# Item generation function (borrowed from Matt)\n\ngenerate_item <- function(item_size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = item_size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# Item matrix (original, before applying learning rate) function\n\ngen_item_matrix <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3)) {\n  item_matrix <- t(replicate(n = matrix_size, generate_item(item_size = item_size, prob = prob)))\n  return(item_matrix)\n}\n\n# Form secondary memory -- create encoded matrix (i.e. apply learning rate) and input varying frequencies of items\n\ngen_secondary_mem <- function(item_matrix, l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  learning_matrix <- replicate(n = ncol(item_matrix), sample(c(0,1), size = nrow(item_matrix), prob = c(1 - l_value, l_value), replace = TRUE))\n  encoded_matrix <- item_matrix * learning_matrix\n  \n  freq_multiplier <- c()\n  for (i in 1:max_num_of_copies) {\n    current_multiplier <- rep(i, num_of_traces_per_freq)\n    freq_multiplier <- c(freq_multiplier, current_multiplier)\n  }\n  secondary_memory <- c()\n  for(i in 1:nrow(encoded_matrix)) {\n    current_rows <- matrix(encoded_matrix[i , ], nrow = freq_multiplier[i], ncol = ncol(encoded_matrix), byrow = TRUE)\n    secondary_memory <- rbind(secondary_memory, current_rows)\n  }\n return(secondary_memory)\n}\n\n# Form probe matrix i.e. item_matrix + 4 more random items\n\ngen_probes <- function(item_matrix, prob = c(1/3, 1/3, 1/3), max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  random_items <- t(replicate(n = num_of_traces_per_freq, generate_item(item_size = ncol(item_matrix), prob = prob)))\n  probe_matrix <- rbind(random_items, item_matrix)\n  return(probe_matrix)\n}\n\n# Calculate activations for multiple probes\n\ncalc_activs_for_mult_probes <- function(probe_matrix, secondary_memory) {\n  activations_matrix <- c()\n  for(i in 1:nrow(probe_matrix)) {\n    current_activs <- get_activations_3(probe_matrix[i, ], secondary_memory)\n    activations_matrix <- rbind(activations_matrix, current_activs)\n  }\n  return(activations_matrix)\n}\n\n# Convert activations matrix to transformed intensity matrix ready for plotting\n\nconvert_to_intensity_mat <- function(activations_matrix, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  intensity_vector <- rowSums(activations_matrix)\n  intensity_matrix <- matrix(intensity_vector, nrow = max_num_of_copies + 1, ncol = num_of_traces_per_freq, byrow = TRUE)\n  return(intensity_matrix)\n}\n\n\n# Overall simulation function\n\nsim_intensity_once <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  secondary_memory <- gen_secondary_mem(item_matrix, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  activations_matrix <- calc_activs_for_mult_probes(probe_matrix, secondary_memory)\n  intensity_matrix <- convert_to_intensity_mat(activations_matrix, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  return(intensity_matrix)\n}\n\nsim_intensity_multiple <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  raw_intensity_matrix <- c()\n  for(i in 1:n_of_sim) {\n    temp_intensity <- sim_intensity_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n    raw_intensity_matrix <- cbind(raw_intensity_matrix, temp_intensity)\n  }\n  row_names <- as.data.frame(0:max_num_of_copies)\n  names(row_names) <- \"Frequency\"\n  intensity_df <- bind_cols(row_names, data.frame(raw_intensity_matrix)) %>%\n      pivot_longer(!Frequency, names_to = \"Drop\", values_to = \"Intensity\") %>% select(\"Frequency\", \"Intensity\")\n  return(intensity_df)\n}\n\n\ndf_intensity <- sim_intensity_multiple(1000)\nggplot(df_intensity, aes(x = Intensity, color = factor(Frequency))) + geom_density(show.legend = TRUE) + xlim(-1, 2)\n\nWarning: Removed 255 rows containing non-finite values (stat_density).\n\n\n\n\n\n\n\n\n\nintensity_df <- sim_intensity_multiple(250, matrix_size = 16, l_value = 0.8, max_num_of_copies = 4)\ndiscrim_intensity_df <- intensity_df %>% mutate(freq_judgement = case_when(\n  Intensity < 0.17 ~ 0,\n  Intensity >= 0.17 & Intensity < 0.67 ~ 1,\n  Intensity >= 0.67 & Intensity < 1.33 ~ 2,\n  Intensity >= 1.33 & Intensity < 2 ~ 3,\n  Intensity >= 2 ~ 4\n))\n\nfreq_judgment_df <- data.frame(table(factor(discrim_intensity_df$Frequency, levels = 0:4), factor(discrim_intensity_df$freq_judgement, levels = 0:4))) %>% mutate(Freq = Freq/1000) %>% rename(Real_freq = Var1, Freq_judg = Var2, Proportion_of_resp = Freq)\n\n\nggplot(freq_judgment_df, aes(x = Freq_judg, y = Proportion_of_resp, group = Real_freq, color = Real_freq)) + geom_path() + geom_point()\n\n\n\n\nI’ve reproduced the rough shape of the graph, but the values are lower than in Hintzman’s (1988) graph. It is also strange that the proportion of correct responses when frequency = 3 is lower than that for frequency = 4. The general trend of lower peaks as frequency_judgment increases is not reflected in my graph.\n\n\n\n\n\n\n# Initial parameters\n\nnum_feat_shallow <- 10\nnum_feat_deep <- 15\nnum_of_traces_per_freq <- 4\nmax_num_of_copies <- 5\n\n# Creating the appropriate number of copies for each item\n\ngen_item_mat_copies <- function(item_matrix) {\n  \n  freq_multiplier <- c()\n  for (i in 1:max_num_of_copies) {\n    current_multiplier <- rep(i, num_of_traces_per_freq)\n    freq_multiplier <- c(freq_multiplier, current_multiplier)\n  }\n  \n  item_mat_copies <- c()\n  for(i in 1:nrow(item_matrix)) {\n    current_rows <- matrix(item_matrix[i , ], nrow = freq_multiplier[i], ncol = ncol(item_matrix), byrow = TRUE)\n    item_mat_copies <- rbind(item_mat_copies, current_rows)\n  }\n return(item_mat_copies)\n}\n\n# Functions for shallow learning filter and deep learning filter\n\ngen_shallow_learning_filter <- function(l_value1 = 0.6, l_value2 = 0) {\n  matrix_shallow_part_1 <- sample(c(0,1), size = num_feat_shallow, prob = c(1 - l_value1, l_value1), replace = TRUE)\n  matrix_shallow_part_2 <- sample(c(0,1), size = num_feat_deep, prob = c(1 - l_value2, l_value2), replace = TRUE)\n  matrix_shallow <- c(matrix_shallow_part_1, matrix_shallow_part_2)\n  return(matrix_shallow)\n}\n\ngen_deep_learning_filter <- function(l_value1 = 0.6, l_value2 = 0) {\n  matrix_deep_part_1 <- sample(c(0,1), size = num_feat_shallow, prob = c(1 - l_value2, l_value2), replace = TRUE)\n  matrix_deep_part_2 <- sample(c(0,1), size = num_feat_deep, prob = c(1 - l_value1, l_value1), replace = TRUE)\n  matrix_deep <- c(matrix_deep_part_1, matrix_deep_part_2)\n  return(matrix_deep)\n}\n\n# Generating secondary memory - putting item copies matrix through learning filters\n\ngen_orienting_secondary_mem <- function(item_copies_matrix, l_value1 = 0.6, l_value2 = 0) {\n  \norienting_sec_mem <- c()\nfor(i in 1:nrow(item_copies_matrix)) {\n  if(i %% 2 == 0) {\n    deep_filter <- gen_deep_learning_filter(l_value1 = l_value1, l_value2 = l_value2)\n    orienting_sec_mem <- rbind(orienting_sec_mem, item_copies_matrix[i, ] * deep_filter)\n  } else {\n  shallow_filter <- gen_shallow_learning_filter(l_value1 = l_value1, l_value2 = l_value2)\n    orienting_sec_mem <- rbind(orienting_sec_mem, item_copies_matrix[i, ] * shallow_filter)\n    }\n}\nreturn(orienting_sec_mem)\n}\n\n# Convert activations matrix to intensity matrix (slightly different from previous function)\n\norient_convert_to_intensity_matrix <- function(activations_matrix, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  intensity_vector <- rowSums(activations_matrix)\n  intensity_matrix <- matrix(intensity_vector, nrow = max_num_of_copies + 1, ncol = num_of_traces_per_freq/2, byrow = TRUE)\n  return(intensity_matrix)\n}\n\n\n# Encode traces into secondary memory\nitem_mat <- gen_item_matrix(item_size = 25)\nitem_copies_mat <- gen_item_mat_copies(item_mat)\nor_sec_mem <- gen_orienting_secondary_mem(item_copies_mat)\nprobe_mat <- gen_probes(item_mat)\n\nactiv_mat_raw <- calc_activs_for_mult_probes(probe_mat, or_sec_mem)\nactiv_mat_shallow <- c()\nactiv_mat_deep <- c()\nfor(i in 1:nrow(activ_mat_raw)) {\n  if(i %% 2 == 0) {\n    activ_mat_deep <- rbind(activ_mat_deep, activ_mat_raw[i, ])\n  } else {\n    activ_mat_shallow <- rbind(activ_mat_shallow, activ_mat_raw[i, ])\n  }\n}\n\n\n\nintensity_shallow <- orient_convert_to_intensity_matrix(activ_mat_shallow)\n\n\nsim_orienting_once <- function(matrix_size = 20, item_size = 25, prob = c(1/3, 1/3, 1/3), l_value1 = 0.6, l_value2 = 0) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  item_matrix_copies <- gen_item_mat_copies(item_matrix)\n  orienting_memory <- gen_orienting_secondary_mem(item_matrix_copies, l_value1 = l_value1, l_value2 = l_value2)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  raw_activations <- calc_activs_for_mult_probes(probe_matrix, orienting_memory)\n  \n  shallow_activations <- c()\n  deep_activations <- c()\n  for(i in 1:nrow(raw_activations)) {\n  if(i %% 2 == 0) {\n    deep_activations <- rbind(deep_activations, raw_activations[i, ])\n  } else {\n    shallow_activations <- rbind(shallow_activations, raw_activations[i, ])\n  }\n  }\n  intensity_matrix <- rbind(orient_convert_to_intensity_matrix(shallow_activations), orient_convert_to_intensity_matrix(deep_activations))\n  return(intensity_matrix)\n  }\n  \n\nsim_orienting_multiple <- function(n_of_sim, matrix_size = 20, item_size = 25, prob = c(1/3, 1/3, 1/3), l_value1 = 0.6, l_value2 = 0) {\n  raw_intensity_matrix <- c()\n  for(i in 1:n_of_sim) {\n    temp_intensity <- sim_orienting_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value1 = l_value1, l_value2 = l_value2)\n    raw_intensity_matrix <- cbind(raw_intensity_matrix, temp_intensity)\n  }\n  level_of_process <- c(rep(\"shallow\", max_num_of_copies + 1), rep(\"deep\", max_num_of_copies + 1))\n frequency <- rep(0:max_num_of_copies, 2)\n  intensity_df <- bind_cols(level_of_process, frequency, data.frame(raw_intensity_matrix)) %>% rename(level_of_process = ...1, frequency = ...2) %>% \n    pivot_longer(cols = starts_with(\"X\"), names_to = \"Drop\", values_to = \"Intensity\") %>% select(!Drop)\n  return(intensity_df)\n}\n\n\norient_intensity <- sim_orienting_multiple(1000)\n\nNew names:\n• `` -> `...1`\n• `` -> `...2`\n\nsummarized_intensity <- orient_intensity %>% group_by(level_of_process, frequency) %>% summarize(mean_intensity = mean(Intensity))\n\n`summarise()` has grouped output by 'level_of_process'. You can override using\nthe `.groups` argument.\n\n\n\nggplot(summarized_intensity, aes(x = frequency, y = mean_intensity, group = level_of_process, color = level_of_process)) + geom_path() + geom_point()\n\n\n\n\nThis graph is very different from Hintzman’s. I think I may have made the same mistake as last time with applying the learning rate before making copies, resulting in identically learnt traces."
  },
  {
    "objectID": "posts/Attention1/index.html",
    "href": "posts/Attention1/index.html",
    "title": "A MINERVA model for attention",
    "section": "",
    "text": "Applying MINERVA to the Stroop task\n\nset.seed(30)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(gtools)\nlibrary(bench)\n\ncolors <- c(\"red\", \"yellow\", \"green\", \"blue\")\n\nnum_of_feat <- 10\nnum_of_congruent_trials <- 48\nnum_of_incongruent_trials <- 48\nattention_dampeners <- c(.5, .6, .7, .8, .9, 1)\ncon_trials_per_color <- num_of_congruent_trials/length(colors)\ncongruent_answer_column <- c()\nfor(i in 1:length(colors)){\n  current_correct_answer <- rep(colors[i], con_trials_per_color)\n  congruent_answer_column <- c(congruent_answer_column, current_correct_answer)\n}\n\nincon_permuts <- permutations(length(colors), 2)\nnum_of_repeat_incon_trials <- num_of_incongruent_trials/nrow(incon_permuts)\nincongruent_answer_column <- c()\nfor(i in 1:nrow(incon_permuts)){\n  current_correct_answer <- rep(colors[incon_permuts[i, 1]], num_of_repeat_incon_trials)\n  incongruent_answer_column <- c(incongruent_answer_column, current_correct_answer)\n}\n\nfull_correct_answer_column <- c(congruent_answer_column, incongruent_answer_column)\n\ngen_color <- function() {\n  color <- sample(c(1, 0, -1), num_of_feat, replace = TRUE, prob = c(1/3, 1/3, 1/3))\n  return(color)\n}\n\npatterns <- c()\nfor(i in 1:length(colors)) {\n  temp_pattern <- gen_color()\n  patterns <- rbind(patterns, temp_pattern)\n}\n\nrow.names(patterns) <- colors\nblank <- numeric(length = num_of_feat)\n\nmemory <- c()\nfor(i in 1:nrow(patterns)) {\n  ink_set <- c(patterns[i, ], blank, patterns[i, ], blank)\n  word_set <- c(blank, patterns[i, ], blank, patterns[i, ])\n  congruent_set <- c(patterns[i, ], patterns[i, ], patterns[i, ], patterns[i, ])\n  memory <- rbind(memory, ink_set, word_set, congruent_set)\n}\nfull_memory <- rbind(memory, memory, memory, memory, memory)\n\n\ncongruent_probes <- c()\nfor(i in 1:nrow(patterns)){\n  current_trials <- c(patterns[i, ], patterns[i, ], blank, blank)\n  current_combi <- matrix(current_trials, nrow = con_trials_per_color, ncol = length(current_trials), byrow=TRUE)\n  congruent_probes <- rbind(congruent_probes, current_combi)\n}\n\n\nincongruent_probes <- c()\nfor(i in 1:nrow(incon_permuts)){\n  current_trials <- c(patterns[incon_permuts[i, 1], ], patterns[incon_permuts[i, 2], ], blank, blank)\n  current_combi <- matrix(current_trials, nrow = num_of_repeat_incon_trials, ncol = length(current_trials), byrow = TRUE)\n  incongruent_probes <- rbind(incongruent_probes, current_combi)\n}\n\nfull_probes <- rbind(congruent_probes, incongruent_probes)\n\n# Activation function for single probe (borrowed from Matt)\nget_activations_3 <- function(probe, mem) {\n  \n  as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))^3)\n}\n\n# Function for calculating echo content for single probe (modified from previous exercise)\ncalc_echo_content_for_each_probe <- function(activations_vector, memory_matrix) {\n    echo_content_for_each_probe <- c()\n    for(feat in 1:ncol(memory_matrix)){\n      content_temp <- 0\n        for(memory in 1:nrow(memory_matrix)) {\n          current_product <- activations_vector[memory] * memory_matrix[memory, feat]\n          content_temp <- content_temp + current_product\n        }\n      echo_content_for_each_probe <- c(echo_content_for_each_probe, content_temp)\n  }\n  return(echo_content_for_each_probe)\n}\n\n# Function for generating dampened content\n\ndampen_distractor <- function(probe_content){\n  non_zero <- FALSE\n  while(non_zero == FALSE){\n  dampen_value <- sample(attention_dampeners, 1)\n  attention_dampening_filter <- c(rep(1, num_of_feat*3), sample(c(0, 1), num_of_feat, replace = TRUE, prob = c(1 - dampen_value, dampen_value)))\n   dampened_content <- probe_content * attention_dampening_filter\n   if(sum(abs(dampened_content[(num_of_feat*3+1):(num_of_feat*4)])) != 0){\n     non_zero <- TRUE\n   }\n  }\n     dampened_output <- c(dampened_content, dampen_value)\n   return(dampened_output)\n}\n\n# Function for one Stroop trial (dampening mechanism)\nstroop_one_trial_dampen <- function(probe_vector, memory_matrix, color_patterns){\n # Attention dampening filter dampens attention to words (in favor of ink color)\n  success <- FALSE\n  start_time <- Sys.time()\n  while(success == FALSE) {\n  probe_activation <- get_activations_3(probe_vector, memory_matrix)\n   probe_content <- calc_echo_content_for_each_probe(probe_activation, memory_matrix)\n   dampened_output <- dampen_distractor(probe_content)\n   dampened_content <- dampened_output[1:(num_of_feat*4)]\n   dampen_value <- dampened_output[num_of_feat*4 + 1]\n   trial_ink_corrs <- c()\n   trial_word_corrs <- c()\n   for(j in 1:nrow(color_patterns)){\n    ink_corr <- cor(color_patterns[j, ], dampened_content[(num_of_feat*2 + 1) : (num_of_feat*3)])\n    if(is.na(ink_corr)){\n      ink_corr <- 0\n    }\n    word_corr <- cor(color_patterns[j, ], dampened_content[(num_of_feat*3 + 1) : (num_of_feat*4)])\n    if(is.na(word_corr)){\n      word_corr <- 0\n    }\n    trial_ink_corrs <- c(trial_ink_corrs, ink_corr)\n    trial_word_corrs <- c(trial_word_corrs, word_corr)\n }\n   if(max(trial_ink_corrs) >= max(trial_word_corrs)){\n     response <- color_patterns[which.max(trial_ink_corrs), ]\n   } else {\n     response <- color_patterns[which.max(trial_word_corrs), ]\n   }\n   accuracy <- all(response == probe_vector[1:num_of_feat])\n   success <- accuracy == TRUE\n  }\n  end_time <- Sys.time()\n  if(all(probe_vector[1:num_of_feat] == probe_vector[(num_of_feat+1):(num_of_feat*2)])){\n    condition <- \"congruent\"\n  } else {\n    condition <- \"incongruent\"\n  }\n  trial_outcome <- data.frame(condition = condition, dampener = dampen_value, time_elapsed = end_time - start_time, accuracy = accuracy)\n   return(trial_outcome)\n}\n\n# Function for full Stroop task simulation (dampening mechanism)\nfull_stroop_sim_dampen <- function(probe_matrix, memory_matrix, color_patterns) {\n  all_outcomes <- c()\n  for(i in 1:nrow(probe_matrix)){\n    trial_outcome <- stroop_one_trial_dampen(probe_matrix[i, ], memory_matrix, color_patterns)\n    all_outcomes <- bind_rows(all_outcomes, trial_outcome)\n  }\n  all_outcomes <- all_outcomes %>% mutate(correct_answer = full_correct_answer_column)\n  return(all_outcomes)\n}\n\n\nstroop_results <- data.frame()\n\nfor(i in 1:100){\n  current_participant <- full_stroop_sim_dampen(full_probes, full_memory, patterns)\n  stroop_results <- bind_rows(stroop_results, current_participant)\n}\n\n\nstroop_means <- stroop_results %>% group_by(condition, dampener) %>% summarize(mean_dur = mean(time_elapsed))\n\n`summarise()` has grouped output by 'condition'. You can override using the\n`.groups` argument.\n\nggplot(stroop_means, aes(x = dampener, y = mean_dur, color = condition)) + geom_line()\n\nDon't know how to automatically pick scale for object of type difftime. Defaulting to continuous.\n\n\n\n\n\nThe trend for the congruent condition makes sense, since the amount of dampening of the distractor should not influence performance, since the target and distractor do not compete for the response. However, for the incongruent condition, it doesn’t really make sense that the more the distractors is dampened, the longer it takes to give the correct response. In fact, that’s the opposite of what we would expect."
  },
  {
    "objectID": "posts/Attention2/index.html",
    "href": "posts/Attention2/index.html",
    "title": "A MINERVA model for attention (Part 2)",
    "section": "",
    "text": "# Define Functions\n\n# function to generate a single item\ngenerate_item <- function(size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# compute activations\nget_activations <- function(probe,memory){\n   as.numeric(((probe %*% t(memory)) / rowSums(t((probe == 0) * t(memory == 0)) == 0)))\n}\n\n# generate echo\nget_echo <- function(probe, mem, tau=3, output='intensity') {\n    activations <- get_activations(probe,mem)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\nget_cosine_sim <- function(echo_features, response_matrix) {\n  similarity_df <- data.frame()\n  \n  for(i in 1:nrow(response_matrix)){\n    current_sim <- cosine(echo_features, response_matrix[i, ])\n    similarity_df <- rbind(similarity_df, current_sim)\n  }\n  names(similarity_df) <- \"similarity\"\n  row.names(similarity_df) <- c(\"red\", \"green\", \"blue\", \"yellow\")\n  return(similarity_df)\n}\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lsa)\n\nLoading required package: SnowballC\n\n# Generate Stroop dataframe\n\ncongruent_items <- tibble(word = c(\"red\",\"green\",\"blue\",\"yellow\"),\n                          color = c(\"red\",\"green\",\"blue\",\"yellow\"),\n                          response = c(\"red\",\"green\",\"blue\",\"yellow\"),\n                          congruency = rep(\"con\",4))\n\nincongruent_items <- tibble(word = c(\"red\",\"red\",\"red\",\n                                     \"green\",\"green\",\"green\",\n                                     \"blue\",\"blue\",\"blue\",\n                                     \"yellow\",\"yellow\",\"yellow\"),\n                          color = c(\"green\",\"blue\",\"yellow\",\n                                    \"red\",\"blue\",\"yellow\",\n                                    \"red\",\"green\",\"yellow\",\n                                    \"red\",\"green\",\"blue\"),\n                          response = c(\"green\",\"blue\",\"yellow\",\n                                    \"red\",\"blue\",\"yellow\",\n                                    \"red\",\"green\",\"yellow\",\n                                    \"red\",\"green\",\"blue\"),\n                          congruency = rep(\"inc\",12))\n\n\n\ntraining_trials <- congruent_items            \n\nstroop_trials <- congruent_items %>% \n  slice(rep(1:n(),each=3)) %>%\n  rbind(incongruent_items) %>%\n  slice(sample(1:n())) %>%\n  slice(rep(1:n(),each=2))\n\n# Function for simulating multiple subjects on Stroop task\n\nsim_multiple_stroop <- function(training, stroop, n_of_sim) {\n  all_sub_results <- data.frame()\n\n  for(i in 1:n_of_sim){\n# Make vector representations\n\nword <- t(replicate(4, generate_item(size=20,prob=c(1/3,1/3,1/3))))\nrow.names(word) <- c(\"red\",\"green\",\"blue\",\"yellow\")\n\ncolor <- t(replicate(4, generate_item(size=20,prob=c(1/3,1/3,1/3))))\nrow.names(color) <- c(\"red\",\"green\",\"blue\",\"yellow\")\n\nresponse <- t(replicate(4, generate_item(size=20,prob=c(1/3,1/3,1/3))))\nrow.names(response) <- c(\"red\",\"green\",\"blue\",\"yellow\")\n\n## Trial constructor\n\nminerva_trials <- function(df,vec_length=20){\n  minerva_matrix <- matrix(0,nrow = dim(df)[1],ncol=vec_length*3)\n  \n  for(i in 1:dim(df)[1]){\n    minerva_matrix[i,] <- c(word[df[i,]$word, ], color[df[i,]$color, ], response[df[i,]$response, ])\n  }\n  \n  return(minerva_matrix)\n}\n\ntraining_matrix <- minerva_trials(training)\ntrial_matrix <- minerva_trials(stroop)\n\n## run model\n\nsubject_results <- data.frame()\n\nmemory <- training_matrix\n\nfor(i in 1:dim(trial_matrix)[1]){\n  \n  probe <- c(trial_matrix[i,1:40],rep(0,20))\n  model_output <- get_echo(probe, memory, tau=3, output = \"both\")\n  similarities <- get_cosine_sim(model_output$echo[41:60], response)\n  memory <- rbind(memory,trial_matrix[i,]) # So the correct answer for that trial is saved in memory, even if the answer was wrong?\n  \n  trial_data <- data.frame(red = similarities['red', ],\n                           green = similarities['green', ],\n                           blue = similarities['blue', ],\n                           yellow = similarities['yellow', ],\n                           intensity = model_output$intensity,\n                           max_sim = row.names(similarities)[which.max(similarities$similarity)],\n                           max_sim_num = max(similarities),\n                           coactivation = max(similarities)/(sum(abs(similarities))))\n  \n  subject_results <- rbind(subject_results,trial_data)\n}\n\nrow.names(subject_results) <- 1:dim(subject_results)[1]\n\n## analyze data \n\none_sub <- cbind(stroop,subject_results)\n\none_sub <- one_sub %>%\n  mutate(correct = response == max_sim)\n\nsummarized_one_sub <- one_sub %>%\n  group_by(congruency) %>%\n  summarize(mean_correct = mean(correct),\n            mean_max = mean(max_sim_num),\n            mean_coactivation = mean(coactivation))\nall_sub_results <- rbind(all_sub_results, summarized_one_sub)\n  }\nreturn(all_sub_results)\n}\n\n\n# Equal number of congruent and incongruent trials\n\nequal_sim <- sim_multiple_stroop(training_trials, stroop_trials, 100)\nggplot(equal_sim, aes(x = congruency, y = mean_correct)) + geom_boxplot()\n\n\n\n# 75% congruent and 25% incongruent trials\n\nstroop_75_25 <- congruent_items %>% \n  slice(rep(1:n(),each=9)) %>%\n  rbind(incongruent_items) %>%\n  slice(sample(1:n()))\n\nmore_congruent_sim <- sim_multiple_stroop(training_trials, stroop_75_25, 100)\nggplot(more_congruent_sim, aes(x = congruency, y = mean_correct)) + geom_boxplot()\n\n\n\n# 25% congruent and 75% incongruent trials\n\nrep_incongruent <- incongruent_items %>% slice(rep(1:n(), each = 3))\nstroop_25_75 <- congruent_items %>%\n  slice(rep(1:n(),each=3)) %>%\n  rbind(rep_incongruent) %>%\n  slice(sample(1:n()))\n\nmore_incongruent_sim <- sim_multiple_stroop(training_trials, stroop_25_75, 100)\nggplot(more_incongruent_sim, aes(x = congruency, y = mean_correct)) + geom_boxplot()\n\n\n\n\nFixed an error and finally got the expected results."
  },
  {
    "objectID": "posts/DiscrepancyEncoding/index.html",
    "href": "posts/DiscrepancyEncoding/index.html",
    "title": "Discrepancy Encoding in MINERVA (Part 1)",
    "section": "",
    "text": "Applying discrepancy encoding to frequency judgments\nThe goal of this exploration is to look into how the discrepancy encoding assumption introduced in MINERVA-AL (Jamieson et al., 2012) affects Hintzman’s (1988) frequency judgment simulations.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\n\nmax_frequency <- 5\nnum_of_item_per_freq <- 4\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(RsemanticLibrarian::cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Item generation function (borrowed from Matt)\n\ngenerate_item <- function(item_size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = item_size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# Item matrix (original, before applying learning rate) function\n\ngen_item_matrix <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3)) {\n  item_matrix <- t(replicate(n = matrix_size, generate_item(item_size = item_size, prob = prob)))\n  return(item_matrix)\n}\n\n# Form probe matrix i.e. item_matrix multiplied by respective frequencies + 4 more random items\n\ngen_probes <- function(item_matrix, prob = c(1/3, 1/3, 1/3), max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  freq_multiplier <- rep(1:max_num_of_copies, each = num_of_traces_per_freq)\n  probe_matrix <- c()\n  for(i in 1:length(freq_multiplier)) {\n    current_rows <- matrix(rep(item_matrix[i, ], freq_multiplier[i]), nrow = freq_multiplier[i], ncol = ncol(item_matrix), byrow = TRUE)\n    probe_matrix <- rbind(probe_matrix, current_rows)\n}\n  return(probe_matrix)\n}\n\n# Vector of frequencies for each item\nfrequency_vec <- rep(c(1:max_frequency), each = num_of_item_per_freq)\n\n# Rows to extract (with experienced frequency = designated frequency)\nimportant_rows <- numeric(length = max_frequency*num_of_item_per_freq)\ntracker <- 0\n  for(h in 1:length(frequency_vec)){\n    current_value <- tracker + frequency_vec[h]\n    important_rows[h] <- current_value\n    tracker <- current_value\n  }\n\n\n# Overall simulation function (no discrepancy encoding)\n\nsim_intensity_once <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  df_items <- data.frame(subject = NA, item = 1:nrow(item_matrix), frequency = frequency_vec)\n  \n  trial_df <- data.frame()\n  for(i in 1:nrow(df_items)){\n    current_rows <- bind_rows(replicate(df_items[i, ]$frequency, df_items[i, ], simplify = FALSE))\n    trial_df <- rbind(trial_df, current_rows)\n  }\n  trial_df <- trial_df %>% slice(sample(1:n()))\n  \n  # Starting state secondary memory\nmemory <- rbind(t(replicate(n = 4, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3)))), matrix(0, nrow = nrow(trial_df), ncol = item_size))\n\n# Caluclating intensities and storing each probe\n  for(i in 1:nrow(trial_df)){\n    current_intensity <- get_echo(item_matrix[trial_df[i, ]$item, ], memory, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- current_intensity\n    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))\n    learned_probe <- item_matrix[trial_df[i, ]$item, ] * learning_filter\n    memory[4 + i, ] <- learned_probe\n  }\ntrial_arr_slice <- trial_df %>% arrange(item) %>% slice(important_rows)\nreturn(trial_arr_slice)\n}\n\nsim_intensity_multiple <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  all_subs_df <- data.frame()\n  for(s in 1:n_of_sim) {\n    sub_df <- sim_intensity_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq)\n    sub_df$subject <- s\n    all_subs_df <- rbind(all_subs_df, sub_df)\n  }\n  return(all_subs_df)\n}\n\n\nraw_df_no_de <- sim_intensity_multiple(1000)\n\nsubject_sum <- raw_df_no_de %>% group_by(subject, frequency) %>% summarize(mean_int = mean(intensity))\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\nggplot(subject_sum, aes(x = mean_int, group = frequency, color = as.factor(frequency))) + geom_density()\n\n\n\n\nWe see a similar overall shape to that found in Hintzman (1988), with the amplitudes decreasing and the variance increasing as frequency increases. The actual values are higher than those reported in Hintzman (1988), probably because the setup here is different? Hintzman had all the traces pre-stored in memory, while here we are adding them to memory one after the other as they are presented. I’m not sure why the differences would be this large though.\n\n# Overall simulation function (with discrepancy encoding)\n\nsim_intensity_once_de <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  df_items <- data.frame(subject = NA, item = 1:nrow(item_matrix), frequency = frequency_vec, intensity = NA)\n\n# Setting up trial dataframe with randomized trial order\n  trial_df <- data.frame()\n  for(i in 1:nrow(df_items)){\n    current_rows <- bind_rows(replicate(df_items[i, ]$frequency, df_items[i, ], simplify = FALSE))\n    trial_df <- rbind(trial_df, current_rows)\n  }\n  trial_df <- trial_df %>% slice(sample(1:n()))\n\n  # Starting state secondary memory\nmemory <- rbind(t(replicate(n = 4, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3)))), matrix(0, nrow = nrow(trial_df), ncol = item_size))\n\n# Caluclating intensities and storing each probe\n  for(i in 1:nrow(trial_df)){\n    current_output <- get_echo(item_matrix[trial_df[i, ]$item, ], memory, output = \"both\", type = \"cosine\")\n    trial_df$intensity[i] <- current_output[[1]]\n    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))\n    norm_echo <- current_output[[2]]/max(abs(current_output[[2]]))\n    discrep <- item_matrix[trial_df[i, ]$item, ] - norm_echo\n    learned_trace <- discrep * learning_filter\n    memory[4 + i, ] <- learned_trace\n  }\ntrial_arr_slice <- trial_df %>% arrange(item) %>% slice(important_rows)\nreturn(trial_arr_slice)\n}\n\nsim_intensity_multiple_de <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  all_subs_df <- data.frame()\n  for(s in 1:n_of_sim) {\n    sub_df <- sim_intensity_once_de(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq)\n    sub_df$subject <- s\n    all_subs_df <- rbind(all_subs_df, sub_df)\n  }\n  return(all_subs_df)\n}\n\n\n\nraw_dfs_with_de <- sim_intensity_multiple_de(1000)\nsubject_sum_de <- raw_dfs_with_de %>% group_by(subject, frequency) %>%\n  summarize(mean_int = mean(intensity))\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\nggplot(subject_sum_de, aes(x = mean_int, group = frequency, color = as.factor(frequency))) + geom_density()\n\n\n\n\nThis surprisingly looks more like Hintzman’s results, in comparison to the no discrepancy encoding condition. The shape is the same and the values are closer to Hintzman’s.\n\nsim_intensity_once_collins <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_max = 1.0, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  df_items <- data.frame(subject = NA, item = 1:nrow(item_matrix), frequency = frequency_vec)\n  \n  trial_df <- data.frame()\n  for(i in 1:nrow(df_items)){\n    current_rows <- bind_rows(replicate(df_items[i, ]$frequency, df_items[i, ], simplify = FALSE))\n    trial_df <- rbind(trial_df, current_rows)\n  }\n  trial_df <- trial_df %>% slice(sample(1:n()))\n  \n  # Starting state secondary memory\nmemory <- rbind(t(replicate(n = 4, generate_item(item_size = 20, prob = c(1/3, 1/3, 1/3)))), matrix(0, nrow = nrow(trial_df), ncol = item_size))\n\n# Caluclating intensities and storing each probe\n  for(i in 1:nrow(trial_df)){\n    current_intensity <- get_echo(item_matrix[trial_df[i, ]$item, ], memory, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- current_intensity\n    l_value <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))\n    learning_filter <- sample(c(0, 1), item_size, replace = TRUE, prob = c(1-l_value, l_value))\n    learned_trace <- item_matrix[trial_df[i, ]$item, ] * learning_filter\n    memory[4 + i, ] <- learned_trace\n  }\ntrial_arr_slice <- trial_df %>% arrange(item) %>% slice(important_rows)\nreturn(trial_arr_slice)\n}\n\nsim_intensity_multiple_collins <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_max = .5, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq) {\n  all_subs_df <- data.frame()\n  for(s in 1:n_of_sim) {\n    sub_df <- sim_intensity_once_collins(matrix_size = matrix_size, item_size = item_size, prob = prob, l_max = l_max, max_num_of_copies = max_frequency, num_of_traces_per_freq = num_of_item_per_freq)\n    sub_df$subject <- s\n    all_subs_df <- rbind(all_subs_df, sub_df)\n  }\n  return(all_subs_df)\n}\n\n\nraw_df_collins <- sim_intensity_multiple_collins(1000)\n\nsubject_sum_collins <- raw_df_collins %>% group_by(subject, frequency) %>% summarize(mean_int = mean(intensity))\n\n`summarise()` has grouped output by 'subject'. You can override using the\n`.groups` argument.\n\nggplot(subject_sum_collins, aes(x = mean_int, group = frequency, color = as.factor(frequency))) + geom_density()\n\n\n\n\nThis is unexpected, but it does seem to show that the increase in intensity becomes exponentially smaller with increasing frequency. The increase in amplitude is hard to explain though."
  },
  {
    "objectID": "posts/ITS1/index.html",
    "href": "posts/ITS1/index.html",
    "title": "ITS (part 1)",
    "section": "",
    "text": "I couldn’t get the LSAfun package to work (something to do with rgl). Here is my own crude attempt at modeling ITS:\n\nlibrary(RsemanticLibrarian)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(RsemanticLibrarian::cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Function to replace words with their dictionary ids\nreplace_word_with_id <- function(sentence_df, dict_df, max_sent_length){\n  sentence_id_matrix <- c()\n  for(i in 1:length(sentence_df$sentence)){\n    current_sent <- rep(0, max_sent_length)\n    words_list <- tokenize_words(sentence_df$sentence[i])\n    words <- words_list[[1]]\n    for(j in 1:length(words)){\n      current_id <- which(dict_df$word == words[j])\n      if(length(current_id) == 0){\n        current_id <- 0\n      }\n      current_sent[j] <- current_id\n    }\n    sentence_id_matrix <- rbind(sentence_id_matrix, current_sent)\n  }\n  return(sentence_id_matrix)\n}\n\n# Function to generate random environment vectors for each word in dictionary\nmake_env_vectors <- function(dict_df, length, sparsity) {\n  environment_matrix <- matrix(0, nrow = nrow(dict_df), ncol = length)\n  for(i in 1:nrow(dict_df)){\n    current <- rep(0, length)\n    positions <- sample(1:length, sparsity)\n    ones_vector <- c(rep(1, sparsity/2), rep(-1, sparsity/2))\n    ordered_ones_vector <- sample(ones_vector, sparsity, replace = FALSE)\n    for(j in 1:length(positions)){\n      current[positions[j]] <- ordered_ones_vector[j]\n    }\n    environment_matrix[i, ] <- current\n  }\n  return(environment_matrix)\n}\n\n# Function to add up environment vectors for all words in a sentence\nmake_sentence_vectors <- function(compiled_word_ids, env_matrix) {\n  sentence_ids <- matrix(0, nrow = nrow(compiled_word_ids), ncol = ncol(env_matrix))\n  for(i in 1:nrow(compiled_word_ids)){\n    current_word_ids <- compiled_word_ids[i, ]\n    current_sentence <- rep(0, ncol(env_matrix))\n    for(j in 1:length(current_word_ids)){\n      if(current_word_ids[j] == 0){\n        temp_vector <- rep(0, ncol(env_matrix))\n      } else {\n      temp_vector <- env_matrix[current_word_ids[j], ]\n      }\n      current_sentence <- current_sentence + temp_vector\n  }\n  sentence_ids[i, ] <- current_sentence\n  }\n  return(sentence_ids)\n}\n\n# Function to generate semantic meaning vectors for each word\nmake_meaning_vectors <- function(env_matrix, sentence_memory){\n  meaning_matrix <- matrix(0, nrow = nrow(env_matrix), ncol = ncol(env_matrix))\n  for(i in 1:nrow(env_matrix)){\n    meaning_matrix[i, ] <- get_echo(env_matrix[i, ], sentence_memory, output = \"echo\", type = \"cosine\")\n  }\n  return(meaning_matrix)\n}\n\n\n\n\n# Pre-processing\nTASA_full <- read.table(\"tasaDocsPara.txt\", \n                          sep=\"\\t\", \n                          fill=FALSE, \n                          strip.white=TRUE)\ntraining_TASA <- data.frame(sentence = TASA_full$V1[1:2000])\n\n\nTASA_dict <- training_TASA %>% unnest_tokens(output = \"word\", input = sentence, token = \"words\") %>% unique() %>% arrange(word)\n\nTASA_ids <- replace_word_with_id(training_TASA, TASA_dict, 200)\n\nenv_vectors_TASA <- make_env_vectors(TASA_dict, 100, 6)\n\nsent_vectors_TASA <- make_sentence_vectors(TASA_ids, env_vectors_TASA)\n\nmemory_TASA <- sent_vectors_TASA\n\nmeaning_mat_TASA <- make_meaning_vectors(env_vectors_TASA, memory_TASA)\n\nsimilarities_TASA <- cosine_x_to_m(meaning_mat_TASA[which(TASA_dict$word == \"american\"), ], meaning_mat_TASA)\nsim_df_TASA <- data.frame(TASA_dict, similarities = similarities_TASA)\nsim_top_TASA <- sim_df_TASA %>% arrange(desc(similarities)) %>% slice(1:10)\nsim_top_TASA\n\n          word similarities\n1     american    1.0000000\n2       camera    0.6935286\n3  individuals    0.6538814\n4      clouded    0.6025774\n5    corrosive    0.5948728\n6     harpoons    0.5914552\n7       warned    0.5884815\n8        stung    0.5865209\n9    swooshing    0.5856842\n10       races    0.5754529\n\n\nThe meaning vectors do not seem to reflect actual semantic meaning. It could be that my makeshift code is flawed. This could also be because the TASA corpus is arranged in paragraphs, rather than sentences.\n\n\n\n\n# Pre-processing with Stanford sentences\ncorpus <- read_tsv(\"snli_1.0_train.txt\", col_names = TRUE)\n\nRows: 550152 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (14): gold_label, sentence1_binary_parse, sentence2_binary_parse, senten...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncorpus_short <- corpus[ , 6]\ncorpus_sent <- unique(corpus_short)\ncorpus_sent_clean <- tolower(corpus_sent$sentence1)\ncorpus_sent_clean <- removeWords(corpus_sent_clean, stopwords(\"en\"))\ncorpus_sent_clean <- gsub(\"[[:punct:]]\", \" \", corpus_sent_clean)\ncorpus_sent_clean <- gsub(\"[[:digit:]]+\", \" \", corpus_sent_clean)\ncorpus_sent_clean <- gsub(\"\\\\s+\", \" \", corpus_sent_clean)\ncorpus_sent_clean <- data.frame(sentence = (trimws(corpus_sent_clean)))\n\n\npartial_corpus <- corpus_sent_clean %>% slice(1:20000)\n\n# Make dictionary\ndictionary <- partial_corpus %>% unnest_tokens(output = \"word\", input = sentence, token = \"words\") %>% unique() %>% arrange(word)\n\n\ncorpus_ids <- replace_word_with_id(partial_corpus, dictionary, 50)\n\n\nenv_vectors <- make_env_vectors(dictionary, 100, 6)\n\n\nsentence_vectors <- make_sentence_vectors(corpus_ids, env_vectors)\n\n\nmemory <- sentence_vectors\nmeaning_matrix <- make_meaning_vectors(env_vectors, memory)\n\n\nsimilarities <- cosine_x_to_m(meaning_matrix[which(dictionary$word == \"boy\"), ], meaning_matrix)\nsim_df <- data.frame(dictionary, similarities)\nsim_df_top <- sim_df %>% arrange(desc(similarities)) %>% slice(1:10)\nsim_df_top\n\n          word similarities\n1          boy    1.0000000\n2      vibrant    0.8882643\n3     competes    0.8197240\n4      booklet    0.7292181\n5       ascend    0.7246267\n6  vacationing    0.7104282\n7       bricks    0.7013709\n8      bricked    0.6913892\n9    including    0.6876275\n10     highway    0.6442398\n\n\nThe words that are ostensibly semantically similar really aren’t at all. Perhaps the sentences in the corpus are unrelated and don’t form a coherent narrative. Below is an attempt using The Great Gatsby.\n\n\n\n\nlibrary(gutenbergr)\nraw_gatsby <- gutenberg_download(64317)\n\nDetermining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest\n\n\nUsing mirror http://aleph.gutenberg.org\n\ngatsby_text <- raw_gatsby %>% slice(31:6396) %>% select(-gutenberg_id)\ngatsby_pasted <- paste(gatsby_text$text, collapse = \" \")\ngatsby_sentences <- tokenize_sentences(gatsby_pasted, lowercase = TRUE, simplify = TRUE)\ngatsby_clean <- removeWords(gatsby_sentences, c(stopwords(\"en\"), \"ve\", \"t\", \"d\"))\ngatsby_clean <- gsub(\"[[:punct:]]\", \" \", gatsby_clean)\ngatsby_clean <- gsub(\"[[:digit:]]+\", \" \", gatsby_clean)\ngatsby_clean <- gsub(\"\\\\s+\", \" \", gatsby_clean)\ngatsby_clean_df <- data.frame(sentence = (trimws(gatsby_clean)))\n\n\ndictionary_gat <- gatsby_clean_df %>% unnest_tokens(output = \"word\", input = sentence, token = \"words\") %>% unique() %>% arrange(word)\ncomp_word_ids_gat <- replace_word_with_id(gatsby_clean_df, dictionary_gat, 50)\nenv_vectors_gat <- make_env_vectors(dictionary_gat, 100, 6)\nsent_vectors_gat <- make_sentence_vectors(comp_word_ids_gat, env_vectors_gat)\n\nmemory_gat <- sent_vectors_gat\nmeaning_mat_gat <- make_meaning_vectors(env_vectors_gat, memory_gat)\n\n\nsim_gat <- cosine_x_to_m(meaning_mat_gat[which(dictionary_gat$word == \"party\"), ], meaning_mat_gat)\nsim_gat_df <- data.frame(dictionary_gat, similarity = sim_gat)\nsim_gat_top <- sim_gat_df %>% arrange(desc(similarity)) %>% slice(1:10)\nsim_gat_top\n\n            word similarity\n1          party  1.0000000\n2        obliged  0.6571311\n3       tropical  0.6448372\n4       position  0.6197950\n5          chase  0.6192337\n6       blocking  0.6120850\n7       hundreds  0.6042915\n8  ramifications  0.5928379\n9  undergraduate  0.5839898\n10       swollen  0.5796333\n\n\nSlightly better, but most words are still quite unrelated. Perhaps a longer novel with more sentences would work better?"
  },
  {
    "objectID": "posts/ITS2/index.html",
    "href": "posts/ITS2/index.html",
    "title": "ITS (part 2)",
    "section": "",
    "text": "The goal is to replicate the top panel of Figure 8 in Jamieson et al. (2018): the two-dimensional MDS solutions for various words from different categories (shown in Table 2 in the paper).\n\nlibrary(RsemanticLibrarian)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(tidytext)\nlibrary(tokenizers)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(lsa)\n\nLoading required package: SnowballC\n\n\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(RsemanticLibrarian::cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Function to replace words with their dictionary ids\nreplace_word_with_id <- function(sentence_df, dict_df, max_sent_length){\n  sentence_id_matrix <- c()\n  for(i in 1:length(sentence_df$sentence)){\n    current_sent <- rep(0, max_sent_length)\n    words_list <- tokenize_words(sentence_df$sentence[i])\n    words <- words_list[[1]]\n    for(j in 1:length(words)){\n      current_id <- which(dict_df$word == words[j])\n      if(length(current_id) == 0){\n        current_id <- 0\n      }\n      current_sent[j] <- current_id\n    }\n    sentence_id_matrix <- rbind(sentence_id_matrix, current_sent)\n  }\n  return(sentence_id_matrix)\n}\n\n# Function to generate random environment vectors for each word in dictionary\nmake_env_vectors <- function(dict_df, length, sparsity) {\n  environment_matrix <- matrix(0, nrow = nrow(dict_df), ncol = length)\n  for(i in 1:nrow(dict_df)){\n    current <- rep(0, length)\n    positions <- sample(1:length, sparsity)\n    ones_vector <- c(rep(1, sparsity/2), rep(-1, sparsity/2))\n    ordered_ones_vector <- sample(ones_vector, sparsity, replace = FALSE)\n    for(j in 1:length(positions)){\n      current[positions[j]] <- ordered_ones_vector[j]\n    }\n    environment_matrix[i, ] <- current\n  }\n  return(environment_matrix)\n}\n\n# Function to add up environment vectors for all words in a sentence\nmake_sentence_vectors <- function(compiled_word_ids, env_matrix) {\n  sentence_ids <- matrix(0, nrow = nrow(compiled_word_ids), ncol = ncol(env_matrix))\n  for(i in 1:nrow(compiled_word_ids)){\n    current_word_ids <- compiled_word_ids[i, ]\n    current_sentence <- rep(0, ncol(env_matrix))\n    for(j in 1:length(current_word_ids)){\n      if(current_word_ids[j] == 0){\n        temp_vector <- rep(0, ncol(env_matrix))\n      } else {\n      temp_vector <- env_matrix[current_word_ids[j], ]\n      }\n      current_sentence <- current_sentence + temp_vector\n  }\n  sentence_ids[i, ] <- current_sentence\n  }\n  return(sentence_ids)\n}\n\n# Function to generate semantic meaning vectors for each word\nmake_meaning_vectors <- function(env_matrix, sentence_memory){\n  meaning_matrix <- matrix(0, nrow = nrow(env_matrix), ncol = ncol(env_matrix))\n  for(i in 1:nrow(env_matrix)){\n    meaning_matrix[i, ] <- get_echo(env_matrix[i, ], sentence_memory, output = \"echo\", type = \"cosine\")\n  }\n  return(meaning_matrix)\n}\n\n# Find top n similar words in terms of semantic meaning\nfind_similar_words <- function(meaning_matrix, dictionary, word_to_find, n) {\n  similarities <- cosine_x_to_m(meaning_matrix[which(dictionary$word == word_to_find), ], meaning_matrix)\nsimilarities_df <- data.frame(dictionary, similarities = similarities)\nsim_top <- similarities_df %>% arrange(desc(similarities)) %>% slice(1:n)\nreturn(sim_top)\n}\n\nUnfortunately, my computer doesn’t have sufficient processing power to run 10,000 TASA sentences with vectors 10,000 elements long (sparsity = 8). I am going to try using only the first 1000 sentences.\n\nTASA_full <- read.table(\"tasaDocsPara.txt\", \n                          sep = \"\\t\", \n                          fill = FALSE, \n                          strip.white = TRUE)\n\nJonesMewhort_Figure3 <- c(\"financial\",\"savings\",\"finance\",\"pay\",\"invested\",\n                          \"loaned\",\"borrow\",\"lend\",\"invest\",\"investments\",\n                          \"bank\",\"spend\",\"save\",\"astronomy\",\"physics\",\n                          \"chemistry\",\"psychology\",\"biology\",\"scientific\",\n                          \"mathematics\",\"technology\",\"scientists\",\"science\",\n                          \"research\",\"sports\",\"team\",\"teams\",\"football\",\n                          \"coach\",\"sport\",\"players\",\"baseball\",\"soccer\",\n                          \"tennis\",\"basketball\")\n\n\n\n\ntraining_TASA <- data.frame(sentence = TASA_full$V1[1:1000])\n\nTASA_dict <- training_TASA %>% unnest_tokens(output = \"word\", input = sentence, token = \"words\") %>% unique()\n\nTASA_ids <- replace_word_with_id(training_TASA, TASA_dict, 300)\n\nenv_vectors_TASA <- make_env_vectors(TASA_dict, 10000, 8)\n\nsent_vectors_TASA <- make_sentence_vectors(TASA_ids, env_vectors_TASA)\n\nmemory_TASA <- sent_vectors_TASA\n\ndict_env_df <- data.frame(TASA_dict, env_vectors_TASA) \n\nenv_its <- dict_env_df[which(TASA_dict$word %in% JonesMewhort_Figure3), ]\n\nenv_its_vecs <- env_its %>% select(-word) %>% as.matrix()\n\nits_meaning_vecs <- make_meaning_vectors(env_its_vecs, memory_TASA)\nrownames(its_meaning_vecs) <- env_its$word\n\ncosine_table_bram <- lsa::cosine(t(its_meaning_vecs))\n\ncorrplot::corrplot(cosine_table_bram,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5)\n\n\n\n\nWords from the same categories aren’t always related to each other. Something’s wrong, possibly because of the smaller corpus size?\n\nits_mds <- its_meaning_vecs %>% dist() %>% cmdscale() %>% as_tibble(.name_repair = \"minimal\")\ncolnames(its_mds) <- c(\"Dim1\", \"Dim2\")\n\nggplot(its_mds, aes(x = Dim1, y = Dim2, label = rownames(its_meaning_vecs))) + geom_point() + geom_text()\n\n\n\n\nThis doesn’t make sense.\n\n\n\n\nmatt_training <- TASA_full$V1[1:10000]\n\ncorpus_dictionary <- function(words){\n  neat_words <- words %>%\n                as.character() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                qdapRegex::rm_white_lead_trail() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                unique()\n  return(neat_words)\n}\n\ndictionary <- corpus_dictionary(matt_training)\n\nclean <- function(words){\n  if(length(words) == 1) {\n    neat_words <- words %>%\n      as.character() %>%\n      strsplit(split=\" \") %>%\n      unlist() %>%\n      qdapRegex::rm_white_lead_trail() %>%\n      strsplit(split=\" \") %>%\n      unlist() \n    return(neat_words)\n  }\n}\n\nclean_vector <- function(words){\n  return(lapply(unlist(strsplit(words,split=\"[.]\")),clean))\n}\n\nsentences_list <- clean_vector(matt_training)\n\nword_ids <- sl_word_ids_sentences(sentences_list, dictionary)\n\nenvironments <- sl_create_riv(10000, length(dictionary), 8)\n\nits_memory <- matrix(0,ncol=dim(environments)[2],nrow=length(word_ids))\nfor(i in 1:length(sentences_list)){\n  its_memory[i,] <- colSums(environments[word_ids[[i]],])\n}\n\n# function to generate echoes for a list of words\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  \n  return(word_meaning)\n  \n}\n\nJonesMewhort_vectors <- subset_semantic_vectors(JonesMewhort_Figure3,\n                                                dictionary,\n                                                environments,\n                                                its_memory,\n                                                tau=3)\n\ncosine_table <- lsa::cosine(t(JonesMewhort_vectors))\n\ncorrplot::corrplot(cosine_table,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5)\n\n\n\n\nThe correlation matrix makes sense. Words from the different categories are more or less correlated with others in the same category.\n\nmatt_mds <- JonesMewhort_vectors %>% dist() %>% cmdscale() %>% as_tibble(.name_repair = \"minimal\")\ncolnames(matt_mds) <- c(\"Dim1\", \"Dim2\")\n\nggplot(matt_mds, aes(x = Dim1, y = Dim2, label = rownames(its_meaning_vecs))) + geom_point() + geom_text()\n\n\n\n\nThis doesn’t make sense! Could it be because I used a different MDS function?\n\n\n\n\n# Redoing Matt's process with just the first 1000 sentences\n\nmatt_training_test <- TASA_full$V1[1:1000]\n\ndictionary_test <- corpus_dictionary(matt_training_test)\n\nsentences_list_test <- clean_vector(matt_training_test)\n\nword_ids_test <- sl_word_ids_sentences(sentences_list_test, dictionary_test)\n\nenvironments_test <- sl_create_riv(10000, length(dictionary_test), 8)\n\nits_memory_test <- matrix(0,ncol=dim(environments_test)[2],nrow=length(word_ids_test))\nfor(i in 1:length(sentences_list_test)){\n  its_memory_test[i,] <- colSums(environments_test[word_ids_test[[i]],])\n}\n\nJones_vector_test <- subset_semantic_vectors(JonesMewhort_Figure3, dictionary_test, environments_test, its_memory_test, tau = 3) \n\n# Comparing dictionaries\n\nall(TASA_dict$word == dictionary_test)\n\n[1] TRUE\n\n# Comparing sentences replaced with IDs (choosing a random sentence to compare i.e. sentence 19)\n\nall(TASA_ids[19, 1:length(word_ids_test[[19]])] == word_ids_test[[19]])\n\n[1] TRUE\n\n# Comparing sentence memory (using the same environment vectors i.e. Matt's)\n\ntest_using_matt_env <- make_sentence_vectors(TASA_ids, environments_test)\n\nall(test_using_matt_env == its_memory_test)\n\n[1] TRUE\n\n# Comparing semantic meaning vectors\n\ntest_env_its <- environments_test[which(TASA_dict$word %in% JonesMewhort_Figure3), ]\n\ntest_meaning_vecs <- make_meaning_vectors(test_env_its, test_using_matt_env)\n\nall(test_meaning_vecs == JonesMewhort_vectors)\n\n[1] FALSE\n\n\nOur functions that create semantic meaning vectors differ. Looking at the code for each function, however, I cannot see why the two would diverge.\n\n\n\nAs laid out in the paper, his environment vectors don’t contain integers (1s, 0s and -1s) but a randomly sampled value with mean 0 and variance 1/(vector length), although the variance is actually 1/sqrt(vector length) in the code. Not sure how this compares to using only the three integer values and taking into account sparsity.\n\nsemantic_vecs <- function (probe_words, env_vectors, memory, tau = 3) {\n  semantic_vectors <- matrix(0, length(probe_words), ncol(memory))\n  for (i in 1:length(probe_words)) {\n    probe <- unlist(strsplit(probe_words[i], split=\"/\"))\n    if (all(probe %in% rownames(env_vectors)) == TRUE) {\n      for (j in 1:nrow(memory)) {\n        A <- 1.0\n        for (k in 1:length(probe)) {\n          A <- A * cosine_randy(env_vectors[probe[k],], memory[j,])**tau\n        }\n        semantic_vectors[i,] <- semantic_vectors[i,] + A * memory[j,]\n      }\n    }\n  }\n  rownames(semantic_vectors) <- probe_words\n  semantic_vectors <- semantic_vectors[abs(rowSums(semantic_vectors)) != 0,]\n  return(semantic_vectors)\n}\n\nI don’t understand this function for making semantic vectors. Why is A multiplied by the memory again afterwards and why is A cumulative over different probes?"
  },
  {
    "objectID": "posts/ITS_DE1/index.html",
    "href": "posts/ITS_DE1/index.html",
    "title": "Exploring discrepancy encoding with ITS (Part 1)",
    "section": "",
    "text": "Goals for this exercise:\n\nReplicate toy language involving the homonym “break” from Jamieson et al. (2018)\nImplement discrepancy encoding in Jamieson et al.’s (2018) toy language\n\n\n\n\nlibrary(RsemanticLibrarian)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lsa)\n\nLoading required package: SnowballC\n\n\n\n\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Create dictionary (modified from RsemanticLibrarian)\n\ncorpus_dictionary <- function(words){\n  neat_words <- words %>%\n                as.character() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                qdapRegex::rm_white_lead_trail() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                unique()\n  return(neat_words)\n}\n\n# Functions to ensure strings only contain words (modified from RsemanticLibrarian)\nclean <- function(words){\n  if(length(words) == 1) {\n    neat_words <- words %>%\n      as.character() %>%\n      strsplit(split=\" \") %>%\n      unlist() %>%\n      qdapRegex::rm_white_lead_trail() %>%\n      strsplit(split=\" \") %>%\n      unlist() \n    return(neat_words)\n  }\n}\n\nclean_vector <- function(words){\n  return(lapply(unlist(strsplit(words,split=\"[.]\")), clean))\n}\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    echo <- get_echo(probe, sentence_memory, tau = tau, output = \"echo\", type = \"cosine\")\n    word_meaning[i, ] <- echo\n  }\n  row.names(word_meaning) <- strings\n  \n  return(word_meaning)\n}\n\n# Find top n similar words in terms of semantic meaning\nfind_similar_words <- function(meaning_matrix, dictionary, word_to_find, n) {\n  similarities <- cosine_x_to_m(meaning_matrix[which(dictionary$word == word_to_find), ], meaning_matrix)\nsimilarities_df <- data.frame(dictionary, similarities = similarities)\nsim_top <- similarities_df %>% arrange(desc(similarities)) %>% slice(1:n)\nreturn(sim_top)\n}\n\n\n\n\n\nl_value <- 0.7\n\nnoun_human <- c(\"man\", \"woman\")\nverb_vehicle <- c(\"stop\", \"break\")\nverb_dinner <- c(\"smash\", \"break\")\nverb_news <- c(\"report\", \"break\")\nnoun_vehicle <- c(\"car\", \"truck\")\nnoun_dinner <- c(\"plate\", \"glass\")\nnoun_news <- c(\"story\", \"news\")\n\nsentences <- data.frame(sentence_no = 1:1000, sentence = NA)\nsentence_frames <- c(\"vehicle\", \"dinner\", \"news\")\nfor(i in 1:nrow(sentences)) {\n  current_frame <- sample(sentence_frames, 1)\n  if(current_frame == \"vehicle\"){\n  sentences$sentence[i] <- paste(sample(noun_human, 1), sample(verb_vehicle, 1), sample(noun_vehicle, 1))\n  } else if(current_frame == \"dinner\"){\n  sentences$sentence[i] <- paste(sample(noun_human, 1), sample(verb_dinner, 1), sample(noun_dinner, 1))\n  } else {\n  sentences$sentence[i] <- paste(sample(noun_human, 1), sample(verb_news, 1), sample(noun_news, 1))\n  }\n}\n\ndictionary <- unique(c(noun_human, verb_vehicle, verb_dinner, verb_news, noun_vehicle, noun_dinner, noun_news))\nclean_sentences <- clean_vector(sentences$sentence)\nsentence_ids <- sl_word_ids_sentences(clean_sentences, dictionary)\nenv_vectors <- sl_create_riv(20000, length(dictionary), 16)\n\n# Setting up prior memory (cannot start with other three-word sentences, as they are too sparse. Have to start with less sparse memory to avoid NaN values). Then encoding sentences without discrepancy encoding (also applying L value)\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(env_vectors), replace = TRUE)))\ntoy_memory <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(sentence_ids)))\nfor(i in 1:length(sentence_ids)){\n  current_sentence <- colSums(env_vectors[sentence_ids[[i]], ])\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))\n  toy_memory[4+i, ] <- current_sentence*learning_vector\n}\n\n# Get echoes (semantic meaning vectors)\nmeaning <- subset_semantic_vectors(dictionary, dictionary, env_vectors, toy_memory, tau = 3)\n\ncosine_toy <- cosine(t(meaning))\n\ncorrplot::corrplot(cosine_toy,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n\n\n\n\n\nmds_toy <- cmdscale(1-cosine_toy) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_toy) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_toy, aes(x = Dim1, y = Dim2, label = dictionary)) + geom_text() + geom_point()\n\n\n\n\nEven with just 1000 sentences (much fewer than the 20,000 in Jamieson et al.), I was able to reproduce the MDS solution.\nI have a question about what the distances mean. They don’t seem to measure “direct” co-occurence, since man/woman, car/truck, glass/plate and news/story never appear in the same sentences together. Do they measure a higher-order similarity, since for e.g. stop only occurs with car/truck?\n\n\n\n\n\ntau_de <- 3\n\n# Setting up prior memory and discrepancy encoding (with L value)\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(env_vectors), replace = TRUE)))\ntoy_memory_de <- rbind(prior_memory, matrix(0, nrow = length(sentence_ids), ncol = ncol(env_vectors)))\n\nfor(i in 1:length(sentence_ids)){\n  current_sentence <- colSums(env_vectors[sentence_ids[[i]], ])\n  echo <- get_echo(current_sentence, toy_memory_de, tau = tau_de, output = \"echo\", type = \"cosine\")\n  echo_norm <- echo/max(abs(echo))\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))\n  discrep <- current_sentence - echo_norm\n  toy_memory_de[4+i, ] <- discrep * learning_vector\n  }\n\n\n# Get echoes (semantic meaning vectors)\nmeaning_de <- subset_semantic_vectors(dictionary, dictionary, env_vectors, toy_memory_de, tau = 3)\n\ncosine_toy_de <- cosine(t(meaning_de))\n\ncorrplot::corrplot(cosine_toy_de,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n\n\n\n\nClustering is less obvious.\n\nmds_toy_de <- cmdscale(1-cosine_toy_de) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_toy_de) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_toy_de, aes(x = Dim1, y = Dim2, label = dictionary)) + geom_text() + geom_point()\n\n\n\n\nThe distinction between some of the clusters is lost!\n\n\n\n\nl_max <- 1.0\n\n# Setting up prior memory and discrepancy encoding (with L value)\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(env_vectors), replace = TRUE)))\ntoy_memory_collins <- rbind(prior_memory, matrix(0, nrow = length(sentence_ids), ncol = ncol(env_vectors)))\n\nfor(i in 1:length(sentence_ids)){\n  current_sentence <- colSums(env_vectors[sentence_ids[[i]], ])\n  current_intensity <- get_echo(current_sentence, toy_memory_collins, tau = tau_de, output = \"intensity\", type = \"cosine\")\n    l_collins <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_collins, l_collins))\n  toy_memory_collins[4+i, ] <- current_sentence * learning_vector\n  }\n\n\n# Get echoes (semantic meaning vectors)\nmeaning_collins <- subset_semantic_vectors(dictionary, dictionary, env_vectors, toy_memory_collins, tau = 3)\n\ncosine_toy_collins <- cosine(t(meaning_collins))\n\ncorrplot::corrplot(cosine_toy_collins,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n\n\n\n\nThere seems to be clustering!\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nmds_toy_collins <- cmdscale(1-cosine_toy_collins) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_toy_collins) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_toy_collins, aes(x = Dim1, y = Dim2, label = dictionary)) + geom_text() + geom_point()\n\n\n\n\nThe words are clustered differently but are still in discernible clusters. Stop, report and smash seem to be roughly equidistant from break, as in the original ITS diagram. However, as opposed to the ITS diagram, man and woman are far from each other. Perhaps a higher-order dimensionality (e.g. 3D) plot would make this more sensible?"
  },
  {
    "objectID": "posts/ITS_DE2/index.html",
    "href": "posts/ITS_DE2/index.html",
    "title": "Exploring discrepancy encoding with ITS (Part 1)",
    "section": "",
    "text": "Goals for this exercise:\n\nReplicate MDS solution for TASA corpus as in Jamieson et al. (2018)\nIdentify words with varying frequencies and plot intensity graphs as per Hintzman (1988)\nRepeat while applying discrepancy encoding\n\n\n\n\nlibrary(RsemanticLibrarian)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lsa)\n\nLoading required package: SnowballC\n\nTASA_full <- read.table(\"tasaDocsPara.txt\", \n                          sep=\"\\t\", \n                          fill=FALSE, \n                          strip.white=TRUE)\n\ntraining <- TASA_full$V1[1:2000]\n\nl_value <- 0.7\n\n\n\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations <- function(probe, mem, type) {\n  if(type == \"hintzman\"){\n    return(as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))))\n  }\n  if(type == \"cosine\"){\n    temp_activ <- as.numeric(cosine_x_to_m(probe,mem))\n    temp_activ[is.nan(temp_activ) == TRUE] <- 0\n    return(temp_activ)\n  }\n}\n\n# Generate echo (borrowed from Matt)\nget_echo <- function(probe, mem, tau=3, output='intensity', type) {\n    activations <- get_activations(probe, mem, type)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\n# Create dictionary (modified from RsemanticLibrarian)\n\ncorpus_dictionary <- function(words){\n  neat_words <- words %>%\n                as.character() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                qdapRegex::rm_white_lead_trail() %>%\n                strsplit(split=\" \") %>%\n                unlist() %>%\n                unique()\n  return(neat_words)\n}\n\n# Functions to ensure strings only contain words (modified from RsemanticLibrarian)\nclean <- function(words){\n  if(length(words) == 1) {\n    neat_words <- words %>%\n      as.character() %>%\n      strsplit(split=\" \") %>%\n      unlist() %>%\n      qdapRegex::rm_white_lead_trail() %>%\n      strsplit(split=\" \") %>%\n      unlist() \n    return(neat_words)\n  }\n}\n\nclean_vector <- function(words){\n  return(lapply(unlist(strsplit(words,split=\"[.]\")), clean))\n}\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    echo <- get_echo(probe, sentence_memory, tau = tau, output = \"echo\", type = \"cosine\")\n    word_meaning[i, ] <- echo\n  }\n  row.names(word_meaning) <- strings\n  \n  return(word_meaning)\n}\n\n# Find top n similar words in terms of semantic meaning\nfind_similar_words <- function(meaning_matrix, dictionary, word_to_find, n) {\n  similarities <- cosine_x_to_m(meaning_matrix[which(dictionary$word == word_to_find), ], meaning_matrix)\nsimilarities_df <- data.frame(dictionary, similarities = similarities)\nsim_top <- similarities_df %>% arrange(desc(similarities)) %>% slice(1:n)\nreturn(sim_top)\n}\n\n\n\n\n\ndictionary <- corpus_dictionary(training)\n\nclean_sentences <- clean_vector(training)\n\nword_ids <- sl_word_ids_sentences(clean_sentences, dictionary)\n\nenv_vectors <- sl_create_riv(5000, length(dictionary), 4)\n\n# Setting up prior memory without discrepancy encoding (also applying L value)\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(env_vectors), replace = TRUE)))\nmemory_no_de <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(word_ids)))\n\nfor(i in 1:length(word_ids)){\n  current_sentence <- colSums(env_vectors[word_ids[[i]], ])\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))\n  memory_no_de[4+i, ] <- current_sentence*learning_vector\n}\n\n# Identifying words with different frequencies\n\nJonesMewhort_Figure3 <- c(\"financial\",\"savings\",\"finance\",\"pay\",\"invested\",\n                          \"loaned\",\"borrow\",\"lend\",\"invest\",\"investments\",\n                          \"bank\",\"spend\",\"save\",\"astronomy\",\"physics\",\n                          \"chemistry\",\"psychology\",\"biology\",\"scientific\",\n                          \"mathematics\",\"technology\",\"scientists\",\"science\",\n                          \"research\",\"sports\",\"team\",\"teams\",\"football\",\n                          \"coach\",\"sport\",\"players\",\"baseball\",\"soccer\",\n                          \"tennis\",\"basketball\")\n\nword_freq_table <- clean_sentences %>% unlist() %>% table() %>% as_tibble() %>% filter(. %in% JonesMewhort_Figure3) %>% arrange(n)\n\nword_subset <- c(\"astronomy\", \"loaned\", \"scientific\", \"save\", \"science\", \"bank\", \"scientists\", \"pay\")\n\n# Get echoes (semantic meaning vectors)\nmeaning_no_de <- subset_semantic_vectors(word_subset, dictionary, env_vectors, memory_no_de, tau = 3)\n\ncosine_no_de <- cosine(t(meaning_no_de))\n\ncorrplot::corrplot(cosine_no_de,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n\n\n\n\n\nmds_no_de <- cmdscale(1-cosine_no_de) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_no_de) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_no_de, aes(x = Dim1, y = Dim2, label = word_subset)) + geom_text() + geom_point()\n\n\n\n\nSome evidence of clustering with 2000 sentences, though it’s clear the low frequency words are not as close to the other words in their category.\n\n\n\nDue to RAM limitations, I had to reduce the vector length for the word environment vectors to 5000 with sparsity = 4. Also, I am only simulating with 100 subjects.\n\n# No discrepancy encoding: Function to calculate intensity for one participant (modified from Matt's)\n\nsim_intensity_once_no_de <- function(strings, dict, vector_length, sparsity, sentence_with_ids, tau = 3) {\n  env_vecs <- sl_create_riv(vector_length, length(dict), sparsity)\n  \n  sentence_mem <- matrix(0, ncol = ncol(env_vecs), nrow = length(sentence_with_ids)+4)\n  sentence_mem[1:4, ] <- t(replicate(4, sample(c(-1,0,1), ncol(env_vecs), replace = TRUE)))\n  \n  reordered_sentences <- sample(sentence_with_ids)\n  \nfor(i in 1:length(reordered_sentences)){\n  current_sentence <- colSums(env_vecs[reordered_sentences[[i]], ])\n  learning_vector <- sample(c(0,1), ncol(env_vecs), replace = TRUE, prob = c(1-l_value, l_value))\n  sentence_mem[4+i, ] <- current_sentence*learning_vector\n}\n  \n  reordered_strings <- sample(strings)\n  trial_df <- data.frame(subject = NA, word = reordered_strings, intensity = NA)\n  \n  for (i in 1:length(reordered_strings)) {\n    word_id <- which(dict %in% reordered_strings[i])\n    probe <- env_vecs[word_id, ]\n    intensity <- get_echo(probe, sentence_mem, tau = tau, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- intensity\n  }\n  return(trial_df)\n}\n\n\nn_of_sim <- 100 \nfull_subjects_df <- data.frame()\n\nfor(i in 1:n_of_sim){\n  current_sub <- sim_intensity_once_no_de(word_subset, dictionary, 5000, 4, word_ids, tau = 3)\n  current_sub$subject <- i\n  full_subjects_df <- bind_rows(full_subjects_df, current_sub)\n}\n\n\nscience_df <- full_subjects_df %>% filter(word %in% c(\"astronomy\", \"scientific\", \"science\", \"scientists\"))\nfinance_df <- full_subjects_df %>% filter(word %in% c(\"loaned\", \"save\", \"bank\", \"pay\"))\n\nggplot(science_df, aes(x = intensity, color = word, group = word)) + geom_density()\n\n\n\nggplot(finance_df, aes(x = intensity, color = word, group = word)) + geom_density()\n\n\n\n\nRunning 100 simulations took slightly more than an hour.\nThe science cluster graph makes sense (word frequency: astronomy < scientific < science < scientists), although the lack of a difference between tje science and scientists curves is interesting.\nThe finance cluster graph is somewhat unexpected (word frequency: loaned < save, bank, pay), given that the bank curve is flatter and has a higher intensity than the pay curve.\n\n\n\n\nGiven the long processing time with the no DE condition, only 50 subjects will be simulated for the DE conditions, as the discrepancy encoding steps add roughly 4 mins per subject to the total duration.\n\n# Setting up prior memory with discrepancy encoding (also applying L value)\nmemory_AL_de <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(word_ids)))\n\nfor(i in 1:length(word_ids)){\n  current_sentence <- colSums(env_vectors[word_ids[[i]], ])\n  echo <- get_echo(current_sentence, memory_AL_de, tau = 3, output = \"echo\", type = \"cosine\")\n  echo_norm <- echo/max(abs(echo))\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_value, l_value))\n  memory_AL_de[4+i, ] <- (current_sentence - echo_norm) * learning_vector\n}\n\n# Get echoes (semantic meaning vectors)\nmeaning_AL_de <- subset_semantic_vectors(word_subset, dictionary, env_vectors, memory_AL_de, tau = 3)\n\ncosine_AL_de <- cosine(t(meaning_AL_de))\n\ncorrplot::corrplot(cosine_AL_de,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n\n\n\n\nThe science cluster is somewhat visible, but the finance cluster is seemingly non-existent.\n\nmds_AL_de <- cmdscale(1-cosine_AL_de) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_AL_de) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_AL_de, aes(x = Dim1, y = Dim2, label = word_subset)) + geom_text() + geom_point()\n\n\n\n\nAt least the clusters are still somewhat distinguishable, although this could also be because there are only 2 clusters.\n\n# Use MINERVA-AL discrepancy encoding: Function to calculate intensity for one participant (modified from Matt's)\n\nsim_intensity_once_AL_de <- function(strings, dict, vector_length, sparsity, sentence_with_ids, tau = 3) {\n  env_vecs <- sl_create_riv(vector_length, length(dict), sparsity)\n  \n  sentence_mem <- matrix(0, ncol = ncol(env_vecs), nrow = length(sentence_with_ids)+4)\n  sentence_mem[1:4, ] <- t(replicate(4, sample(c(-1,0,1), ncol(env_vecs), replace = TRUE)))\n  \n  reordered_sentences <- sample(sentence_with_ids)\n  \nfor(i in 1:length(reordered_sentences)){\n  current_sentence <- colSums(env_vecs[reordered_sentences[[i]], ])\n  echo <- get_echo(current_sentence, sentence_mem, tau = 3, output = \"echo\", type = \"cosine\")\n  echo_norm <- echo/max(abs(echo))\n  learning_vector <- sample(c(0,1), ncol(env_vecs), replace = TRUE, prob = c(1-l_value, l_value))\n  sentence_mem[4+i, ] <- (current_sentence - echo_norm) *learning_vector\n}\n  \n  reordered_strings <- sample(strings)\n  trial_df <- data.frame(subject = NA, word = reordered_strings, intensity = NA)\n  \n  for (i in 1:length(reordered_strings)) {\n    word_id <- which(dict %in% reordered_strings[i])\n    probe <- env_vecs[word_id, ]\n    intensity <- get_echo(probe, sentence_mem, tau = tau, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- intensity\n  }\n  return(trial_df)\n}\n\nfull_subjects_df_AL <- data.frame()\n\nfor(i in 1:50){\n  current_sub <- sim_intensity_once_AL_de(word_subset, dictionary, 5000, 4, word_ids, tau = 3)\n  current_sub$subject <- i\n  full_subjects_df_AL <- bind_rows(full_subjects_df_AL, current_sub)\n}\n\n\nscience_df_AL <- full_subjects_df_AL %>% filter(word %in% c(\"astronomy\", \"scientific\", \"science\", \"scientists\"))\nfinance_df_AL <- full_subjects_df_AL %>% filter(word %in% c(\"loaned\", \"save\", \"bank\", \"pay\"))\n\nggplot(science_df_AL, aes(x = intensity, color = word, group = word)) + geom_density()\n\n\n\nggplot(finance_df_AL, aes(x = intensity, color = word, group = word)) + geom_density()\n\n\n\n\nThis took 4 hours! The graphs seem similar to those in the no DE condition. The anomaly of the bank curve having a higher mean and variance than the pay curve is seen here as well.\n\n\n\nSince this is faster than the MINERVA-AL condition, 100 subjects were simulated.\n\nl_max <- 1.0\n\n# Setting up prior memory with discrepancy encoding (also applying L value)\nmemory_collins_de <- rbind(prior_memory, matrix(0, ncol = ncol(env_vectors), nrow = length(word_ids)))\n\nfor(i in 1:length(word_ids)){\n  current_sentence <- colSums(env_vectors[word_ids[[i]], ])\n  current_intensity <- get_echo(current_sentence, memory_collins_de, tau = 3, output = \"intensity\", type = \"cosine\")\n    l_collins <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))\n  learning_vector <- sample(c(0,1), ncol(env_vectors), replace = TRUE, prob = c(1-l_collins, l_collins))\n  memory_collins_de[4+i, ] <- current_sentence * learning_vector\n}\n\n# Get echoes (semantic meaning vectors)\nmeaning_collins_de <- subset_semantic_vectors(word_subset, dictionary, env_vectors, memory_collins_de, tau = 3)\n\ncosine_collins_de <- cosine(t(meaning_collins_de))\n\ncorrplot::corrplot(cosine_collins_de,\n                   method=\"circle\",\n                   order=\"hclust\",\n                   tl.col=\"black\",\n                   tl.cex=.7,\n                   title=\"\",\n                   cl.cex=.6)\n\n\n\n\nThe science words cluster well and the finance words cluster to a lesser degree.\n\nmds_collins_de <- cmdscale(1-cosine_collins_de) %>% as_tibble(.name_repair = \"minimal\")\ncolnames(mds_collins_de) <- c(\"Dim1\", \"Dim2\")\n\nggplot(mds_collins_de, aes(x = Dim1, y = Dim2, label = word_subset)) + geom_text() + geom_point()\n\n\n\n\nDiscernible clusters, though this could once again be because there are only 2 clusters to differentiate.\n\n# Use Collins et al. (2020) discrepancy encoding: Function to calculate intensity for one participant (modified from Matt's)\n\nsim_intensity_once_collins <- function(strings, dict, vector_length, sparsity, sentence_with_ids, tau = 3) {\n  env_vecs <- sl_create_riv(vector_length, length(dict), sparsity)\n  \n  sentence_mem <- matrix(0, ncol = ncol(env_vecs), nrow = length(sentence_with_ids)+4)\n  sentence_mem[1:4, ] <- t(replicate(4, sample(c(-1,0,1), ncol(env_vecs), replace = TRUE)))\n  \n  reordered_sentences <- sample(sentence_with_ids)\n  \nfor(i in 1:length(reordered_sentences)){\n  current_sentence <- colSums(env_vecs[reordered_sentences[[i]], ])\n  current_intensity <- get_echo(current_sentence, sentence_mem, tau = 3, output = \"intensity\", type = \"cosine\")\n  l_collins <- l_max * (1 - 1/(1 + exp(1)^(-12 * current_intensity + 2)))\n  learning_vector <- sample(c(0,1), ncol(env_vecs), replace = TRUE, prob = c(1-l_collins, l_collins))\n  sentence_mem[4+i, ] <- current_sentence *learning_vector\n}\n  \n  reordered_strings <- sample(strings)\n  trial_df <- data.frame(subject = NA, word = reordered_strings, intensity = NA)\n  \n  for (i in 1:length(reordered_strings)) {\n    word_id <- which(dict %in% reordered_strings[i])\n    probe <- env_vecs[word_id, ]\n    intensity <- get_echo(probe, sentence_mem, tau = tau, output = \"intensity\", type = \"cosine\")\n    trial_df$intensity[i] <- intensity\n  }\n  return(trial_df)\n}\n\nfull_subjects_df_collins <- data.frame()\n\nfor(i in 1:100){\n  current_sub <- sim_intensity_once_collins(word_subset, dictionary, 5000, 4, word_ids, tau = 3)\n  current_sub$subject <- i\n  full_subjects_df_collins <- bind_rows(full_subjects_df_collins, current_sub)\n}\n\nThat took slightly more than 2.5 hours.\n\nscience_df_collins <- full_subjects_df_collins %>% filter(word %in% c(\"astronomy\", \"scientific\", \"science\", \"scientists\"))\nfinance_df_collins <- full_subjects_df_collins %>% filter(word %in% c(\"loaned\", \"save\", \"bank\", \"pay\"))\n\nggplot(science_df_collins, aes(x = intensity, color = word, group = word)) + geom_density()\n\n\n\nggplot(finance_df_collins, aes(x = intensity, color = word, group = word)) + geom_density()\n\n\n\n\nSame patterns as in the other two conditions. Does discrepancy encoding not make a difference for frequency judgments?"
  },
  {
    "objectID": "posts/ITS_DE3/index.html",
    "href": "posts/ITS_DE3/index.html",
    "title": "Exploring discrepancy encoding with ITS (Part 3)",
    "section": "",
    "text": "The goal of this exercise is to use the toy language setup in Sandin et al. (2017) (without actually using the intricacies of random indexing) to:\n\nFind the number of sentences required for learning at each L to obtain echoes similar to that of L = 1.0\n\nmost appropriate parameters (tau, L) without discrepancy encoding\n\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(RsemanticLibrarian)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nn_words_matt <- 1000\nband_width_matt <- 50\n\n#rows are words, columns are sentences\nband_matrix_matt <- Matrix::bandSparse(n = n_words_matt,k=0:band_width_matt)\nband_matrix_matt <- as.matrix(band_matrix_matt)*1\n\n# the following use functions from Rsemanticlibrarian\n\n# get all unique words from training text\ndictionary_words_matt <- 1:n_words_matt\n\n# create a list of sentences by index in dictionary\nsentence_ids_matt <- list()\nfor(i in 1:ncol(band_matrix_matt)){\n  sentence_ids_matt[[i]] <- which(band_matrix_matt[,i] == 1)\n}\n\n#create random environment vectors for each word in dictionary\nenvironment_matt <- sl_create_riv(10000,length(dictionary_words_matt),8)\n\n# make ITS sentence memory\n# for each sentence, sum the environment vectors for the words in the sentence\nits_memory_matt <- matrix(0,ncol=dim(environment_matt)[2],nrow=length(sentence_ids_matt))\nfor(i in 1:length(sentence_ids_matt)){\n  if(is.null(nrow(environment_matt[sentence_ids_matt[[i]],]))){\n    its_memory_matt[i,] <- environment_matt[sentence_ids_matt[[i]],]\n  } else {\n    its_memory_matt[i,] <- colSums(environment_matt[sentence_ids_matt[[i]],])\n  }\n  \n}\n\nwords_to_plot_matt <- c(1:100)\n\n# function to generate echoes for a list of words\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  \n  return(word_meaning)\n  \n}\n\nword_semantic_vectors_matt <- subset_semantic_vectors(words_to_plot_matt,                                                dictionary_words_matt,\n    environment_matt,\n      its_memory_matt,\n          tau=3)\n\ncosine_matt <- lsa::cosine(t(word_semantic_vectors_matt))\n\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines_matt <- lsa::cosine(t(band_matrix_matt[words_to_plot_matt,]))\nsecond_order_cosines_matt <- lsa::cosine(t(first_order_cosines_matt))\n\nmatt_R2_first <- cor(c(cosine_matt),c(first_order_cosines_matt))^2\nmatt_R2_second <- cor(c(cosine_matt),c(second_order_cosines_matt))^2\n\n\n\n\n\nn_words <- 130\nband_width <- 10\n\n# Setup of word-sentence co-occurence matrix\nband_matrix <- Matrix::bandSparse(n = n_words,k=0:(band_width-1))\nband_matrix <- as.matrix(band_matrix)*1\n\n# ITS setup (each word appears in the same number of sentences)\ndictionary_words <- 1:n_words\n\nsentence_ids <- list()\nfor(i in 1:ncol(band_matrix)){\n  sentence_ids[[i]] <- which(band_matrix[,i] == 1)\n}\nsentence_ids <- sentence_ids[11:119]\nenvironment <- sl_create_riv(10000,length(dictionary_words),8)\n\nwords_to_plot <- c(11:110)\n\n\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines <- lsa::cosine(t(band_matrix[words_to_plot,]))\nsecond_order_cosines <- lsa::cosine(t(first_order_cosines))\n\n\nprior_memory <- t(replicate(4, sample(c(-1,0,1), ncol(environment), replace = TRUE)))\n\nits_memory_1 <- rbind(prior_memory, matrix(0,ncol=dim(environment)[2],nrow=length(sentence_ids)))\nfor(i in 1:length(sentence_ids)){\n  if(is.null(nrow(environment[sentence_ids[[i]],]))){\n    its_memory_1[4+i,] <- environment[sentence_ids[[i]],]\n  } else {\n    its_memory_1[4+i,] <- colSums(environment[sentence_ids[[i]],])\n  }\n}\n\nsemantic_vectors_1 <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment,\n                                                its_memory_1,\n                                                tau=3)\n\ncosine_1 <- lsa::cosine(t(semantic_vectors_1))\n\ncorrplot::corrplot(cosine_1,\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n\n\n\n\n\nR2_first_1 <- cor(c(cosine_1),c(first_order_cosines))^2\nR2_first_1\n\n[1] 0.9425725\n\nmatt_R2_first\n\n[1] 0.9578603\n\nR2_second_1 <- cor(c(cosine_1),c(second_order_cosines))^2\nR2_second_1\n\n[1] 0.9978066\n\nmatt_R2_second\n\n[1] 0.9681356\n\n\nI’ve changed three things in Matt’s code: i) changing the bandwidth from 11 to 10; iii) making sure each sentence has the same number of words (i.e. 10); ii) including prior memory (i.e. noise) into ITS memory. Our first-order R-squared values are similar, but my second-order R-squared is slightly higher?\n\n\n\nUsing the R-squared between ITS and the second-order word-word similarities as the metric, we can find out, given a L value (e.g. L = .3, .5, .7), how many sentences a subject needs to obtain learning comparable to when L = 1.0.\n\n\n\nmax_sent_freq <- 15\n\nn_of_sim <- 10\n\n# L = 1.0\n\nR2_full_1.0 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_1.0 <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_1.0),c(second_order_cosines))^2\n  }\nR2_full_1.0[i, ] <- R2_second\n}\n\n\n\n\nFirst, let’s visualize how the word-sentence similarities change as sentences are presented with higher frequency.\n\ncosines_list_0.3 <- list()\n\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nits_memory <- prior_memory\n\nfor(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .3, .3))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_list_0.3[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n\n\nfor(i in 1:length(cosines_list_0.3)){\n  corrplot::corrplot(cosines_list_0.3[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, for the actual simulations.\n\nR2_full_0.3 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .3, .3))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.3 <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_0.3),c(second_order_cosines))^2\n  }\nR2_full_0.3[i, ] <- R2_second\n}\n\n\n\n\nVisualizing the word-sentence similarities with increasing sentence frequency:\n\ncosines_list_0.5 <- list()\n\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .5, .5))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_list_0.5[[j]] <- lsa::cosine(t(semantic_vectors))\n  }\n\n\nfor(i in 1:length(cosines_list_0.5)){\n  corrplot::corrplot(cosines_list_0.5[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActual simulation:\n\nR2_full_0.5 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .5, .5))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.5 <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_0.5),c(second_order_cosines))^2\n  }\nR2_full_0.5[i, ] <- R2_second\n}\n\n\n\n\nVisualizing word-sentence similarities with increasing sentence frequency:\n\ncosines_list_0.7 <- list()\n\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .7, .7))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_list_0.7[[j]] <- lsa::cosine(t(semantic_vectors))\n  }\n\n\nfor(i in 1:length(cosines_list_0.7)){\n  corrplot::corrplot(cosines_list_0.7[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActual simulation:\n\nR2_full_0.7 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .7, .7))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_0.7),c(second_order_cosines))^2\n  }\nR2_full_0.7[i, ] <- R2_second\n}\n\n\n\n\nLet’s visualize the progression of ITS encoding by visualizing the word-sentence similarities.\n\ncosines_list_AL <- list()\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n  \nits_memory_AL <- prior_memory\n\nfor(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids_reordered)) {\n    current_sentence <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    echo <- colSums(as.numeric(cosine_x_to_m(current_sentence, its_memory_AL) ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n\n\nfor(i in 1:length(cosines_list_AL)){\n  corrplot::corrplot(cosines_list_AL[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWord-sentence similarities seem to stabilize around the 7th presentation of the sentences. Now, to run the actual simulations.\n\nR2_full_de <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- sl_create_riv(10000, length(dictionary_words), 8)\n\nR2_second <- numeric(length = max_sent_freq)\n  \nits_memory_AL <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids_reordered)) {\n    current_sentence <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    echo <- colSums(as.numeric(cosine_x_to_m(current_sentence, its_memory_AL) ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_ITS_AL <- lsa::cosine(t(semantic_vectors))\n  R2_second[j] <- cor(c(cosines_ITS_AL),c(second_order_cosines))^2\n  }\nR2_full_de[i, ] <- R2_second\n}\n\n\nits_memory_AL[1:10, 1:5]\n\n            [,1]       [,2]        [,3]       [,4]       [,5]\n [1,]  0.0000000 -1.0000000  1.00000000  1.0000000  0.0000000\n [2,] -1.0000000 -1.0000000 -1.00000000  1.0000000  1.0000000\n [3,] -1.0000000 -1.0000000  1.00000000  0.0000000 -1.0000000\n [4,]  1.0000000  1.0000000 -1.00000000  1.0000000 -1.0000000\n [5,] -0.9977807 -1.0000000  0.58611904  0.2091598 -0.5838998\n [6,]  0.6407729  0.3494726  0.02382425  0.4831534 -0.4895270\n [7,]  0.2735105  0.4661442 -0.99262772  0.6106153 -0.2800206\n [8,] -0.6453965 -0.5849472  0.54819815 -0.6710448  0.6492927\n [9,]  0.3771347  0.2623084 -0.59621311  0.2836291  0.7073388\n[10,] -0.1322865 -0.2228952  0.47443002 -0.2947606  0.1353827\n\nits_memory_AL[1629:1639, 1:5]\n\n               [,1]          [,2]          [,3]          [,4]          [,5]\n [1,] -1.264035e-04 -3.462184e-05  1.011630e-04 -1.083267e-04 -2.263957e-04\n [2,]  3.710836e-05  2.076570e-05 -2.219678e-06  3.243088e-05 -3.216009e-05\n [3,]  1.062101e-05  3.316479e-05 -1.720831e-05 -2.593322e-05 -1.451305e-05\n [4,]  2.593332e-04  1.438617e-04  7.729729e-07  1.967663e-04 -1.925258e-04\n [5,] -8.150816e-05 -6.375385e-05  2.501117e-05 -2.393398e-05  8.336514e-06\n [6,]  5.451425e-05  3.212950e-05 -6.159582e-06  4.448722e-05 -4.163051e-05\n [7,]  4.718266e-04  5.000087e-04 -3.298992e-04 -1.171870e-04  3.096171e-04\n [8,] -1.563369e-04 -1.373852e-04  7.035068e-05 -1.849167e-05 -2.318766e-05\n [9,]  9.120338e-06  3.859228e-05  2.639440e-05 -6.061685e-05 -5.856319e-05\n[10,]  1.778721e-05  3.230424e-05  2.561530e-06 -3.144276e-05 -1.809289e-05\n[11,]  2.849881e-04  1.335925e-04  9.879837e-05  1.526164e-04 -1.822352e-04\n\n\nThe MINERVA-AL discrepancy encoding code seems to be working well. The first few traces encoded have much higher values than the last few traces encoded (when each sentence had already been presented around 14 times before).\n\n\n\n\nR2_no_de <- data.frame(l_value = c(1.0, 0.7, 0.5, 0.3), rbind(colMeans(R2_full_1.0), colMeans(R2_full_0.7), colMeans(R2_full_0.5), colMeans(R2_full_0.3))) \ncolnames(R2_no_de) <- c(\"l_value\", 1:max_sent_freq)\nR2_no_de <- R2_no_de %>% pivot_longer(-l_value, names_to = \"sent_freq\", values_to = \"R2_second_order\")\n\nR2_de <- data.frame(sent_freq = 1:max_sent_freq, colMeans(R2_full_de))\ncolnames(R2_de) <- c(\"sent_freq\", \"R2_second_order\")\n\n\nggplot() + geom_line(data = R2_no_de, aes(x = factor(sent_freq, level = 1:15), y = R2_second_order, group = as.factor(l_value), color = as.factor(l_value))) + geom_line(data = R2_de, aes(x = factor(sent_freq, level = 1:15), y = R2_second_order, group = 1), linetype = \"dashed\")\n\n\n\n\nDoes this mean that, with discrepancy encoding, learning can never reach the same level as that without discrepancy encoding? Also, does this mean that discrepancy encoding even performs worse than no discrepancy encoding at a low learning rate (e.g. L = 0.3)?"
  },
  {
    "objectID": "posts/ITS_DE4/index.html",
    "href": "posts/ITS_DE4/index.html",
    "title": "Exploring discrepancy encoding with ITS (Part 4)",
    "section": "",
    "text": "We identified a difficulty with combining ITS and MINERVA-AL: in ITS, sentence vectors are formed by adding word vectors together, and two or more word vectors can have non-zero elements in the same vector columns (e.g. A = [1, 1, 0, 1], B = [1, 0, 1, 1]. Thus, values in the sentence vectors can exceed the -1 to 1 range. In contrast, in MINERVA-AL, compound stimuli is represented by adding basic components with non-overlapping vector columns (e.g. A = [1, 1, 0, 0], B = [0, 0, 1, 1]). Also, MINERVA-AL always normalizes the echo to the -1 to 1 range before computing the probe-echo discrepancy. Hence, in last week’s exercise, the probes (sentence vectors) had values way larger in magnitude than the echoes, resulting in the discrepancy vectors being strange.\nOne way to mitigate this issue is to normalize the sentence vectors so that they also fall within the -1 to 1 range. Another method is to apply something similar to one-hot encoding to ensure the word vectors don’t have overlapping vector columns, so sentences always remain within the -1 to 1 range.\nThe goal of this exercise is to repeat last week’s exercise while apply one-hot encoding-style representations for word vectors.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(RsemanticLibrarian)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nn_words <- 130\nband_width <- 10\n\n# Setup of word-sentence co-occurence matrix\nband_matrix <- Matrix::bandSparse(n = n_words,k=0:(band_width-1))\nband_matrix <- as.matrix(band_matrix)*1\n\n# ITS setup (each word appears in the same number of sentences)\ndictionary_words <- 1:n_words\n\nsentence_ids <- list()\nfor(i in 1:ncol(band_matrix)){\n  sentence_ids[[i]] <- which(band_matrix[,i] == 1)\n}\nsentence_ids <- sentence_ids[11:119]\n\nwords_to_plot <- c(11:110)\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  return(word_meaning)\n}\n\n# Function to create environment vectors with non-overlapping columns (modified from RsemanticLibrarian::sl_create_riv)\nno_overlap_riv <- function(dimensions, no_of_words, sparsity = 10){\n  if(sparsity %% 2 != 0) stop(\"sparsity must be even integer\")\n  temp_matrix <- matrix(0,ncol=dimensions,nrow=no_of_words)\n  for(i in 1:no_of_words){\n    temp_matrix[i, (i*10):(i*10+sparsity-1)] <- sample(c(rep(1, sparsity/2), rep(-1, sparsity/2)))\n  }\n  return(temp_matrix)\n}\n\n\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines <- lsa::cosine(t(band_matrix[words_to_plot,]))\nsecond_order_cosines <- lsa::cosine(t(first_order_cosines))\n\n\n\n\nmax_sent_freq <- 15\nn_of_sim <- 10\n\n\n\n\nR2_first_1.0 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_1.0 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory <- prior_memory\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_1.0 <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_1.0),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_1.0),c(second_order_cosines))^2\n  }\nR2_first_1.0[i, ] <- R2_first\nR2_second_1.0[i, ] <- R2_second\n}\n\n\n\n\nFirst, let’s visualize how the semantic vector similarities change as sentences are presented with higher frequency.\n\ncosines_list_0.3 <- list()\n\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\nprior_memory_test <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory <- prior_memory_test\n\nfor(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .3, .3))\n    }\n    its_memory <- rbind(its_memory, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_list_0.3[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n\n\nfor(i in 1:length(cosines_list_0.3)){\n  corrplot::corrplot(cosines_list_0.3[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, for the actual simulations.\n\nR2_first_0.3 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_0.3 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory_0.3 <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .3, .3))\n    }\n    its_memory_0.3 <- rbind(its_memory_0.3, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_0.3,\n                                                tau=3)\n  cosines_ITS_0.3 <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_0.3),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_0.3),c(second_order_cosines))^2\n  }\nR2_first_0.3[i, ] <- R2_first\nR2_second_0.3[i, ] <- R2_second\n}\n\n\n\n\n\nR2_first_0.5 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_0.5 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory_0.5 <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .5, .5))\n    }\n    its_memory_0.5 <- rbind(its_memory_0.5, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_0.5,\n                                                tau=3)\n  cosines_ITS_0.5 <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_0.5),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_0.5),c(second_order_cosines))^2\n  }\nR2_first_0.5[i, ] <- R2_first\nR2_second_0.5[i, ] <- R2_second\n}\n\n\n\n\n\nR2_first_0.7 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_0.7 <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory_0.7 <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    its_memory_chunk <- matrix(0, ncol=dim(environment_sub)[2],nrow=length(sentence_ids))\n    for(k in 1:length(sentence_ids_reordered)) {\n    its_memory_chunk[k, ] <- colSums(environment_sub[sentence_ids_reordered[[k]],]) * sample(c(0,1), ncol(environment_sub), replace = TRUE, prob = c(1 - .7, .7))\n    }\n    its_memory_0.7 <- rbind(its_memory_0.7, its_memory_chunk)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_0.7,\n                                                tau=3)\n  cosines_ITS_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_0.7),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_0.7),c(second_order_cosines))^2\n  }\nR2_first_0.7[i, ] <- R2_first\nR2_second_0.7[i, ] <- R2_second\n}\n\n\n\n\nLet’s visualize the progression of ITS encoding by visualizing the semantic vector similarities. First, let’s examine the similarities when the sentences are in the original order.\n\ncosines_list_AL <- list()\n\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nfull_norm_echoes <- c()\n\nintensities_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL <- prior_memory_test\n\nfor(j in 1:max_sent_freq){\n  intensities <- numeric(length = length(sentence_ids))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  intensities_matrix[j, ] <- intensities\n  full_norm_echoes <- rbind(full_norm_echoes, normalized_echoes)\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n\n\nfor(i in 1:length(cosines_list_AL)){\n  corrplot::corrplot(cosines_list_AL[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can we explain the red lines? The red lines are visible from the first iteration and get clearer with increasing sentence frequency. They’re are also well-defined and sharp, instead of fading out. Each semantic vector has 2 semantic vectors it is negatively related to: one that is 10 words before it, and one that is 10 words after it. The magnitude of the red lines also follows a regular pattern, growing stronger and weaker in regular cycles.\nLet’s also plot the intensities to get a better idea of whether what’s being stored is still related to the original sentences.\n\nintensities_df <- data.frame(sent_freq = 1:15, intensities_matrix)\nintensities_df <- intensities_df %>% pivot_longer(-sent_freq, names_to = \"sentence\", names_prefix = \"X\", names_transform = list(sentence = as.numeric), values_to = \"intensities\")\nintensities_sliver <- intensities_df %>% filter(sent_freq %in% c(1, 5, 10, 15))\nggplot(intensities_sliver, aes(x = sentence, y = intensities, group = sent_freq, color = sent_freq)) + geom_line()\n\n\n\n\nNot really sure what to make of the peaks and valleys across the sentences, but at least intensity increases with sentence frequency, which is what we expect.\nNow let’s see the similarities when the sentences are randomly reordered every time they are presented.\n\ncosines_list_AL <- list()\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n  \nits_memory_AL <- prior_memory_test\n\nfor(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids_reordered)) {\n    current_sentence <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    echo <- colSums(as.numeric(cosine_x_to_m(current_sentence, its_memory_AL) ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n}\n\n\nfor(i in 1:length(cosines_list_AL)){\n  corrplot::corrplot(cosines_list_AL[[i]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe reordering of the sentences affects the regular pattern of the magnitudes of the red regions. Here, the blue areas also disappear with increasing sentence frequency.\n\nR2_first_AL <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\nR2_second_AL <- matrix(0, nrow = n_of_sim, ncol = max_sent_freq)\n\nfor(i in 1:n_of_sim){\nenvironment_sub <- no_overlap_riv(10000, length(dictionary_words))\n\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory_AL <- prior_memory\n\n  for(j in 1:max_sent_freq){\n    sentence_ids_reordered <- sample(sentence_ids)\n    for(k in 1:length(sentence_ids_reordered)) {\n    current_sentence <- colSums(environment_sub[sentence_ids_reordered[[k]],])\n    echo <- colSums(as.numeric(cosine_x_to_m(current_sentence, its_memory_AL) ^ 3) * (its_memory_AL))\n    echo_norm <- echo/max(abs(echo))\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_ITS_AL <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS_AL),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS_AL),c(second_order_cosines))^2\n  }\nR2_first_AL[i, ] <- R2_first\nR2_second_AL[i, ] <- R2_second\n}\n\n\n\n\n\n# R-squared wrt first-order similarity\n\nR2_first_no_de <- data.frame(l_value = c(1.0, 0.7, 0.5, 0.3), rbind(colMeans(R2_first_1.0), colMeans(R2_first_0.7), colMeans(R2_first_0.5), colMeans(R2_first_0.3)))\ncolnames(R2_first_no_de) <- c(\"l_value\", 1:max_sent_freq)\nR2_first_no_de <- R2_first_no_de %>% pivot_longer(-l_value, names_to = \"sent_freq\", values_to = \"R2_first_order\")\n\nR2_first_de <- data.frame(sent_freq = 1:max_sent_freq, colMeans(R2_first_AL))\ncolnames(R2_first_de) <- c(\"sent_freq\", \"R2_first_order\")\n\n\nggplot() + geom_line(data = R2_first_no_de, aes(x = factor(sent_freq, level = 1:15), y = R2_first_order, group = as.factor(l_value), color = as.factor(l_value))) + geom_line(data = R2_first_de, aes(x = factor(sent_freq, level = 1:15), y = R2_first_order, group = 1), linetype = \"dashed\")\n\n\n\n\n\n# R-squared wrt second-order similarity\n\nR2_second_no_de <- data.frame(l_value = c(1.0, 0.7, 0.5, 0.3), rbind(colMeans(R2_second_1.0), colMeans(R2_second_0.7), colMeans(R2_second_0.5), colMeans(R2_second_0.3)))\ncolnames(R2_second_no_de) <- c(\"l_value\", 1:max_sent_freq)\nR2_second_no_de <- R2_second_no_de %>% pivot_longer(-l_value, names_to = \"sent_freq\", values_to = \"R2_second_order\")\n\nR2_second_de <- data.frame(sent_freq = 1:max_sent_freq, colMeans(R2_second_AL))\ncolnames(R2_second_de) <- c(\"sent_freq\", \"R2_second_order\")\n\n\nggplot() + geom_line(data = R2_second_no_de, aes(x = factor(sent_freq, level = 1:15), y = R2_second_order, group = as.factor(l_value), color = as.factor(l_value))) + geom_line(data = R2_second_de, aes(x = factor(sent_freq, level = 1:15), y = R2_second_order, group = 1), linetype = \"dashed\")\n\n\n\n\nThe discrepancy encoding is still hitting a ceiling early on before declining, never reaching the values for when there is no discrepancy encoding. This is the case for the R-squared with respect to both first- and second-order similarities."
  },
  {
    "objectID": "posts/ITS_DE5/index.html",
    "href": "posts/ITS_DE5/index.html",
    "title": "Exploring discrepancy encoding with ITS (Part 5)",
    "section": "",
    "text": "My R-squared values were peaking around .2, while Matt’s were peaking around .6. After some tinkering, I realized that this is at least partially because my RIVs were much sparser (10000 columns, sparsity = 8) than his (1000 columns, sparsity = 100). Using his code, I ran it once with each of the RIVs and found that the sparser RIVs led to lower R-squared values.\nThis week, I am going to use less sparse RIVs and do the following:\n\nExamine how different learning rates affect the reconstruction, intensity and R-squared values\nWithout reordering the sentences, plot the sentence vectors, echoes, and discrepancies to visualize better what traces are being stored, and how that affects what happens when the same sentence is presented again in the next epochs.\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   1.0.1\n✔ tibble  3.2.0     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(RsemanticLibrarian)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(tm)\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nlibrary(lsa)\n\nLoading required package: SnowballC\n\nn_words <- 130\nband_width <- 10\n\n# Setup of word-sentence co-occurence matrix\nband_matrix <- Matrix::bandSparse(n = n_words,k=0:(band_width-1))\nband_matrix <- as.matrix(band_matrix)*1\n\n# ITS setup (each word appears in the same number of sentences)\ndictionary_words <- 1:n_words\n\nsentence_ids <- list()\nfor(i in 1:ncol(band_matrix)){\n  sentence_ids[[i]] <- which(band_matrix[,i] == 1)\n}\nsentence_ids <- sentence_ids[10:118]\n\nwords_to_plot <- c(10:109)\n\n# Function to generate echoes for a list of words (from Matt)\nsubset_semantic_vectors <- function(strings,dictionary,e_vectors,sentence_memory,tau=3){\n  \n  word_meaning <- matrix(0, ncol = dim(e_vectors)[2],\n                         nrow = length(strings))\n  \n  for (i in 1:length(strings)) {\n    word_id <- which(dictionary %in% strings[i])\n    probe <- e_vectors[word_id, ]\n    activations <- cosine_x_to_m(probe, sentence_memory)\n    echo <- colSums(as.numeric(activations ^ tau) * (sentence_memory))\n    word_meaning[i, ] <- echo\n  }\n  \n  row.names(word_meaning) <- strings\n  return(word_meaning)\n}\n\n# Function to create environment vectors with non-overlapping columns (modified from RsemanticLibrarian::sl_create_riv) - only uses 1s (no -1 values)\nno_overlap_riv <- function(dimensions, no_of_words, sparsity = 10){\n  if(sparsity %% 2 != 0) stop(\"sparsity must be even integer\")\n  temp_matrix <- matrix(0,ncol=dimensions,nrow=no_of_words)\n  for(i in 1:no_of_words){\n    temp_matrix[i, (i*10):(i*10+sparsity-1)] <- sample(c(rep(1, sparsity/2), rep(1, sparsity/2)))\n  }\n  return(temp_matrix)\n}\n\n\n# Ground truths to compare to: first- and second- order similarity\nfirst_order_cosines <- lsa::cosine(t(band_matrix[words_to_plot,]))\nsecond_order_cosines <- lsa::cosine(t(first_order_cosines))\n\nmax_sent_freq <- 10\n\n\n\n\n\n\n\nR2_first <- numeric(length = max_sent_freq)\nR2_second <- numeric(length = max_sent_freq)\n\nenvironment_sub <- no_overlap_riv(1500, length(dictionary_words))\nprior_memory <- t(replicate(100, sample(c(-1,0,1), ncol(environment_sub), replace = TRUE)))\nits_memory <- prior_memory\n\nintensities_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, current_sentence)\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS <- lsa::cosine(t(semantic_vectors))\n  R2_first[j] <- cor(c(cosines_ITS),c(first_order_cosines))^2\n  R2_second[j] <- cor(c(cosines_ITS),c(second_order_cosines))^2\n  intensities_matrix[j, ] <- intensities\n  reconst_matrix[j, ] <- reconstruction\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n\n\n\n\ncosines_list_AL <- list()\n\nR2_first_AL <- numeric(length = max_sent_freq)\nR2_second_AL <- numeric(length = max_sent_freq)\n\nenvironment_sub <- no_overlap_riv(1500, length(dictionary_words))\n\nsentence_matrix <- c()\n\nnorm_echoes_matrix <- c()\n\nintensities_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- echo/max(abs(echo))\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL <- rbind(its_memory_AL, (current_sentence - echo_norm))\n    }\n  sentence_matrix <- rbind(sentence_matrix, sentences)\n  norm_echoes_matrix <- rbind(norm_echoes_matrix, normalized_echoes)\n  intensities_matrix_AL[j, ] <- intensities\n  reconst_matrix_AL[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL,\n                                                tau=3)\n  cosines_list_AL[[j]] <- lsa::cosine(t(semantic_vectors))\n  R2_first_AL[j] <- cor(c(cosines_list_AL[[j]]),c(first_order_cosines))^2\n  R2_second_AL[j] <- cor(c(cosines_list_AL[[j]]),c(second_order_cosines))^2\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n\ncorrplot::corrplot(cosines_list_AL[[10]],\n                   method=\"circle\",\n                   order=\"original\",\n                   tl.col=\"black\",\n                   tl.cex=.4,\n                   title=\"\",\n                   cl.cex=.5,\n                   outline=FALSE,\n                   addgrid.col=NA)\n\n\n\n\n\n\n\n\nno_de_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix), rowMeans(intensities_matrix), R2_first, R2_second)\ncolnames(no_de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_perfect <- no_de_df_perfect %>% pivot_longer(c(first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_perfect <- data.frame(learning_rate = \"1.0\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL), rowMeans(intensities_matrix_AL), R2_first_AL, R2_second_AL)\ncolnames(de_df_perfect) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"first_order_R2\", \"second_order_R2\")\nde_df_perfect <- de_df_perfect %>% pivot_longer(c(first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_perfect <- bind_rows(no_de_df_perfect, de_df_perfect)\nggplot(compare_df_perfect, aes(x = sent_freq, y = reconstruction, color = condition)) + geom_line()\n\n\n\nggplot(compare_df_perfect, aes(x = sent_freq, y = intensity, color = condition)) + geom_line()\n\n\n\nggplot(compare_df_perfect, aes(x = sent_freq, y = R_squared, color = order_of_sim)) + geom_line() + facet_wrap(vars(condition))\n\n\n\n\n\n\n\n\n\n\n\nL <- .7\n\nR2_first_0.7 <- numeric(length = max_sent_freq)\nR2_second_0.7 <- numeric(length = max_sent_freq)\n\nenvironment_sub <- no_overlap_riv(1500, length(dictionary_words))\n\nits_memory <- prior_memory\n\nintensities_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\n  for(j in 1:max_sent_freq){\n    print(j)\n    intensities <- numeric(length = length(sentence_ids))\n    reconstruction <- numeric(length = length(sentence_ids))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    activations <- cosine_x_to_m(current_sentence, its_memory)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    its_memory <- rbind(its_memory, (current_sentence*(rbinom(length(current_sentence), 1, L))))\n    }\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory,\n                                                tau=3)\n  cosines_ITS_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_first_0.7[j] <- cor(c(cosines_ITS_0.7),c(first_order_cosines))^2\n  R2_second_0.7[j] <- cor(c(cosines_ITS_0.7),c(second_order_cosines))^2\n  intensities_matrix_0.7[j, ] <- intensities\n  reconst_matrix_0.7[j, ] <- reconstruction\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n\n\n\n\nR2_first_AL_0.7 <- numeric(length = max_sent_freq)\nR2_second_AL_0.7 <- numeric(length = max_sent_freq)\n\nsentence_matrix_0.7 <- c()\n\nnorm_echoes_matrix_0.7 <- c()\n\nintensities_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n\nreconst_matrix_AL_0.7 <- matrix(0, nrow = max_sent_freq, ncol = length(sentence_ids))\n  \nits_memory_AL_0.7 <- prior_memory\n\nfor(j in 1:max_sent_freq){\n  print(j)\n  intensities <- numeric(length = length(sentence_ids))\n  reconstruction <- numeric(length = length(sentence_ids))\n  sentences <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n  normalized_echoes <- matrix(0, nrow = length(sentence_ids), ncol = ncol(environment_sub))\n    for(k in 1:length(sentence_ids)) {\n    current_sentence <- colSums(environment_sub[sentence_ids[[k]],])\n    sentences[k, ] <- current_sentence\n    activations <- cosine_x_to_m(current_sentence, its_memory_AL_0.7)\n    intensities[k] <- sum(activations^3)\n    echo <- colSums(as.numeric(activations ^ 3) * (its_memory_AL_0.7))\n    reconstruction[k] <- cosine(current_sentence, echo)\n    echo_norm <- echo/max(abs(echo))\n    normalized_echoes[k, ] <- echo_norm\n    its_memory_AL_0.7 <- rbind(its_memory_AL_0.7, ((current_sentence - echo_norm)*(rbinom(length(current_sentence), 1, L))))\n    }\n  sentence_matrix_0.7 <- rbind(sentence_matrix_0.7, sentences)\n  norm_echoes_matrix_0.7 <- rbind(norm_echoes_matrix_0.7, normalized_echoes)\n  intensities_matrix_AL_0.7[j, ] <- intensities\n  reconst_matrix_AL_0.7[j, ] <- reconstruction\n  semantic_vectors <- subset_semantic_vectors(words_to_plot,\n                                                dictionary_words,\n                                                environment_sub,\n                                                its_memory_AL_0.7,\n                                                tau=3)\n  cosines_AL_0.7 <- lsa::cosine(t(semantic_vectors))\n  R2_first_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(first_order_cosines))^2\n  R2_second_AL_0.7[j] <- cor(c(cosines_AL_0.7),c(second_order_cosines))^2\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n\n\n\n\nno_de_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"no_de\", sent_freq = 1:10, rowMeans(reconst_matrix_0.7), rowMeans(intensities_matrix_0.7), R2_first_0.7, R2_second_0.7)\ncolnames(no_de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"first_order_R2\", \"second_order_R2\")\nno_de_df_0.7 <- no_de_df_0.7 %>% pivot_longer(c(first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\nde_df_0.7 <- data.frame(learning_rate = \"0.7\", condition = \"de\", sent_freq = 1:10, rowMeans(reconst_matrix_AL_0.7), rowMeans(intensities_matrix_AL_0.7), R2_first_AL_0.7, R2_second_AL_0.7)\ncolnames(de_df_0.7) <- c(\"learning_rate\", \"condition\", \"sent_freq\", \"reconstruction\", \"intensity\", \"first_order_R2\", \"second_order_R2\")\nde_df_0.7 <- de_df_0.7 %>% pivot_longer(c(first_order_R2, second_order_R2), names_to = \"order_of_sim\", values_to = \"R_squared\")\n\ncompare_df_0.7 <- bind_rows(no_de_df_0.7, de_df_0.7)\nggplot(compare_df_0.7, aes(x = sent_freq, y = reconstruction, color = condition)) + geom_line()\n\n\n\nggplot(compare_df_0.7, aes(x = sent_freq, y = intensity, color = condition)) + geom_line()\n\n\n\nggplot(compare_df_0.7, aes(x = sent_freq, y = R_squared, color = order_of_sim)) + geom_line() + facet_wrap(vars(condition))\n\n\n\n\n\n\n\n\n\ncomplete_df <- bind_rows(compare_df_perfect, compare_df_0.7)\ncomplete_df$learning_rate <- factor(complete_df$learning_rate, levels = c(\"1.0\", \"0.7\"))\nggplot(complete_df, aes(x = sent_freq, y = reconstruction, color = condition, linetype = learning_rate)) + geom_line()\n\n\n\nggplot(complete_df, aes(x = sent_freq, y = intensity, color = condition, linetype = learning_rate)) + geom_line()\n\n\n\nggplot(complete_df, aes(x = sent_freq, y = R_squared, color = order_of_sim, linetype = learning_rate)) + geom_line() + facet_wrap(vars(condition))\n\n\n\n\n\n\n\n\ndiscrep_matrix <- its_memory_AL[101:1190, ]\n\nsent_df <- data.frame(trial_no = 1:nrow(sentence_matrix), type = \"sentence\", sentence_matrix) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\necho_df <- data.frame(trial_no = 1:nrow(norm_echoes_matrix), type = \"echo_norm\", norm_echoes_matrix) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\ndiscrep_df <- data.frame(trial_no = 1:nrow(discrep_matrix), type = \"discrepancy\", discrep_matrix) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\n\ncombined_df <- bind_rows(sent_df, echo_df, discrep_df)\ncombined_df$type <- factor(combined_df$type, levels = c(\"sentence\", \"echo_norm\", \"discrepancy\"))\n\n\ndiscrep_matrix_0.7 <- its_memory_AL_0.7[101:1190, ]\n\nsent_df_0.7 <- data.frame(trial_no = 1:nrow(sentence_matrix_0.7), type = \"sentence\", sentence_matrix_0.7) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\necho_df_0.7 <- data.frame(trial_no = 1:nrow(norm_echoes_matrix_0.7), type = \"echo\", norm_echoes_matrix_0.7) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\ndiscrep_df_0.7 <- data.frame(trial_no = 1:nrow(discrep_matrix_0.7), type = \"discrepancy\", discrep_matrix_0.7) %>% pivot_longer(-c(trial_no,type), names_to = \"element\", names_prefix = \"X\", names_transform = list(element = as.numeric), values_to = \"value\")\n\ncombined_df_0.7 <- bind_rows(sent_df_0.7, echo_df_0.7, discrep_df_0.7)\ncombined_df_0.7$type <- factor(combined_df_0.7$type, levels = c(\"sentence\", \"echo\", \"discrepancy\"))\n\n\ntrial_12 <- combined_df %>% filter(trial_no %in% c(1:5, 110:114))\nggplot(trial_12, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n\n\n\ntrial_12_0.7 <- combined_df_0.7 %>% filter(trial_no %in% c(1:5, 110:114))\nggplot(trial_12_0.7, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n\n\n\n\n\ntrial_34 <- combined_df %>% filter(trial_no %in% c(219:223, 328:332))\nggplot(trial_34, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n\n\n\ntrial_34_0.7 <- combined_df_0.7 %>% filter(trial_no %in% c(219:223, 328:332))\nggplot(trial_34_0.7, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n\n\n\n\n\ntrial_56 <- combined_df %>% filter(trial_no %in% c(437:441, 546:550))\nggplot(trial_56, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n\n\n\ntrial_56_0.7 <- combined_df_0.7 %>% filter(trial_no %in% c(437:441, 546:550))\nggplot(trial_56_0.7, aes(x = element, y = value)) + geom_line() + facet_grid(rows = vars(as.factor(trial_no)), cols = vars(type))\n\n\n\n\nWhen learning rate is 1, the discrepancy traces becomes almost negligible by the 6th presentation of the sentence. When learning rate is .7, the discrepancy traces require more presentations before becoming almost negligible. Thus, there are more traces stored that are more similar to the sentence, leading to higher activations.\nMaybe these higher activations then lead to higher R-squared values?"
  }
]