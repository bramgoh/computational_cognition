[
  {
    "objectID": "posts/Hintzman1/index.html",
    "href": "posts/Hintzman1/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 1)",
    "section": "",
    "text": "#Generating category prototypes and exemplars\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\nvalues <- c(rep(-1, 50), rep(1, 50))\nprotoA <- sample(values, 23, replace = TRUE)\nprotoB <- sample(values, 23, replace = TRUE)\nprotoC <- sample(values, 23, replace = TRUE)\nfeat_index <- 11:23\n\ndistort_low <- function(x) {\n  change_index <- sample(feat_index, 2)\n  x_new <- x\n  x_new[change_index[1]] <- x_new[change_index[1]] * -1\n  x_new[change_index[2]] <- x_new[change_index[2]] * -1\n  return(x_new)\n}\ndistort_high <- function(x) {\n  change_index <- sample(feat_index, 4)\n  x_new <- x\n  x_new[change_index[1]] <- x_new[change_index[1]] * -1\n  x_new[change_index[2]] <- x_new[change_index[2]] * -1\n  x_new[change_index[3]] <- x_new[change_index[3]] * -1\n  x_new[change_index[4]] <- x_new[change_index[4]] * -1\n  return(x_new)\n}\n\nlow_A1 <- distort_low(protoA)\nlow_A2 <- distort_low(protoA)\nlow_A3 <- distort_low(protoA)\nlow_B1 <- distort_low(protoB)\nlow_B2 <- distort_low(protoB)\nlow_B3 <- distort_low(protoB)\nlow_B4 <- distort_low(protoB)\nlow_B5 <- distort_low(protoB)\nlow_B6 <- distort_low(protoB)\nlow_C1 <- distort_low(protoC)\nlow_C2 <- distort_low(protoC)\nlow_C3 <- distort_low(protoC)\nlow_C4 <- distort_low(protoC)\nlow_C5 <- distort_low(protoC)\nlow_C6 <- distort_low(protoC)\nlow_C7 <- distort_low(protoC)\nlow_C8 <- distort_low(protoC) \nlow_C9 <- distort_low(protoC)\n\nlow_C_full <- rbind(low_C1, low_C2, low_C3, low_C4, low_C5, low_C6, low_C7, low_C8, low_C9)\n\nhigh_A1 <- distort_high(protoA)\nhigh_B1 <- distort_high(protoB)\nhigh_C1 <- distort_high(protoC)\n\nThe code works, but is very inelegant and requires a lot of copy and pasting. There has to be a way to automate this with a function so that I can simulate this whole thing 20 times.\n\nindexC_high <- seq(1, 23, 1)\nfor(i in 1:9) {\n  temp <- distort_high(protoC)\n  indexC_high <- rbind(indexC_high, temp)\n}\n\nThis is faster, but still requires specifying the variable to store it in, the number of exemplars, the function and the prototype. Attempts to create a function to automate this failed, as the dataframe returned (e.g. indexC_high) was unchanged."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "I’m a millennial and technology is not my forte.\nIn spite of that, here is my attempt at a blog record of things I’ve learned. Here’s to jumping off the deep end…"
  },
  {
    "objectID": "posts/TechSetUp/index.html",
    "href": "posts/TechSetUp/index.html",
    "title": "Setting up the Techymabobs",
    "section": "",
    "text": "Matt introduced me to a ton of new software to download and familiarize myself with.\nQuarto blog\nI updated both R and RStudio, and the Quarto blog works as intended. However, remember to NOT rename the index.qmd etc. files, as that interferes with the seamless rendering process and the blog will not be updated as you render.\nDragging and dropping images still does not work. It would be really convenient if this could be rectified.\nSometime in the near future, I need to refresh my Rmarkdown code wrt to stylistic features, so I can do away with the default blog visuals.\nZotero\nI downloaded Zotero and the Safari plugin, and it seems to work fine. It syncs with R and with Microsoft Word, and my brief tests don’t raise any issues.\nI do have to note that most articles downloaded from Google Scholar do not contain DOI information, so this is something I still have to add in manually.\nGithub Desktop\nDownloading this was the easy part. Committing to changes and publishing/pushing was also not too difficult to figure out.\nThe only roadblock I’ve faced thus far is obtaining the blog URL. After setting the output directory to docs, I get a message saying that the pages failed to be built? I’m not sure how to fix this.\nI’ve noticed also that, while I’ve downloaded Github Desktop, I can’t seem to get the “Create a git repository” option when I start a new project in RStudio. An hour of Googling and troubleshooting has gotten me nowhere, so I’m stumped for now."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Cognition",
    "section": "",
    "text": "A blog record of a computational cognition journey\n\n\n\n\n\n\n\n\n  \n\n\n\n\nA MINERVA model for attention (Part 2)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nA MINERVA model for attention\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nHintzman’s (1988) MINERVA (Part 2)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nHintzman’s (1988) MINERVA (Part 1)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nBram Goh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproducing Hintzman’s MINERVA (Part 5)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nReproducing Hintzman’s MINERVA (Part 4)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nReproducing Hintzman’s MINERVA (Part 3)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nReproducing Hintzman’s MINERVA (Part 2)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nReproducing Hintzman’s MINERVA (Part 1)\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nBram Goh\n\n\n\n\n\n\n  \n\n\n\n\nSetting up the Techymabobs\n\n\n\n\n\n\n\ntech\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nBram Goh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Hintzman2/index.html",
    "href": "posts/Hintzman2/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 2)",
    "section": "",
    "text": "Generating traces and calculating the echo\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\nnum_feat <- 13\nnum_name <- 10\nnum_trace <- num_name + num_feat\nvalues <- c(rep(1, 50), rep(-1, 50))\n\n# Generating exemplars with distortions\n\ngen_exemp <- function(num_exemp, num_distort) {\n  \n  name <- sample(values, num_name)\n  name_copies <- matrix(rep(name, num_exemp), nrow = num_exemp, ncol = num_name, byrow = TRUE)\n  \n  feat_raw <- t(replicate(num_exemp, sample(values, num_feat)))\n  feat_filter <- t(replicate(n = num_exemp, sample(c(rep(1, num_feat - num_distort), rep(-1, num_distort)), num_feat)))\n  \n  feat_final <- feat_raw * feat_filter\n\n  exemp <- cbind(name_copies, feat_final)\n  \n  return(exemp)\n}\n\n\nempty_feat <- rep(0, num_feat)\n\nabstraction <- function(n1, n2, n3, num_distort) {\n\n# Generating traces and probes\na <- gen_exemp(n1, num_distort)\nprobe_a <- c(a[1, 1:num_name], empty_feat)\nb <- gen_exemp(n2, num_distort)\nprobe_b <- c(b[1, 1:num_name], empty_feat)\nc <- gen_exemp(n3, num_distort)\nprobe_c <- c(c[1, 1:num_name], empty_feat)\n  \nsm <- rbind(a, b, c)\n  \n# Echo activation function\necho_activation <- function(probe, sec_mem) {\n  e_int <- c()\n      for(i in 1:nrow(sec_mem)){\n          n_rel <- 0\n          temp_sim <- 0\n              for(j in 1:num_trace){\n                   current <- probe[j] * sec_mem[i, j]\n                   temp_sim <- current + temp_sim\n                          if(probe[j] != 0 & sec_mem[i, j] != 0) {\n                                     n_rel <- n_rel + 1\n      }\n}\ntrace_sim <- temp_sim/n_rel\ntrace_act <- trace_sim^3\ne_int <- c(e_int, trace_act)\n      }\n  return(e_int)\n}\n\nactivs_a <- echo_activation(probe_a, sm)\nactivs_b <- echo_activation(probe_b, sm)\nactivs_c <- echo_activation(probe_c, sm)\n\n# Echo content function\n\necho_content <- function(acts, sec_mem) {\n  e_cont <- c()\n  for(j in 1:num_trace){\n    temp_cont <- 0\n    for(i in 1:nrow(sec_mem)){\n      current <- acts[i] * sec_mem[i, j]\n      temp_cont <- current + temp_cont\n    }\n    e_cont <- c(e_cont, temp_cont)\n  }\n  return(e_cont)\n}\n\n# Calculating echo intensity and probe-echo correlations\necho_a <- round(echo_content(activs_a, sm), 3)\ncor_a <- cor(probe_a, echo_a)\nint_a <- sum(activs_a)\necho_b <- round(echo_content(activs_b, sm), 3)\ncor_b <- cor(probe_b, echo_b)\nint_b <- sum(activs_b)\necho_c <- round(echo_content(activs_c, sm), 3)\ncor_c <- cor(probe_c, echo_c)\nint_c <- sum(activs_c)\n\ndf <- data.frame(corr = c(cor_a, cor_b, cor_c), intensity = c(int_a, int_b, int_c))\nrownames(df) <- c(\"catA\", \"catB\", \"catC\")\nreturn(df)\n}\n\nI am close to replicating the Abstraction exercise in Hintzman (1986). I’ve made a mistake, however, as I am supposed to calculate prototype-echo correlations, not probe-echo correlations. I’ll need to rectify this, as well as amend the code to allow for 20 simulations, so that I can calculate mean prototype-echo correlations for the 3-, 6-, and 9-exemplar categories."
  },
  {
    "objectID": "posts/Hintzman3/index.html",
    "href": "posts/Hintzman3/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 3)",
    "section": "",
    "text": "Improving on the Abstraction exercise code and plotting prototypes and echoes\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\nnum_feat <- 13\nnum_name <- 10\nnum_trace <- num_name + num_feat\nvalues <- c(rep(1, 50), rep(-1, 50))\n\nempty_feat <- rep(0, num_feat)\n\nabstraction <- function(num_a, num_b, num_c, num_distort) {\n\n# Generating prototype\n  \ngen_proto <- function() {\n   proto <- sample(values, num_trace, replace = TRUE)\n   return(proto)\n}\n\nproto_a <- gen_proto()\nproto_b <- gen_proto()\nproto_c <- gen_proto()\n\n# Generating exemplars with distortions\n\ngen_exemp <- function(proto, num_exemp, num_distort) {\n  \n  proto_copies <- matrix(rep(proto, num_exemp), nrow = num_exemp, ncol = num_trace, byrow = TRUE)\n  \n  name_matrix_ones <- matrix(1, nrow = num_exemp, ncol = num_name)\n  feat_matrix_distort <- t(replicate(n = num_exemp, sample(c(rep(1, num_feat - num_distort), rep(-1, num_distort)), num_feat)))\n  distort_filter <- cbind(name_matrix_ones, feat_matrix_distort)\n\n  exemp <- proto_copies * distort_filter\n  return(exemp)\n}\n  \n# Generating traces and probes\na <- gen_exemp(proto_a, num_a, num_distort)\nprobe_a <- c(a[1, 1:num_name], empty_feat)\nb <- gen_exemp(proto_b, num_b, num_distort)\nprobe_b <- c(b[1, 1:num_name], empty_feat)\nc <- gen_exemp(proto_c, num_c, num_distort)\nprobe_c <- c(c[1, 1:num_name], empty_feat)\n  \nsm <- rbind(a, b, c)\n  \n# Echo activation function\necho_activation <- function(probe, sec_mem) {\n  e_activs <- c()\n      for(i in 1:nrow(sec_mem)){\n          n_rel <- 0\n          temp_sim <- 0\n              for(j in 1:num_trace){\n                   current <- probe[j] * sec_mem[i, j]\n                   temp_sim <- current + temp_sim\n                          if(probe[j] != 0 & sec_mem[i, j] != 0) {\n                                     n_rel <- n_rel + 1\n      }\n}\ntrace_sim <- temp_sim/n_rel\ntrace_act <- trace_sim^3\ne_activs <- c(e_activs, trace_act)\n      }\n  return(e_activs)\n}\n\nactivs_a <- echo_activation(probe_a, sm)\nactivs_b <- echo_activation(probe_b, sm)\nactivs_c <- echo_activation(probe_c, sm)\n\n# Echo content function\n\necho_content <- function(acts, sec_mem) {\n  e_cont <- c()\n  for(j in 1:num_trace){\n    temp_cont <- 0\n    for(i in 1:nrow(sec_mem)){\n      current <- acts[i] * sec_mem[i, j]\n      temp_cont <- current + temp_cont\n    }\n    e_cont <- c(e_cont, temp_cont)\n  }\n  return(e_cont)\n}\n\n# Calculating echo intensity and probe-echo correlations\necho_a <- round(echo_content(activs_a, sm), 3)\ncor_a <- cor(proto_a[(num_name + 1):num_trace], echo_a[(num_name + 1):num_trace])\nint_a <- sum(activs_a)\necho_b <- round(echo_content(activs_b, sm), 3)\ncor_b <- cor(proto_b[(num_name + 1):num_trace], echo_b[(num_name + 1):num_trace])\nint_b <- sum(activs_b)\necho_c <- round(echo_content(activs_c, sm), 3)\ncor_c <- cor(proto_c[(num_name + 1):num_trace], echo_c[(num_name + 1):num_trace])\nint_c <- sum(activs_c)\n\noutput_mat <- rbind(proto_a, echo_a, proto_b, echo_b, proto_c, echo_c)\nreturn(output_mat)\n}\n\nI edited the correlation code to reflect only the correlation for the 13 stimulus features (excluding the 10 name features; clearly, I misunderstood Hintzman the first time). Also, I changed the output to a matrix with the probes and echoes for the 3 categories. However, why do the echoes have values greater than 1 and less than -1? That’s not what Hintzman got.\n\nreplicate(20, abstraction(3,6,9,4))\n\n, , 1\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]  [,9]  [,10]\nproto_a -1.000  1.000  1.000  1.000 -1.000  1.000 -1.000 -1.000 1.000 -1.000\necho_a  -3.048  2.952  2.952  3.048 -3.048  3.048 -3.048 -2.952 2.952 -3.048\nproto_b  1.000  1.000  1.000 -1.000  1.000 -1.000  1.000 -1.000 1.000  1.000\necho_b   6.600  6.552  6.552 -6.600  6.600 -6.600  5.448 -6.552 5.400  5.448\nproto_c -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000  1.000 1.000  1.000\necho_c  -9.384 -9.384 -9.384  9.384 -9.384  9.384  8.616  9.384 8.616  8.616\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17] [,18] [,19]  [,20]\nproto_a -1.000 -1.000  1.000  1.000 -1.000 -1.000  1.000   1.0 1.000 -1.000\necho_a  -1.016  1.032  1.032  2.968  1.048 -3.000  1.016  -1.0 0.984 -0.984\nproto_b -1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000  -1.0 1.000 -1.000\necho_b   2.328 -4.072 -4.328  3.784 -5.816 -0.296 -1.816   0.2 1.544 -1.800\nproto_c -1.000  1.000  1.000 -1.000 -1.000  1.000 -1.000  -1.0 1.000 -1.000\necho_c  -5.128  1.256  5.256  2.744 -2.616  5.000 -2.872  -3.0 6.872 -2.872\n         [,21]  [,22]  [,23]\nproto_a  1.000  1.000  1.000\necho_a   1.000  3.032  3.032\nproto_b -1.000 -1.000 -1.000\necho_b   0.312 -4.216 -4.344\nproto_c -1.000  1.000  1.000\necho_c  -5.000  3.256  5.256\n\n, , 2\n\n         [,1]  [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]  [,9]  [,10]\nproto_a 1.000 1.000  1.000 -1.000 -1.000 -1.000  1.000 -1.000 1.000 -1.000\necho_a  3.072 3.072  2.928 -2.928 -3.072 -3.072  2.928 -2.928 3.072 -3.072\nproto_b 1.000 1.000 -1.000  1.000 -1.000  1.000  1.000  1.000 1.000  1.000\necho_b  6.576 6.576 -6.576  6.576 -6.576  5.424  5.424  6.576 6.576  5.424\nproto_c 1.000 1.000 -1.000  1.000 -1.000 -1.000 -1.000  1.000 1.000 -1.000\necho_c  9.408 9.408 -9.360  9.360 -9.408 -8.640 -8.592  9.360 9.408 -8.640\n         [,11] [,12]  [,13]  [,14]  [,15] [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.00 -1.000  1.000  1.000 -1.00  1.000  1.000 -1.000 -1.000\necho_a  -1.008 -0.96 -2.960  2.976  0.992 -0.96  0.944 -0.944 -1.024 -2.960\nproto_b  1.000  1.00  1.000  1.000  1.000 -1.00  1.000 -1.000 -1.000 -1.000\necho_b   3.936  2.32  2.320 -0.192  3.936  2.32  3.552 -1.552 -0.192  0.320\nproto_c  1.000  1.00  1.000 -1.000 -1.000  1.00 -1.000  1.000  1.000  1.000\necho_c  -0.752  5.12  5.104 -2.976 -0.736  5.12 -6.736  6.864 -3.008  4.976\n         [,21]  [,22]  [,23]\nproto_a -1.000  1.000  1.000\necho_a  -0.960  0.976 -0.976\nproto_b -1.000  1.000 -1.000\necho_b  -3.680  5.808 -3.808\nproto_c  1.000 -1.000  1.000\necho_c   4.736 -2.608  2.736\n\n, , 3\n\n           [,1]    [,2]    [,3]   [,4]   [,5]    [,6]    [,7]   [,8]    [,9]\nproto_a  -1.000  -1.000  -1.000 -1.000  1.000  -1.000  -1.000  1.000  -1.000\necho_a   -7.608  -7.608  -7.608  1.608  7.608  -7.608  -7.608  7.608  -7.608\nproto_b   1.000   1.000  -1.000  1.000  1.000  -1.000   1.000  1.000   1.000\necho_b    5.928   5.928  -6.072  6.072  6.072  -6.072   5.928  6.072   5.928\nproto_c  -1.000  -1.000  -1.000  1.000  1.000  -1.000  -1.000  1.000  -1.000\necho_c  -10.488 -10.488 -10.584  7.512 10.584 -10.584 -10.488 10.584 -10.488\n         [,10]  [,11]  [,12]  [,13]  [,14] [,15]  [,16]  [,17]  [,18]  [,19]\nproto_a  1.000 -1.000 -1.000 -1.000 -1.000 1.000  1.000  1.000  1.000 -1.000\necho_a   7.608 -4.584  2.536  0.536  0.488 4.584  1.464  1.560  3.512  0.536\nproto_b  1.000  1.000 -1.000 -1.000 -1.000 1.000 -1.000 -1.000 -1.000  1.000\necho_b   6.072  3.944 -3.976 -1.976 -2.008 6.056 -0.024 -1.960 -1.992  0.024\nproto_c  1.000 -1.000  1.000  1.000 -1.000 1.000 -1.000  1.000  1.000  1.000\necho_c  10.584 -7.480  3.480  2.472 -0.504 7.560 -1.464  4.472  2.520  2.488\n         [,20] [,21]  [,22]  [,23]\nproto_a  1.000 -1.00 -1.000 -1.000\necho_a   3.512 -0.44  1.560 -2.536\nproto_b  1.000  1.00  1.000  1.000\necho_b   2.008  2.04  2.040  1.976\nproto_c -1.000  1.00  1.000 -1.000\necho_c   2.552  3.48  4.504 -3.496\n\n, , 4\n\n          [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000 -1.000  1.000  1.000 1.000 -1.000 -1.000  1.000 -1.000 -1.000\necho_a   3.624 -3.624  3.624  3.624 2.376 -2.376 -3.624  2.472 -3.528 -3.528\nproto_b -1.000  1.000 -1.000 -1.000 1.000 -1.000  1.000 -1.000 -1.000 -1.000\necho_b  -6.600  6.600 -6.600 -6.600 6.552 -6.552  6.600 -5.448 -5.400 -5.400\nproto_c -1.000  1.000 -1.000 -1.000 1.000 -1.000  1.000  1.000  1.000  1.000\necho_c  -9.576  9.576 -9.576 -9.576 9.192 -9.192  9.576  8.424  8.808  8.808\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000  1.000  1.000  1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000\necho_a  -2.584  1.320 -1.464  3.192 -2.808 -3.336  3.208 -3.336 -2.952 -2.456\nproto_b  1.000 -1.000  1.000  1.000  1.000  1.000 -1.000  1.000  1.000  1.000\necho_b   3.576 -0.328  2.456 -0.216 -0.168  2.344 -2.216  2.344  1.960  3.448\nproto_c -1.000  1.000  1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000\necho_c  -6.552 -5.064  7.192 -3.192 -2.808  5.320 -3.320  5.320 -0.680 -8.552\n         [,21] [,22]  [,23]\nproto_a  1.000 1.000 -1.000\necho_a  -1.112 0.392  2.952\nproto_b  1.000 1.000 -1.000\necho_b   6.072 4.568 -1.960\nproto_c -1.000 1.000 -1.000\necho_c   1.448 9.192  0.680\n\n, , 5\n\n         [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\nproto_a 1.000  1.000  1.000 -1.000 -1.000 -1.000 1.000 -1.000 -1.000 -1.000\necho_a  3.000  3.000  3.000 -3.000 -3.000 -3.000 3.000 -3.000 -3.000 -3.000\nproto_b 1.000  1.000  1.000  1.000  1.000 -1.000 1.000  1.000  1.000  1.000\necho_b  6.072  5.928  5.928  5.928  5.928 -6.072 6.072  6.072  6.072  6.072\nproto_c 1.000 -1.000 -1.000 -1.000 -1.000 -1.000 1.000  1.000  1.000  1.000\necho_c  9.048 -8.952 -8.952 -8.952 -8.952 -9.048 9.048  9.048  9.048  9.048\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17] [,18]  [,19]  [,20]\nproto_a -1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000  1.00 -1.000  1.000\necho_a  -3.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000  3.00 -3.000  3.000\nproto_b  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000  1.00  1.000 -1.000\necho_b   1.992 -3.992  3.960  6.024  0.024  0.056 -1.960 -0.04  4.024 -4.024\nproto_c  1.000  1.000 -1.000  1.000  1.000  1.000  1.000 -1.00  1.000 -1.000\necho_c  -0.984  0.968 -4.968  3.048  3.000  7.000  4.984 -5.00  3.032 -3.032\n         [,21] [,22]  [,23]\nproto_a -1.000 -1.00  1.000\necho_a  -1.000 -1.00  3.000\nproto_b  1.000  1.00 -1.000\necho_b   4.056  0.04 -0.008\nproto_c  1.000  1.00  1.000\necho_c   7.032  5.00 -1.000\n\n, , 6\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9] [,10]\nproto_a -1.000 -1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.000 -1.000 1.000\necho_a  -2.928 -2.928 -3.072 -3.072  3.072 -2.928 -3.072  2.928 -3.072 3.072\nproto_b -1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000  1.000 1.000\necho_b  -6.000  6.000 -6.000  6.000 -6.000 -6.000 -6.000 -6.000  6.000 6.000\nproto_c  1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000 1.000\necho_c   8.976  8.976 -9.024 -9.024  9.024  8.976 -9.024 -8.976 -9.024 9.024\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000  1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000  1.000 -1.000\necho_a   0.992 -1.024 -2.992 -1.024 -2.992  0.992  0.960  2.976 -0.944 -0.928\nproto_b  1.000 -1.000  1.000  1.000  1.000  1.000 -1.000  1.000  1.000 -1.000\necho_b   0.000 -6.000  4.000  0.000  4.000  4.000 -2.000  4.000  4.000  2.000\nproto_c -1.000 -1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000\necho_c  -0.992 -3.008  0.976 -3.008  0.976 -0.992 -4.992 -2.976  6.992  8.992\n         [,21]  [,22]  [,23]\nproto_a -1.000 -1.000  1.000\necho_a  -2.928 -0.960  1.008\nproto_b -1.000 -1.000 -1.000\necho_b  -2.000  2.000 -4.000\nproto_c  1.000  1.000  1.000\necho_c   8.976  4.992  1.008\n\n, , 7\n\n          [,1]  [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000 1.000  1.000 -1.000  1.000 -1.000  1.000 -1.000  1.000  1.000\necho_a   2.688 3.456  3.312 -3.456  3.312 -3.456  2.688 -3.312  2.688  3.312\nproto_b -1.000 1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_b  -6.384 5.616  6.768 -5.616  6.768 -5.616 -6.384 -6.768 -6.384  6.768\nproto_c  1.000 1.000 -1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000 -1.000\necho_c   9.408 8.640 -9.360 -8.640 -9.360 -8.640  9.408  9.360  9.408 -9.360\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18] [,19]  [,20]\nproto_a -1.000  1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000 1.000  1.000\necho_a  -2.592  1.088 -1.040 -0.672 -0.864 -1.280 -2.736  3.040 1.168 -1.264\nproto_b  1.000  1.000  1.000  1.000  1.000 -1.000  1.000 -1.000 1.000 -1.000\necho_b   5.616  2.384  0.256  3.360  1.872 -3.872  3.744 -0.128 1.744 -4.000\nproto_c  1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000 1.000 -1.000\necho_c   2.592 -5.120 -5.008  8.736  0.864 -2.752  0.720  5.024 4.880 -0.752\n         [,21]  [,22]  [,23]\nproto_a  1.000 -1.000 -1.000\necho_a   2.784 -0.992 -1.152\nproto_b -1.000  1.000  1.000\necho_b  -4.128 -0.128 -1.872\nproto_c  1.000  1.000 -1.000\necho_c   5.280  0.992 -2.880\n\n, , 8\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000  1.000  1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000\necho_a  -3.120  3.120  3.120  3.120  2.880 -3.024 -3.024 -2.880 -2.976 -2.976\nproto_b -1.000  1.000  1.000  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000\necho_b  -6.096  6.096  6.096  6.096 -6.048  5.904  5.904  6.048 -5.952 -5.952\nproto_c  1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000 -1.000\necho_c   9.072 -9.072 -9.072 -9.072  9.024  8.976  8.976 -9.024 -8.928 -8.928\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_a   1.056 -3.008  3.056  3.008 -2.992 -0.976 -0.992 -0.944 -0.960 -1.040\nproto_b  1.000  1.000  1.000  1.000  1.000 -1.000  1.000  1.000  1.000  1.000\necho_b   4.032  1.952  2.064  2.016  1.968 -1.968  3.968  4.016  2.016 -0.048\nproto_c -1.000  1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000\necho_c  -3.040  3.008 -5.040  0.960  1.008 -4.976  2.976 -3.024 -3.008  5.008\n         [,21]  [,22]  [,23]\nproto_a -1.000  1.000  1.000\necho_a  -2.960  2.928  1.056\nproto_b  1.000 -1.000  1.000\necho_b   2.000 -0.048  4.032\nproto_c -1.000  1.000 -1.000\necho_c  -2.992  8.976 -3.040\n\n, , 9\n\n          [,1]   [,2]   [,3]   [,4]  [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000  1.000  1.000 -1.000 1.000  1.000  1.000 -1.000 -1.000  1.000\necho_a   3.120  3.120  2.976 -3.120 2.880  3.024  3.024 -2.880 -3.120  2.976\nproto_b -1.000 -1.000 -1.000  1.000 1.000  1.000  1.000 -1.000  1.000 -1.000\necho_b  -6.096 -6.096 -5.952  6.096 6.048  5.904  5.904 -6.048  6.096 -5.952\nproto_c -1.000 -1.000  1.000  1.000 1.000 -1.000 -1.000 -1.000  1.000  1.000\necho_c  -9.072 -9.072  8.928  9.072 9.024 -8.976 -8.976 -9.024  9.072  8.928\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19] [,20]\nproto_a  1.000  1.000 -1.000 -1.000  1.000  1.000  1.000  1.000  1.000 1.000\necho_a   0.992  2.976 -0.944 -0.896  1.024 -0.992  2.976  2.992  3.008 0.992\nproto_b  1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000  1.000 1.000\necho_b   3.968 -1.984 -0.048 -6.048 -2.016  1.984  5.952 -3.984 -0.032 0.000\nproto_c  1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000 -1.000 1.000\necho_c  -2.976  4.960 -6.992 -7.040 -1.024 -2.976 -2.976  4.944 -1.024 0.992\n         [,21]  [,22]  [,23]\nproto_a  1.000  1.000 -1.000\necho_a   1.040 -3.056 -1.056\nproto_b -1.000  1.000  1.000\necho_b   1.936  2.064  4.032\nproto_c -1.000  1.000  1.000\necho_c  -6.992  5.040  3.040\n\n, , 10\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9] [,10]\nproto_a -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000 1.000\necho_a  -2.928 -2.928  3.072 -3.072  3.072 -3.072 -3.072  3.072  2.928 2.928\nproto_b  1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000 1.000\necho_b   6.576  6.576 -5.424 -6.576  6.576 -6.576 -6.576 -5.424 -6.576 5.424\nproto_c -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000 1.000\necho_c  -9.360 -9.360 -8.640  9.408 -9.408  9.408  9.408 -8.640  9.360 8.592\n         [,11]  [,12] [,13]  [,14]  [,15]  [,16]  [,17]  [,18] [,19]  [,20]\nproto_a -1.000 -1.000 -1.00  1.000 -1.000  1.000  1.000  1.000 1.000 -1.000\necho_a  -2.992 -1.024 -2.96 -2.944 -0.944  1.008  0.976  1.040 2.944 -0.960\nproto_b  1.000  1.000  1.00  1.000  1.000 -1.000 -1.000  1.000 1.000 -1.000\necho_b   6.064  1.808  6.32  0.448  4.448 -1.936 -2.192  2.320 3.552  4.320\nproto_c -1.000  1.000 -1.00 -1.000 -1.000  1.000  1.000 -1.000 1.000 -1.000\necho_c  -1.360  2.880 -5.36 -6.976 -7.248 -0.880  3.120 -5.136 6.720 -5.248\n         [,21]  [,22]  [,23]\nproto_a  1.000 -1.000 -1.000\necho_a  -1.024 -1.008 -2.992\nproto_b -1.000  1.000 -1.000\necho_b  -2.192  3.936  0.064\nproto_c  1.000 -1.000 -1.000\necho_c   3.136  0.752 -0.976\n\n, , 11\n\n          [,1]   [,2]   [,3]  [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000  1.000  1.000 1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000\necho_a  -2.424  3.576  3.576 3.576  3.576 -3.576 -3.576  2.424 -3.576 -2.424\nproto_b  1.000 -1.000 -1.000 1.000 -1.000 -1.000 -1.000  1.000  1.000 -1.000\necho_b   5.928 -6.072 -6.072 5.928 -6.072 -5.928 -5.928  6.072  6.072 -6.072\nproto_c  1.000  1.000  1.000 1.000  1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_c   8.760  9.240  9.240 9.144  9.240 -9.144 -9.144 -8.856 -9.240  8.856\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18] [,19]  [,20]\nproto_a -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000 -1.000  1.00  1.000\necho_a  -3.064  2.808  0.552 -0.936 -2.936 -2.808  0.808  0.808  1.32  1.448\nproto_b  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000  1.000 -1.00  1.000\necho_b  -1.992 -3.976  6.056  3.992  1.992 -6.024 -1.976  2.024 -2.04 -0.056\nproto_c -1.000 -1.000 -1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.00  1.000\necho_c  -1.176 -2.776 -6.984  0.904  0.792  2.856 -2.920 -2.952  5.08  7.064\n         [,21]  [,22] [,23]\nproto_a -1.000 -1.000 1.000\necho_a   0.808  1.576 1.192\nproto_b  1.000 -1.000 1.000\necho_b   2.024 -0.072 1.976\nproto_c -1.000  1.000 1.000\necho_c  -2.952  9.064 3.048\n\n, , 12\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 -1.000  1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000\necho_a  -3.048 -2.952  3.048 -3.048 -2.952  3.048 -3.048  2.952 -3.048 -2.952\nproto_b -1.000  1.000  1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000\necho_b  -6.024  5.976  6.024 -6.024  5.976  6.024 -6.024 -5.976 -6.024  5.976\nproto_c  1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_c   9.000 -9.000 -9.000 -9.000 -9.000 -9.000 -9.000 -9.000 -9.000  9.000\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a  1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000\necho_a   2.968 -1.032 -1.016 -0.984 -1.048 -2.968 -3.000  3.000  1.016 -3.000\nproto_b -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000 -1.000\necho_b  -3.976 -4.008 -2.008  1.992 -6.008  3.976 -0.024  0.024  2.008 -0.024\nproto_c  1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000\necho_c  -1.000  3.000 -1.000 -3.000  7.000  1.000 -3.000 -3.000 -1.000 -5.000\n         [,21]  [,22]  [,23]\nproto_a -1.000  1.000 -1.000\necho_a  -1.032  0.968 -1.016\nproto_b -1.000 -1.000  1.000\necho_b  -4.008 -3.992 -2.008\nproto_c -1.000  1.000 -1.000\necho_c  -7.000  7.000 -5.000\n\n, , 13\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000  1.000  1.000  1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000\necho_a   3.384  3.384  3.384  3.384  2.616  2.616 -3.384 -2.616 -3.384 -3.384\nproto_b -1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000 -1.000  1.000  1.000\necho_b  -6.120 -6.120 -6.264 -6.264  5.880  5.880  6.264 -5.736  6.264  6.120\nproto_c -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000\necho_c  -8.952 -8.952  9.048  9.048 -9.048 -9.048 -9.048 -8.952 -9.048  8.952\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000  1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000  1.000 -1.000\necho_a  -3.256 -1.128 -1.256 -3.000 -3.256  3.384 -0.872 -1.128  1.128 -1.128\nproto_b  1.000  1.000  1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000\necho_b   4.152  2.056  4.024  0.216  4.168 -6.232 -1.928  2.056 -2.024  2.024\nproto_c  1.000 -1.000  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000  1.000\necho_c   4.968  0.984  4.968 -3.000  2.968  5.048 -0.984  0.984 -4.984  4.984\n         [,21]  [,22]  [,23]\nproto_a -1.000  1.000 -1.000\necho_a  -0.744  1.000  0.872\nproto_b -1.000  1.000  1.000\necho_b  -3.976 -0.104  1.992\nproto_c  1.000  1.000 -1.000\necho_c   5.032  5.000 -7.016\n\n, , 14\n\n          [,1]   [,2]   [,3]    [,4]   [,5]    [,6]    [,7]  [,8]   [,9]\nproto_a  1.000 -1.000 -1.000  -1.000  1.000  -1.000  -1.000 1.000  1.000\necho_a   1.056 -4.944 -4.944  -4.944  1.056  -4.944  -4.944 4.944  4.944\nproto_b  1.000 -1.000 -1.000   1.000  1.000   1.000   1.000 1.000 -1.000\necho_b   6.576 -5.424 -5.424   6.576  6.576   6.576   6.576 5.424 -6.576\nproto_c -1.000 -1.000 -1.000  -1.000 -1.000  -1.000  -1.000 1.000  1.000\necho_c  -8.736 -9.264 -9.264 -10.032 -8.736 -10.032 -10.032 9.264 10.032\n          [,10]  [,11] [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18] [,19]\nproto_a  -1.000  1.000 1.000  1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.00\necho_a   -4.944  1.216 0.080  1.488 -0.784 -0.352  1.216  0.512  0.080 -1.92\nproto_b   1.000 -1.000 1.000 -1.000  1.000 -1.000  1.000 -1.000  1.000 -1.00\necho_b    6.576 -4.064 3.680  0.448  3.936 -6.192  1.936 -4.448  1.680 -2.32\nproto_c  -1.000  1.000 1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000  1.00\necho_c  -10.032  1.472 4.528 -6.352  0.528  3.168  1.088  7.040  4.656  4.48\n         [,20]  [,21]  [,22]  [,23]\nproto_a -1.000 -1.000  1.000  1.000\necho_a  -4.512 -3.216 -2.080 -2.080\nproto_b  1.000 -1.000 -1.000  1.000\necho_b   0.448 -3.936  0.320 -1.680\nproto_c -1.000 -1.000 -1.000 -1.000\necho_c  -7.648 -1.392 -5.216 -5.088\n\n, , 15\n\n          [,1]  [,2]   [,3]   [,4]  [,5]  [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 1.000  1.000 -1.000 1.000 1.000  1.000  1.000 -1.000  1.000\necho_a  -3.024 3.120  2.976 -3.120 3.120 3.120  2.976  2.880 -2.880  3.024\nproto_b  1.000 1.000  1.000 -1.000 1.000 1.000  1.000 -1.000  1.000 -1.000\necho_b   5.904 6.096  5.952 -6.096 6.096 6.096  5.952 -6.048  6.048 -5.904\nproto_c -1.000 1.000 -1.000 -1.000 1.000 1.000 -1.000 -1.000  1.000  1.000\necho_c  -8.976 9.072 -8.928 -9.072 9.072 9.072 -8.928 -9.024  9.024  8.976\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000\necho_a   0.944 -1.040 -0.992  1.008  3.008  0.912 -3.024  0.944 -2.992  1.072\nproto_b -1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000  1.000  1.000\necho_b  -2.032 -4.016  5.952  0.016  5.984 -4.048 -4.016 -4.016 -3.984  0.080\nproto_c -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000\necho_c  -5.008 -1.040 -4.960  1.008 -4.928 -7.024  0.944 -3.024  4.944  9.008\n         [,21]  [,22]  [,23]\nproto_a  1.000 -1.000  1.000\necho_a   3.024 -1.024  2.960\nproto_b  1.000 -1.000 -1.000\necho_b   2.032 -2.016 -0.016\nproto_c -1.000 -1.000 -1.000\necho_c   1.040 -1.024 -4.976\n\n, , 16\n\n          [,1]  [,2]   [,3]   [,4]   [,5]  [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 1.000 -1.000 -1.000  1.000 1.000  1.000 -1.000  1.000 -1.000\necho_a  -2.952 3.048 -3.048 -3.048  2.952 3.048  3.048 -2.952  2.952 -3.048\nproto_b  1.000 1.000 -1.000 -1.000 -1.000 1.000  1.000  1.000 -1.000 -1.000\necho_b   5.976 6.024 -6.024 -6.024 -5.976 6.024  6.024  5.976 -5.976 -6.024\nproto_c  1.000 1.000 -1.000  1.000  1.000 1.000 -1.000  1.000  1.000  1.000\necho_c   9.000 9.000 -9.000  9.000  9.000 9.000 -9.000  9.000  9.000  9.000\n        [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a 1.000 -1.000  1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000\necho_a  3.000 -2.984 -0.984 -1.016 -0.968  1.000 -1.016  1.032 -3.000  0.968\nproto_b 1.000  1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000\necho_b  0.024  1.976  1.992 -2.008  3.992  0.008 -2.008  4.008 -0.024 -3.992\nproto_c 1.000 -1.000  1.000  1.000  1.000 -1.000  1.000 -1.000 -1.000  1.000\necho_c  3.000 -5.000  5.000  3.000  5.000 -3.000  3.000 -3.000 -1.000  3.000\n        [,21]  [,22]  [,23]\nproto_a 1.000  1.000  1.000\necho_a  3.032  2.968  0.984\nproto_b 1.000 -1.000 -1.000\necho_b  4.024 -3.976 -1.992\nproto_c 1.000 -1.000 -1.000\necho_c  1.000 -5.000 -5.000\n\n, , 17\n\n          [,1]  [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a  1.000 1.000  1.000  1.000  1.000  1.000 -1.000  1.000  1.000 -1.000\necho_a   3.048 2.952  2.952  2.952  3.048  3.048 -3.048  2.952  3.048 -3.048\nproto_b -1.000 1.000  1.000  1.000 -1.000 -1.000  1.000  1.000 -1.000  1.000\necho_b  -6.600 5.400  6.552  6.552 -6.600 -6.600  6.600  6.552 -5.448  5.448\nproto_c  1.000 1.000 -1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000\necho_c   9.384 8.616 -9.384 -9.384  9.384  9.384 -9.384 -9.384 -8.616  8.616\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16] [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.000  1.000 -1.000  1.000 -1.000 1.000 -1.000 -1.000 -1.000\necho_a   0.968  1.000  1.032  0.968  0.968 -3.000 2.968 -0.984 -3.048 -1.000\nproto_b -1.000  1.000 -1.000  1.000  1.000  1.000 1.000 -1.000  1.000  1.000\necho_b   3.800  0.568 -4.072  3.800  4.056  0.088 3.784 -2.440  5.576  0.072\nproto_c  1.000 -1.000 -1.000 -1.000 -1.000 -1.000 1.000  1.000  1.000  1.000\necho_c   2.744 -9.000  1.256  2.744 -1.256 -1.000 2.744  7.128  6.616 -1.000\n         [,21]  [,22]  [,23]\nproto_a  1.000  1.000 -1.000\necho_a  -1.048  3.016 -3.016\nproto_b  1.000 -1.000  1.000\necho_b   6.456 -1.704  2.472\nproto_c -1.000 -1.000 -1.000\necho_c  -7.384 -4.872 -7.128\n\n, , 18\n\n          [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 -1.000 -1.000  1.000 -1.000  1.000 1.000  1.000 -1.000  1.000\necho_a  -2.616 -3.384 -3.384  3.384 -3.384  3.384 2.616  3.384 -3.384  2.616\nproto_b -1.000  1.000  1.000 -1.000  1.000 -1.000 1.000 -1.000  1.000  1.000\necho_b  -5.880  6.264  6.120 -6.264  6.120 -6.120 5.880 -6.264  6.264  5.736\nproto_c -1.000  1.000 -1.000 -1.000 -1.000  1.000 1.000 -1.000  1.000 -1.000\necho_c  -9.048  9.048 -8.952 -9.048 -8.952  8.952 9.048 -9.048  9.048 -8.952\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]  [,20]\nproto_a -1.000 -1.000 -1.000  1.000 -1.000  1.000  1.000 -1.000  1.000 -1.000\necho_a  -2.872 -1.256  0.616 -0.872 -0.744  3.000  1.000 -1.384  1.128 -3.000\nproto_b -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000 -1.000\necho_b  -1.864  4.072  5.976 -2.008 -3.928 -0.248 -0.008  6.088 -2.072  0.136\nproto_c -1.000  1.000  1.000 -1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000\necho_c  -7.016  1.032  5.048 -9.016  0.968 -7.000  7.000  3.048 -1.016 -7.000\n         [,21]  [,22]  [,23]\nproto_a  1.000  1.000  1.000\necho_a   1.000 -1.128  3.384\nproto_b -1.000  1.000 -1.000\necho_b  -0.072  2.072 -6.216\nproto_c  1.000  1.000 -1.000\necho_c  -1.000  1.016 -3.048\n\n, , 19\n\n          [,1]  [,2]  [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\nproto_a -1.000 1.000 1.000 -1.000  1.000  1.000 -1.000 -1.000 -1.000  1.000\necho_a  -1.104 4.896 4.896 -4.896  4.992  4.992 -4.992 -4.896 -1.104  4.992\nproto_b  1.000 1.000 1.000 -1.000 -1.000 -1.000  1.000 -1.000  1.000 -1.000\necho_b   6.096 6.048 6.048 -6.048 -5.952 -5.952  5.952 -6.048  6.096 -5.952\nproto_c  1.000 1.000 1.000 -1.000  1.000  1.000 -1.000 -1.000  1.000  1.000\necho_c   8.400 9.696 9.696 -9.696  9.600  9.600 -9.600 -9.696  8.400  9.600\n         [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17] [,18]  [,19]  [,20]\nproto_a -1.000  1.000 -1.000  1.000  1.000 -1.000 -1.000 1.000  1.000  1.000\necho_a  -2.736  0.368 -1.632  1.664 -0.064 -1.664  1.904 4.928  2.800  0.336\nproto_b -1.000 -1.000 -1.000 -1.000 -1.000  1.000  1.000 1.000 -1.000  1.000\necho_b  -5.968 -2.032 -2.016 -1.984 -2.048  1.984  1.936 2.048 -2.032  1.968\nproto_c  1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.000 1.000 -1.000 -1.000\necho_c   0.304 -2.800 -3.232  3.200 -4.800 -3.200 -4.336 9.664 -0.368 -2.768\n         [,21]  [,22] [,23]\nproto_a  1.000  1.000 1.000\necho_a   0.352  0.368 1.616\nproto_b -1.000 -1.000 1.000\necho_b  -0.032 -2.032 4.016\nproto_c -1.000 -1.000 1.000\necho_c  -2.784 -2.800 3.248\n\n, , 20\n\n          [,1]    [,2]   [,3]    [,4]   [,5]   [,6]    [,7]    [,8]   [,9]\nproto_a  1.000   1.000 -1.000   1.000  1.000 -1.000  -1.000  -1.000 -1.000\necho_a   3.024   3.120 -3.120   3.120  2.880 -3.120  -2.880  -2.880 -3.120\nproto_b -1.000   1.000 -1.000   1.000 -1.000 -1.000   1.000   1.000 -1.000\necho_b  -4.032   7.968 -7.968   7.968 -7.920 -7.968   7.920   7.920 -7.968\nproto_c -1.000  -1.000  1.000  -1.000  1.000  1.000  -1.000  -1.000  1.000\necho_c  -7.728 -10.320 10.320 -10.320 10.272 10.320 -10.272 -10.272 10.320\n         [,10]  [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18]  [,19]\nproto_a -1.000 -1.000 -1.000  1.000 -1.000 -1.000 -1.000  1.000  1.000  1.000\necho_a  -2.976 -1.056 -0.944 -1.040 -1.024 -2.912  3.056  2.992  2.992  2.992\nproto_b -1.000 -1.000  1.000  1.000 -1.000  1.000  1.000  1.000 -1.000 -1.000\necho_b  -4.080 -1.520  3.072 -2.656 -3.792  5.488  3.104 -3.328 -1.760 -4.896\nproto_c -1.000  1.000 -1.000  1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000\necho_c  -7.680  7.008 -5.424  3.440 -0.128 -7.840 -5.456 -2.160 -0.592 -3.728\n         [,20]  [,21]  [,22]  [,23]\nproto_a -1.000  1.000 -1.000  1.000\necho_a  -2.992  3.008  0.912 -0.944\nproto_b  1.000 -1.000 -1.000  1.000\necho_b   4.896 -1.328 -5.504  4.640\nproto_c -1.000 -1.000  1.000 -1.000\necho_c   3.728 -2.592  7.856 -3.856\n\n\nUsing the replicate function allows me to simulate multiple participants easily.\nNext, I need to plot the prototypes against the echoes in a “histogram”. After several failed attempts at doing so with ggplot, I finally came to the conclusion that I had to use the basic R plotting package. An example of the code is below.\n\ngraph_in <- abstraction(3, 6, 9, 4)\npar = (mfrow = c (1, 2))\nbarplot(graph_in[5, ], main = \"proto_a\")\n\n\n\nbarplot(graph_in[6, ], main = \"echo_a\")\n\n\n\n\nThe code works, though it seems to indicate that the name features are a little different in the echo than in the probe. That might make sense, since the probe would not be highly similar to traces from a different category, which could affect the “purity” of the echo.\nI haven’t figured out a way to write a function to easily churn out the 6 graphs. Functions only return 1 output, and I’m not sure the basic plotting package in R allows for saving a graph containing multiple layers as a variable."
  },
  {
    "objectID": "posts/Hintzman4/index.html",
    "href": "posts/Hintzman4/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 4)",
    "section": "",
    "text": "Generalizing the code for more flexibility and brief thoughts on cosine similarity\nFirst, I need to clean up my code: name variables such that I don’t forget what they mean and define functions outside of other functions. Second, I need to generalize the code so that the number of categories is not hard coded .\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\nnum_of_pattern_feat <- 13\nnum_of_name_feat <- 10\nnum_of_trace_feat <- num_of_name_feat + num_of_pattern_feat\nfeat_values <- c(rep(1, 50), rep(-1, 50))\nnum_to_distort <- 4\n\n\n\n\n# Generating prototype function\n  \ngen_proto <- function() {\n   proto <- sample(feat_values, num_of_trace_feat, replace = TRUE)\n   return(proto)\n}\n\n# Generating prototype copies --> yields proto_copies_matrix\n\ngen_proto_copies <- function(input_vector) {\n  full_proto_copies_matrix <- c()\n  for(i in input_vector) {\n    proto_single <- gen_proto()\n    proto_copies <- matrix(rep(proto_single, i), nrow = i, ncol = num_of_trace_feat, byrow = TRUE)\n    full_proto_copies_matrix <- rbind(full_proto_copies_matrix, proto_copies)\n  }\n  return(full_proto_copies_matrix)\n}\n\n# Generating traces in secondary memory with distortion --> yields memory_matrix\n\ngen_secondary_memory <- function(input_vector, proto_copies_matrix) {\n  total_num_traces_in_memory <- sum(input_vector)\n  matrix_of_ones_for_name <- matrix(1, nrow = total_num_traces_in_memory, ncol = num_of_name_feat)\n  matrix_of_pattern_distort <- t(replicate(n = total_num_traces_in_memory, sample(c(rep(1, num_of_pattern_feat - num_to_distort), rep(-1, num_to_distort)), num_of_pattern_feat)))\n  distort_filter <- cbind(matrix_of_ones_for_name, matrix_of_pattern_distort)\n  distorted_memories <- proto_copies_matrix * distort_filter\nreturn(distorted_memories)\n}\n  \n# Extracting unique prototypes (one for each category) --> yields proto_matrix\n\nextract_unique_proto <- function(input_vector, proto_copies_matrix) {\n  full_unique_proto_matrix <- c()\n  row_counter <- 0\n  for(i in input_vector) {\n    proto_current <- proto_copies_matrix[row_counter + 1, ]\n    full_unique_proto_matrix <- rbind(full_unique_proto_matrix, proto_current)\n    row_counter <- row_counter + i\n  }\nreturn(full_unique_proto_matrix)\n}\n\n# Generating probes from prototypes --> yields probe_matrix\n\ngen_probes_from_proto <- function(proto_matrix) {\n  matrix_of_ones_for_name <- matrix(1, nrow = nrow(proto_matrix), ncol = num_of_name_feat)\n  matrix_of_zeroes_for_pattern <- matrix(0, nrow = nrow(proto_matrix), ncol = num_of_pattern_feat)\n  probe_filter <- cbind(matrix_of_ones_for_name, matrix_of_zeroes_for_pattern)\n  probe_unique_matrix <- proto_matrix * probe_filter\n  return(probe_unique_matrix)\n}\n\n# Echo activation function --> yields activations_matrix\ncalc_echo_activations <- function(probe_matrix, memory_matrix) {\nfull_probe_activations_matrix <- c()\n  for(probe in 1:nrow(probe_matrix)) {\n    all_activations_for_each_probe <- c()\n    for(memory in 1:nrow(memory_matrix)) {\n      num_of_relevant_features <- 0\n      similarity_temp <- 0\n      for(feat in 1:num_of_trace_feat) {\n        current_product <- probe_matrix[probe, feat] * memory_matrix[memory, feat]\n        similarity_temp <- similarity_temp + current_product\n          if(probe_matrix[probe, feat] != 0 & memory_matrix[memory, feat] != 0) {\n            num_of_relevant_features <- num_of_relevant_features + 1\n          }\n    }\n    trace_similarity <- similarity_temp/num_of_relevant_features\n    trace_activation <- trace_similarity ^ 3\n    all_activations_for_each_probe <- c(all_activations_for_each_probe, trace_activation)\n  }\nfull_probe_activations_matrix <- rbind(full_probe_activations_matrix, all_activations_for_each_probe)\n}\nreturn(full_probe_activations_matrix)\n}\n\n# Echo intensity function --> yields intensity_matrix\ncalc_echo_intensity <- function(activations_matrix) {\n  full_intensity_matrix <- c()\n  for(probe in 1:nrow(activations_matrix)) {\n    echo_intensity_for_probe <- sum(activations_matrix[probe, ])\n    full_intensity_matrix <- c(full_intensity_matrix, echo_intensity_for_probe) \n  }\n  return(full_intensity_matrix)\n}\n\n# Echo content function --> yields content_matrix\n\ncalc_echo_content <- function(activations_matrix, memory_matrix) {\n  full_echo_content_matrix <- c()\n  for(probe in 1:nrow(activations_matrix)) {\n    echo_content_for_each_probe <- c()\n    for(feat in 1:num_of_trace_feat){\n      content_temp <- 0\n        for(memory in 1:nrow(memory_matrix)) {\n          current_product <- activations_matrix[probe, memory] * memory_matrix[memory, feat]\n          content_temp <- content_temp + current_product\n        }\n      echo_content_for_each_probe <- c(echo_content_for_each_probe, content_temp)\n    }\n    full_echo_content_matrix <- rbind(full_echo_content_matrix, echo_content_for_each_probe)\n  }\n  return(full_echo_content_matrix)\n}\n\n# Calculating prototype-echo correlation --> yields correlation_matrix\n\ncalc_proto_echo_corr <- function(proto_matrix, content_matrix) {\n  full_correlation_matrix <- c()\n  for(proto in 1:nrow(proto_matrix)) {\n    correlation_current <- cor(proto_matrix[proto, ], content_matrix[proto, ])\n    full_correlation_matrix <- c(full_correlation_matrix, correlation_current)\n  }\n  return(full_correlation_matrix)\n}\n\n\nsimulate_name_as_probe_calc_corr <- function(input_vector) {\n  proto_copies_matrix <- gen_proto_copies(input_vector)\n  memory_matrix <- gen_secondary_memory(input_vector, proto_copies_matrix)\n  proto_matrix <- extract_unique_proto(input_vector, proto_copies_matrix)\n  probe_matrix <- gen_probes_from_proto(proto_matrix)\n  activations_matrix <- calc_echo_activations(probe_matrix, memory_matrix)\n  content_matrix <- calc_echo_content(activations_matrix, memory_matrix)\n  correlation_matrix <- calc_proto_echo_corr(proto_matrix, content_matrix)\n  return(correlation_matrix)\n}\n\n\nsimulate_name_as_probe_calc_corr(c(3, 6, 9))\n\n[1] 0.8021890 0.8395375 0.8793400\n\n\nFinally, after much troubleshooting, it finally seems to work as intended. Time to simulate 20 subjects as Hintzman did and compare the mean prototype-echo correlations.\n\nmy_corr <- t(replicate(20, simulate_name_as_probe_calc_corr(c(3,6,9))))\ncorr_means <- c(mean(my_corr[ ,1]), mean(my_corr[ , 2]), mean(my_corr[ , 3]))\ncorr_sds <- c(sd(my_corr[ ,1]), sd(my_corr[ , 2]), sd(my_corr[ , 3]))\ncorr_means\n\n[1] 0.7777373 0.8415486 0.8586056\n\ncorr_sds\n\n[1] 0.04042487 0.02404416 0.01930278\n\n\nInteresting that my mean prototype-echo correlations are higher than Hintzman’s, while my standard deviations are smaller. I’ll need to verify these values with Matt.\nAs an aside, we had a conversation about cosine similarity and how it doesn’t capture differences in vector length, only in the difference in degree.\n\nWhat implications are there for memory? Hintzman conceptualizes memories as -1s or 1s, similar to how computers are able to code complex information into 0s and 1s. Thus, could vector length be already a built-in consideration, as one of the many features?\nGiven the bar chart diagram Matt drew, where the two bar charts are identical in their positive and negative direction figuration, but differ in the length of the bar, that makes me recall an explanation of correlation similarity that I learnt in class. Do both cosine and correlation similarity not capture vector length?"
  },
  {
    "objectID": "posts/Hintzman5/index.html",
    "href": "posts/Hintzman5/index.html",
    "title": "Reproducing Hintzman’s MINERVA (Part 5)",
    "section": "",
    "text": "Attempting to replicate the schema-abstraction task\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\n\n# Initial parameters to set \n\nnum_of_pattern_feat <- 13\nnum_of_name_feat <- 10\nnum_of_trace_feat <- num_of_name_feat + num_of_pattern_feat\nfeat_values <- c(rep(1, 50), rep(-1, 50))\nnum_to_distort <- 4\n\n# Generating prototype function\n  \ngen_proto <- function() {\n   proto <- sample(feat_values, num_of_trace_feat, replace = TRUE)\n   return(proto)\n}\n\n# Generating prototype copies --> yields proto_copies_matrix\n\ngen_proto_copies <- function(input_vector) {\n  full_proto_copies_matrix <- c()\n  for(i in input_vector) {\n    proto_single <- gen_proto()\n    proto_copies <- matrix(rep(proto_single, i), nrow = i, ncol = num_of_trace_feat, byrow = TRUE)\n    full_proto_copies_matrix <- rbind(full_proto_copies_matrix, proto_copies)\n  }\n  return(full_proto_copies_matrix)\n}\n\n# Generating traces in secondary memory with distortion --> yields memory_matrix\n\ngen_secondary_memory <- function(input_vector, proto_copies_matrix) {\n  total_num_traces_in_memory <- sum(input_vector)\n  matrix_of_ones_for_name <- matrix(1, nrow = total_num_traces_in_memory, ncol = num_of_name_feat)\n  matrix_of_pattern_distort <- t(replicate(n = total_num_traces_in_memory, sample(c(rep(1, num_of_pattern_feat - num_to_distort), rep(-1, num_to_distort)), num_of_pattern_feat)))\n  distort_filter <- cbind(matrix_of_ones_for_name, matrix_of_pattern_distort)\n  distorted_memories <- proto_copies_matrix * distort_filter\nreturn(distorted_memories)\n}\n  \n# Extracting unique prototypes (one for each category) --> yields proto_matrix\n\nextract_unique_proto <- function(input_vector, proto_copies_matrix) {\n  full_unique_proto_matrix <- c()\n  row_counter <- 0\n  for(i in input_vector) {\n    proto_current <- proto_copies_matrix[row_counter + 1, ]\n    full_unique_proto_matrix <- rbind(full_unique_proto_matrix, proto_current)\n    row_counter <- row_counter + i\n  }\nreturn(full_unique_proto_matrix)\n}\n\n# Generating probes from prototypes --> yields probe_matrix\n\ngen_probes_from_proto <- function(proto_matrix) {\n  matrix_of_ones_for_name <- matrix(1, nrow = nrow(proto_matrix), ncol = num_of_name_feat)\n  matrix_of_zeroes_for_pattern <- matrix(0, nrow = nrow(proto_matrix), ncol = num_of_pattern_feat)\n  probe_filter <- cbind(matrix_of_ones_for_name, matrix_of_zeroes_for_pattern)\n  probe_unique_matrix <- proto_matrix * probe_filter\n  return(probe_unique_matrix)\n}\n\n# Echo activation function --> yields activations_matrix\ncalc_echo_activations <- function(probe_matrix, memory_matrix) {\nfull_probe_activations_matrix <- c()\n  for(probe in 1:nrow(probe_matrix)) {\n    all_activations_for_each_probe <- c()\n    for(memory in 1:nrow(memory_matrix)) {\n      num_of_relevant_features <- 0\n      similarity_temp <- 0\n      for(feat in 1:num_of_trace_feat) {\n        current_product <- probe_matrix[probe, feat] * memory_matrix[memory, feat]\n        similarity_temp <- similarity_temp + current_product\n          if(probe_matrix[probe, feat] != 0 & memory_matrix[memory, feat] != 0) {\n            num_of_relevant_features <- num_of_relevant_features + 1\n          }\n    }\n    trace_similarity <- similarity_temp/num_of_relevant_features\n    trace_activation <- trace_similarity ^ 3\n    all_activations_for_each_probe <- c(all_activations_for_each_probe, trace_activation)\n  }\nfull_probe_activations_matrix <- rbind(full_probe_activations_matrix, all_activations_for_each_probe)\n}\nreturn(full_probe_activations_matrix)\n}\n\n# Echo intensity function --> yields intensity_matrix\ncalc_echo_intensity <- function(activations_matrix) {\n  full_intensity_matrix <- c()\n  for(probe in 1:nrow(activations_matrix)) {\n    echo_intensity_for_probe <- sum(activations_matrix[probe, ])\n    full_intensity_matrix <- c(full_intensity_matrix, echo_intensity_for_probe) \n  }\n  return(full_intensity_matrix)\n}\n\n# Echo content function --> yields content_matrix\n\ncalc_echo_content <- function(activations_matrix, memory_matrix) {\n  full_echo_content_matrix <- c()\n  for(probe in 1:nrow(activations_matrix)) {\n    echo_content_for_each_probe <- c()\n    for(feat in 1:num_of_trace_feat){\n      content_temp <- 0\n        for(memory in 1:nrow(memory_matrix)) {\n          current_product <- activations_matrix[probe, memory] * memory_matrix[memory, feat]\n          content_temp <- content_temp + current_product\n        }\n      echo_content_for_each_probe <- c(echo_content_for_each_probe, content_temp)\n    }\n    full_echo_content_matrix <- rbind(full_echo_content_matrix, echo_content_for_each_probe)\n  }\n  return(full_echo_content_matrix)\n}\n\n# Calculating prototype-echo correlation --> yields correlation_matrix\n\ncalc_proto_echo_corr <- function(proto_matrix, content_matrix) {\n  full_correlation_matrix <- c()\n  for(proto in 1:nrow(proto_matrix)) {\n    correlation_current <- cor(proto_matrix[proto, ], content_matrix[proto, ])\n    full_correlation_matrix <- c(full_correlation_matrix, correlation_current)\n  }\n  return(full_correlation_matrix)\n}\n\n\n# Makes all name features in secondary memory empty --> yields nameless_memory_matrix\nremove_name_feat_from_memory <- function(memory_matrix) {\n  zero_matrix <- matrix(0, nrow = nrow(memory_matrix), ncol = num_of_name_feat)\n  one_matrix <- matrix(1, nrow = nrow(memory_matrix), ncol = num_of_pattern_feat)\n  remove_name_filter <- cbind(zero_matrix, one_matrix)\n  memory_matrix_without_name <- memory_matrix * remove_name_filter\n  return(memory_matrix_without_name)\n}\n\n# Generates old exemplars already in secondary memory (probe a)\nextract_old_exemplars <- function(input_vector, nameless_memory_matrix) {\n  full_old_exemp_matrix <- c()\n  row_counter <- 0\n  for(i in input_vector) {\n    old_exemp_current <- nameless_memory_matrix[row_counter + 1, ]\n    full_old_exemp_matrix <- rbind(full_old_exemp_matrix, old_exemp_current)\n    row_counter <- row_counter + i\n  }\n  return(full_old_exemp_matrix)\n}\n\n# Extracts prototype matrix with empty name features (probe b) --> yields nameless_proto_matrix\n\nextract_unique_proto_without_names <- function(input_vector, proto_copies_matrix) {\n  \n  unique_protos <- extract_unique_proto(input_vector, proto_copies_matrix)\n  zero_matrix <- matrix(0, nrow = nrow(unique_protos), ncol = num_of_name_feat)\n  one_matrix <- matrix(1, nrow = nrow(unique_protos), ncol = num_of_pattern_feat)\n  remove_name_filter <- cbind(zero_matrix, one_matrix)\n  protos_without_name <- unique_protos * remove_name_filter\n  return(protos_without_name)\n}\n\n# Generates new low-distorted exemplars (probe c)\ngen_new_low_distort_exemp <- function(nameless_proto_matrix) {\n  \n  vector_of_ones_for_name <- rep(1, num_of_name_feat)\n  vector_of_pattern_distort <- sample(c(rep(1, num_of_pattern_feat- 2), rep(-1, 2)), num_of_pattern_feat)\n  distort_filter <- c(vector_of_ones_for_name, vector_of_pattern_distort)\n  \n  full_new_low_distort_matrix <- c()\n  for(proto in 1:nrow(nameless_proto_matrix)) {\n    current_new_low_distort <- nameless_proto_matrix[proto, ] * distort_filter\n    full_new_low_distort_matrix <- rbind(full_new_low_distort_matrix, current_new_low_distort)\n  }\nreturn(full_new_low_distort_matrix)\n  }\n\n# Generates new high-distorted exemplars (probe d)\ngen_new_high_distort_exemp <- function(nameless_proto_matrix) {\n  \n  vector_of_ones_for_name <- rep(1, num_of_name_feat)\n  vector_of_pattern_distort <- sample(c(rep(1, num_of_pattern_feat- 4), rep(-1, 4)), num_of_pattern_feat)\n  distort_filter <- c(vector_of_ones_for_name, vector_of_pattern_distort)\n  \n  full_new_high_distort_matrix <- c()\n  for(proto in 1:nrow(nameless_proto_matrix)) {\n    current_new_high_distort <- nameless_proto_matrix[proto, ] * distort_filter\n    full_new_high_distort_matrix <- rbind(full_new_high_distort_matrix, current_new_high_distort)\n  }\nreturn(full_new_high_distort_matrix)\n  }\n\n# Generates random pattern (probe e)\ngen_random_patterns <- function(nameless_proto_matrix) {\n  full_random_pattern_matrix <- c()\n  for(proto in 1:nrow(nameless_proto_matrix)) {\n    name_vector <- nameless_proto_matrix[proto, 1:num_of_name_feat]\n    random_pattern_feat_vector <- sample(feat_values, num_of_pattern_feat, replace = TRUE)\n    current_random_vector <- c(name_vector, random_pattern_feat_vector)\n    full_random_pattern_matrix <- rbind(full_random_pattern_matrix, current_random_vector)\n  }\n  return(full_random_pattern_matrix)\n}\n\n# Applies forgetting to traces in secondary memory\nforgetting_cycle <- function(nameless_memory_matrix, f_value) {\n  memory_matrix_post_forgetting <- c()\n  for(trace in 1:nrow(nameless_memory_matrix)) {\n    forget_filter <- rbinom(num_of_trace_feat, 1, 1 - f_value)\n    current_forgotten_memory <- nameless_memory_matrix[trace, ] * forget_filter\n    memory_matrix_post_forgetting <- rbind(memory_matrix_post_forgetting, current_forgotten_memory)\n  }\n  return(memory_matrix_post_forgetting)\n}\n\n# Extracts only name features from prototypes --> yields category_names\nextract_category_names <- function(input_vector, proto_copies_matrix) {\n  unique_protos <- extract_unique_proto(input_vector, proto_copies_matrix)\n  cat_names <- unique_protos[ , 1:num_of_name_feat]\n  return(cat_names)\n}\n\n# Extracts only name features from echo content --> yields echo_names\nextract_echo_content_name_feat <- function(content_matrix) {\n  echo_content_name <- content_matrix[ , 1:num_of_name_feat]\n}\n\n# Computes category name-echo content name correlation for each probe and assigns echo to category --> yields category_assign_vector\nassign_category_for_each_echo <- function(category_names, echo_names) {\ncategory_for_each_echo <- c()\nfor(e_name in 1:nrow(echo_names)) {\n  cat_echo_name_cor <- c()\n  for(c_name in 1:nrow(category_names)) {\n    current_cor <- cor(echo_names[e_name, ], category_names[c_name, ])\n    cat_echo_name_cor <- c(cat_echo_name_cor, current_cor)\n  }\n  if(max(cat_echo_name_cor) < 0) {\n   current_answer <- -9999\n  } else {\n    current_answer_temp <- which(cat_echo_name_cor == max(cat_echo_name_cor))\n    if(length(current_answer_temp) > 1) {\n    current_answer <- sample(current_answer_temp, 1)\n    } else {\n    current_answer <- current_answer_temp\n    }\n  }\n  category_for_each_echo <- c(category_for_each_echo, current_answer)\n}\nreturn(category_for_each_echo)\n}\n\ncheck_assign_accuracy <- function(category_assign_vector, input_vector, num_types_of_probe){\n  accuracy_vector <- category_assign_vector == rep(1:length(input_vector), num_types_of_probe)\n  return(accuracy_vector)\n  }\n\n\nschema_abstraction <- function(input_vector, num_types_of_probe) {\n  proto_copies_matrix <- gen_proto_copies(input_vector)\n  secondary_memory <- gen_secondary_memory(input_vector, proto_copies_matrix)\n  \n  nameless_memory_matrix <- remove_name_feat_from_memory(secondary_memory)\n  \n  old_exemplars <- extract_old_exemplars(input_vector, nameless_memory_matrix)\n  nameless_proto_matrix <- extract_unique_proto_without_names(input_vector, proto_copies_matrix)\n  new_low_distorts <- gen_new_low_distort_exemp(nameless_proto_matrix)\n  new_high_distorts <- gen_new_high_distort_exemp(nameless_proto_matrix)\n  random_patterns <- gen_random_patterns(nameless_proto_matrix)\n  \n  diverse_probes <- rbind(old_exemplars, nameless_proto_matrix, new_low_distorts, new_high_distorts, random_patterns)\n  activations_matrix <- calc_echo_activations(diverse_probes, secondary_memory)\n  content_matrix <- calc_echo_content(activations_matrix, secondary_memory)\n  \n  category_names <- extract_category_names(input_vector, proto_copies_matrix)\n  echo_names <- extract_echo_content_name_feat(content_matrix)\n  category_vector <- assign_category_for_each_echo(category_names, echo_names)\n  accuracy_vector <- check_assign_accuracy(category_vector, input_vector, num_types_of_probe)\n  return(accuracy_vector)\n}\n\nsimulate_schema_abstraction <- function(input_vector, num_types_of_probe, num_of_simulations, column_names) {\n  results_matrix <- t(replicate(num_of_simulations, schema_abstraction(input_vector, num_types_of_probe)))\n  results_df <- data.frame(results_matrix)\n  colnames(results_df) <- column_names\n  return(results_df)\n}\n\n\n# Test simulating 10 subjects\n\ndf_col_names <- c(\"old_exemp_1\", \"old_exemp_2\", \"old_exemp_3\", \"proto_1\", \"proto_2\", \"proto_3\", \"low_dist_1\", \"low_dist_2\", \"low_dist_3\", \"high_dist_1\", \"high_dist_2\", \"high_dist_3\", \"random_1\", \"random_2\", \"random_3\")\nhintz_input <- c(3, 6, 9)\nschema_sim <- simulate_schema_abstraction(hintz_input, 5, 10, df_col_names)\n\nThings I’m unsure about:\n\nThe probes contain empty name features, right?\nThe forgetting affects all features, including name features, right?\nDoes it matter which old exemplar I choose? (I’ve just chosen the first of each category)\n\nThere’s something wrong with the code, especially when the number of simulations gets too high (e.g. 100). It seems the problem lies with the assign_category_for_each_echo function; there could be a function that is occasionally turning up an NA value?"
  },
  {
    "objectID": "posts/Hintzman1988a/index.html",
    "href": "posts/Hintzman1988a/index.html",
    "title": "Hintzman’s (1988) MINERVA (Part 1)",
    "section": "",
    "text": "Moving on to Hintzman (1988)\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations_3 <- function(probe, mem) {\n  \n  as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))^3)\n}\n\n# Item generation function (borrowed from Matt)\n\ngenerate_item <- function(item_size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = item_size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# Item matrix (original, before applying learning rate) function\n\ngen_item_matrix <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3)) {\n  item_matrix <- t(replicate(n = matrix_size, generate_item(item_size = item_size, prob = prob)))\n  return(item_matrix)\n}\n\n# Form probe matrix i.e. item_matrix + 4 more random items\n\ngen_probes <- function(item_matrix, prob = c(1/3, 1/3, 1/3), max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  random_items <- t(replicate(n = num_of_traces_per_freq, generate_item(item_size = ncol(item_matrix), prob = prob)))\n  probe_matrix <- rbind(random_items, item_matrix)\n  return(probe_matrix)\n}\n\n# Form secondary memory -- create encoded matrix (i.e. apply learning rate) and input varying frequencies of items\n\ngen_secondary_mem <- function(item_matrix, l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n \n  freq_multiplier <- c()\n  for (i in 1:max_num_of_copies) {\n    current_multiplier <- rep(i, num_of_traces_per_freq)\n    freq_multiplier <- c(freq_multiplier, current_multiplier)\n  }\n  secondary_memory <- c()\n  for(j in 1:length(freq_multiplier)) {\n    current_rows <- matrix(rep(item_matrix[j, ], freq_multiplier[j]), nrow = freq_multiplier[j], ncol = ncol(item_matrix), byrow = TRUE)\n    secondary_memory <- rbind(secondary_memory, current_rows)\n  }\n   learning_matrix <- t(replicate(n = nrow(secondary_memory), sample(c(0,1), size = ncol(secondary_memory), prob = c(1 - l_value, l_value), replace = TRUE)))\n  encoded_memory <- secondary_memory * learning_matrix\n return(encoded_memory)\n}\n\n\n\n# Calculate activations for multiple probes\n\ncalc_activs_for_mult_probes <- function(probe_matrix, secondary_memory) {\n  activations_matrix <- c()\n  for(i in 1:nrow(probe_matrix)) {\n    current_activs <- get_activations_3(probe_matrix[i, ], secondary_memory)\n    activations_matrix <- rbind(activations_matrix, current_activs)\n  }\n  return(activations_matrix)\n}\n\n# Convert activations matrix to transformed intensity matrix ready for plotting\n\nconvert_to_intensity_mat <- function(activations_matrix, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  intensity_vector <- rowSums(activations_matrix)\n  intensity_matrix <- matrix(intensity_vector, nrow = max_num_of_copies + 1, ncol = num_of_traces_per_freq, byrow = TRUE)\n  return(intensity_matrix)\n}\n\n\n# Overall simulation function\n\nsim_intensity_once <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  secondary_memory <- gen_secondary_mem(item_matrix, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  activations_matrix <- calc_activs_for_mult_probes(probe_matrix, secondary_memory)\n  intensity_matrix <- convert_to_intensity_mat(activations_matrix, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  return(intensity_matrix)\n}\n\nsim_intensity_multiple <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  raw_intensity_matrix <- c()\n  for(i in 1:n_of_sim) {\n    temp_intensity <- sim_intensity_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n    raw_intensity_matrix <- cbind(raw_intensity_matrix, temp_intensity)\n  }\n  row_names <- as.data.frame(0:max_num_of_copies)\n  names(row_names) <- \"Frequency\"\n  intensity_df <- bind_cols(row_names, data.frame(raw_intensity_matrix)) %>%\n      pivot_longer(!Frequency, names_to = \"Drop\", values_to = \"Intensity\") %>% select(\"Frequency\", \"Intensity\")\n  return(intensity_df)\n}\n\n\ndf_intensity <- sim_intensity_multiple(1000)\nggplot(df_intensity, aes(x = Intensity, color = factor(Frequency))) + geom_density(show.legend = TRUE) + xlim(-1, 2)\n\nWarning: Removed 4 rows containing non-finite values (stat_density).\n\n\n\n\n\nFinally, I managed to reproduce the graph. I had gotten the general shape earlier (see below) but the mean values were too small and the graphs weren’t spread out enough. It turns out that I was misunderstanding the L value. The L value affects sampling rate (i.e. how many features are selected to be copied versus nullified), not trace quality (i.e. the actual number of features in memory that are different from others). Thus, I had assumed that all copies of the same original item in secondary memory would i) have the same same number of features learnt, ii) have the exact same features. I was wrong on both counts.\nI fixed this by creating the copies first before applying the learning rate, instead of the other way around. This way, the learning rate applies noise, so even among copies of the same item, there is variation in the number of features changes. This is due to the fact that, for some traces, feature values that were originally 0 may be selected to be nullified (resulting in essentially no change), whereas for other traces, feature values that were originally 1 or -1 may be selected to be nullified (resulting in a meaningful change)."
  },
  {
    "objectID": "posts/Hintzman1988b/index.html",
    "href": "posts/Hintzman1988b/index.html",
    "title": "Hintzman’s (1988) MINERVA (Part 2)",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nset.seed(30)\n\n# Activation function (borrowed from Matt) for a single probe (i.e. a probe vector)\n\nget_activations_3 <- function(probe, mem) {\n  \n  as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))^3)\n}\n\n# Item generation function (borrowed from Matt)\n\ngenerate_item <- function(item_size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = item_size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# Item matrix (original, before applying learning rate) function\n\ngen_item_matrix <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3)) {\n  item_matrix <- t(replicate(n = matrix_size, generate_item(item_size = item_size, prob = prob)))\n  return(item_matrix)\n}\n\n# Form secondary memory -- create encoded matrix (i.e. apply learning rate) and input varying frequencies of items\n\ngen_secondary_mem <- function(item_matrix, l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  learning_matrix <- replicate(n = ncol(item_matrix), sample(c(0,1), size = nrow(item_matrix), prob = c(1 - l_value, l_value), replace = TRUE))\n  encoded_matrix <- item_matrix * learning_matrix\n  \n  freq_multiplier <- c()\n  for (i in 1:max_num_of_copies) {\n    current_multiplier <- rep(i, num_of_traces_per_freq)\n    freq_multiplier <- c(freq_multiplier, current_multiplier)\n  }\n  secondary_memory <- c()\n  for(i in 1:nrow(encoded_matrix)) {\n    current_rows <- matrix(encoded_matrix[i , ], nrow = freq_multiplier[i], ncol = ncol(encoded_matrix), byrow = TRUE)\n    secondary_memory <- rbind(secondary_memory, current_rows)\n  }\n return(secondary_memory)\n}\n\n# Form probe matrix i.e. item_matrix + 4 more random items\n\ngen_probes <- function(item_matrix, prob = c(1/3, 1/3, 1/3), max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  random_items <- t(replicate(n = num_of_traces_per_freq, generate_item(item_size = ncol(item_matrix), prob = prob)))\n  probe_matrix <- rbind(random_items, item_matrix)\n  return(probe_matrix)\n}\n\n# Calculate activations for multiple probes\n\ncalc_activs_for_mult_probes <- function(probe_matrix, secondary_memory) {\n  activations_matrix <- c()\n  for(i in 1:nrow(probe_matrix)) {\n    current_activs <- get_activations_3(probe_matrix[i, ], secondary_memory)\n    activations_matrix <- rbind(activations_matrix, current_activs)\n  }\n  return(activations_matrix)\n}\n\n# Convert activations matrix to transformed intensity matrix ready for plotting\n\nconvert_to_intensity_mat <- function(activations_matrix, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  intensity_vector <- rowSums(activations_matrix)\n  intensity_matrix <- matrix(intensity_vector, nrow = max_num_of_copies + 1, ncol = num_of_traces_per_freq, byrow = TRUE)\n  return(intensity_matrix)\n}\n\n\n# Overall simulation function\n\nsim_intensity_once <- function(matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  secondary_memory <- gen_secondary_mem(item_matrix, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  activations_matrix <- calc_activs_for_mult_probes(probe_matrix, secondary_memory)\n  intensity_matrix <- convert_to_intensity_mat(activations_matrix, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  return(intensity_matrix)\n}\n\nsim_intensity_multiple <- function(n_of_sim, matrix_size = 20, item_size = 20, prob = c(1/3, 1/3, 1/3), l_value = .5, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  raw_intensity_matrix <- c()\n  for(i in 1:n_of_sim) {\n    temp_intensity <- sim_intensity_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value = l_value, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n    raw_intensity_matrix <- cbind(raw_intensity_matrix, temp_intensity)\n  }\n  row_names <- as.data.frame(0:max_num_of_copies)\n  names(row_names) <- \"Frequency\"\n  intensity_df <- bind_cols(row_names, data.frame(raw_intensity_matrix)) %>%\n      pivot_longer(!Frequency, names_to = \"Drop\", values_to = \"Intensity\") %>% select(\"Frequency\", \"Intensity\")\n  return(intensity_df)\n}\n\n\ndf_intensity <- sim_intensity_multiple(1000)\nggplot(df_intensity, aes(x = Intensity, color = factor(Frequency))) + geom_density(show.legend = TRUE) + xlim(-1, 2)\n\nWarning: Removed 255 rows containing non-finite values (stat_density).\n\n\n\n\n\n\n\n\n\nintensity_df <- sim_intensity_multiple(250, matrix_size = 16, l_value = 0.8, max_num_of_copies = 4)\ndiscrim_intensity_df <- intensity_df %>% mutate(freq_judgement = case_when(\n  Intensity < 0.17 ~ 0,\n  Intensity >= 0.17 & Intensity < 0.67 ~ 1,\n  Intensity >= 0.67 & Intensity < 1.33 ~ 2,\n  Intensity >= 1.33 & Intensity < 2 ~ 3,\n  Intensity >= 2 ~ 4\n))\n\nfreq_judgment_df <- data.frame(table(factor(discrim_intensity_df$Frequency, levels = 0:4), factor(discrim_intensity_df$freq_judgement, levels = 0:4))) %>% mutate(Freq = Freq/1000) %>% rename(Real_freq = Var1, Freq_judg = Var2, Proportion_of_resp = Freq)\n\n\nggplot(freq_judgment_df, aes(x = Freq_judg, y = Proportion_of_resp, group = Real_freq, color = Real_freq)) + geom_path() + geom_point()\n\n\n\n\nI’ve reproduced the rough shape of the graph, but the values are lower than in Hintzman’s (1988) graph. It is also strange that the proportion of correct responses when frequency = 3 is lower than that for frequency = 4. The general trend of lower peaks as frequency_judgment increases is not reflected in my graph.\n\n\n\n\n\n\n# Initial parameters\n\nnum_feat_shallow <- 10\nnum_feat_deep <- 15\nnum_of_traces_per_freq <- 4\nmax_num_of_copies <- 5\n\n# Creating the appropriate number of copies for each item\n\ngen_item_mat_copies <- function(item_matrix) {\n  \n  freq_multiplier <- c()\n  for (i in 1:max_num_of_copies) {\n    current_multiplier <- rep(i, num_of_traces_per_freq)\n    freq_multiplier <- c(freq_multiplier, current_multiplier)\n  }\n  \n  item_mat_copies <- c()\n  for(i in 1:nrow(item_matrix)) {\n    current_rows <- matrix(item_matrix[i , ], nrow = freq_multiplier[i], ncol = ncol(item_matrix), byrow = TRUE)\n    item_mat_copies <- rbind(item_mat_copies, current_rows)\n  }\n return(item_mat_copies)\n}\n\n# Functions for shallow learning filter and deep learning filter\n\ngen_shallow_learning_filter <- function(l_value1 = 0.6, l_value2 = 0) {\n  matrix_shallow_part_1 <- sample(c(0,1), size = num_feat_shallow, prob = c(1 - l_value1, l_value1), replace = TRUE)\n  matrix_shallow_part_2 <- sample(c(0,1), size = num_feat_deep, prob = c(1 - l_value2, l_value2), replace = TRUE)\n  matrix_shallow <- c(matrix_shallow_part_1, matrix_shallow_part_2)\n  return(matrix_shallow)\n}\n\ngen_deep_learning_filter <- function(l_value1 = 0.6, l_value2 = 0) {\n  matrix_deep_part_1 <- sample(c(0,1), size = num_feat_shallow, prob = c(1 - l_value2, l_value2), replace = TRUE)\n  matrix_deep_part_2 <- sample(c(0,1), size = num_feat_deep, prob = c(1 - l_value1, l_value1), replace = TRUE)\n  matrix_deep <- c(matrix_deep_part_1, matrix_deep_part_2)\n  return(matrix_deep)\n}\n\n# Generating secondary memory - putting item copies matrix through learning filters\n\ngen_orienting_secondary_mem <- function(item_copies_matrix, l_value1 = 0.6, l_value2 = 0) {\n  \norienting_sec_mem <- c()\nfor(i in 1:nrow(item_copies_matrix)) {\n  if(i %% 2 == 0) {\n    deep_filter <- gen_deep_learning_filter(l_value1 = l_value1, l_value2 = l_value2)\n    orienting_sec_mem <- rbind(orienting_sec_mem, item_copies_matrix[i, ] * deep_filter)\n  } else {\n  shallow_filter <- gen_shallow_learning_filter(l_value1 = l_value1, l_value2 = l_value2)\n    orienting_sec_mem <- rbind(orienting_sec_mem, item_copies_matrix[i, ] * shallow_filter)\n    }\n}\nreturn(orienting_sec_mem)\n}\n\n# Convert activations matrix to intensity matrix (slightly different from previous function)\n\norient_convert_to_intensity_matrix <- function(activations_matrix, max_num_of_copies = 5, num_of_traces_per_freq = 4) {\n  intensity_vector <- rowSums(activations_matrix)\n  intensity_matrix <- matrix(intensity_vector, nrow = max_num_of_copies + 1, ncol = num_of_traces_per_freq/2, byrow = TRUE)\n  return(intensity_matrix)\n}\n\n\n# Encode traces into secondary memory\nitem_mat <- gen_item_matrix(item_size = 25)\nitem_copies_mat <- gen_item_mat_copies(item_mat)\nor_sec_mem <- gen_orienting_secondary_mem(item_copies_mat)\nprobe_mat <- gen_probes(item_mat)\n\nactiv_mat_raw <- calc_activs_for_mult_probes(probe_mat, or_sec_mem)\nactiv_mat_shallow <- c()\nactiv_mat_deep <- c()\nfor(i in 1:nrow(activ_mat_raw)) {\n  if(i %% 2 == 0) {\n    activ_mat_deep <- rbind(activ_mat_deep, activ_mat_raw[i, ])\n  } else {\n    activ_mat_shallow <- rbind(activ_mat_shallow, activ_mat_raw[i, ])\n  }\n}\n\n\n\nintensity_shallow <- orient_convert_to_intensity_matrix(activ_mat_shallow)\n\n\nsim_orienting_once <- function(matrix_size = 20, item_size = 25, prob = c(1/3, 1/3, 1/3), l_value1 = 0.6, l_value2 = 0) {\n  item_matrix <- gen_item_matrix(matrix_size = matrix_size, item_size = item_size, prob = prob)\n  item_matrix_copies <- gen_item_mat_copies(item_matrix)\n  orienting_memory <- gen_orienting_secondary_mem(item_matrix_copies, l_value1 = l_value1, l_value2 = l_value2)\n  probe_matrix <- gen_probes(item_matrix, prob = prob, max_num_of_copies = max_num_of_copies, num_of_traces_per_freq = num_of_traces_per_freq)\n  raw_activations <- calc_activs_for_mult_probes(probe_matrix, orienting_memory)\n  \n  shallow_activations <- c()\n  deep_activations <- c()\n  for(i in 1:nrow(raw_activations)) {\n  if(i %% 2 == 0) {\n    deep_activations <- rbind(deep_activations, raw_activations[i, ])\n  } else {\n    shallow_activations <- rbind(shallow_activations, raw_activations[i, ])\n  }\n  }\n  intensity_matrix <- rbind(orient_convert_to_intensity_matrix(shallow_activations), orient_convert_to_intensity_matrix(deep_activations))\n  return(intensity_matrix)\n  }\n  \n\nsim_orienting_multiple <- function(n_of_sim, matrix_size = 20, item_size = 25, prob = c(1/3, 1/3, 1/3), l_value1 = 0.6, l_value2 = 0) {\n  raw_intensity_matrix <- c()\n  for(i in 1:n_of_sim) {\n    temp_intensity <- sim_orienting_once(matrix_size = matrix_size, item_size = item_size, prob = prob, l_value1 = l_value1, l_value2 = l_value2)\n    raw_intensity_matrix <- cbind(raw_intensity_matrix, temp_intensity)\n  }\n  level_of_process <- c(rep(\"shallow\", max_num_of_copies + 1), rep(\"deep\", max_num_of_copies + 1))\n frequency <- rep(0:max_num_of_copies, 2)\n  intensity_df <- bind_cols(level_of_process, frequency, data.frame(raw_intensity_matrix)) %>% rename(level_of_process = ...1, frequency = ...2) %>% \n    pivot_longer(cols = starts_with(\"X\"), names_to = \"Drop\", values_to = \"Intensity\") %>% select(!Drop)\n  return(intensity_df)\n}\n\n\norient_intensity <- sim_orienting_multiple(1000)\n\nNew names:\n• `` -> `...1`\n• `` -> `...2`\n\nsummarized_intensity <- orient_intensity %>% group_by(level_of_process, frequency) %>% summarize(mean_intensity = mean(Intensity))\n\n`summarise()` has grouped output by 'level_of_process'. You can override using\nthe `.groups` argument.\n\n\n\nggplot(summarized_intensity, aes(x = frequency, y = mean_intensity, group = level_of_process, color = level_of_process)) + geom_path() + geom_point()\n\n\n\n\nThis graph is very different from Hintzman’s. I think I may have made the same mistake as last time with applying the learning rate before making copies, resulting in identically learnt traces."
  },
  {
    "objectID": "posts/Attention1/index.html",
    "href": "posts/Attention1/index.html",
    "title": "A MINERVA model for attention",
    "section": "",
    "text": "Applying MINERVA to the Stroop task\n\nset.seed(30)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(gtools)\nlibrary(bench)\n\ncolors <- c(\"red\", \"yellow\", \"green\", \"blue\")\n\nnum_of_feat <- 10\nnum_of_congruent_trials <- 48\nnum_of_incongruent_trials <- 48\nattention_dampeners <- c(.5, .6, .7, .8, .9, 1)\ncon_trials_per_color <- num_of_congruent_trials/length(colors)\ncongruent_answer_column <- c()\nfor(i in 1:length(colors)){\n  current_correct_answer <- rep(colors[i], con_trials_per_color)\n  congruent_answer_column <- c(congruent_answer_column, current_correct_answer)\n}\n\nincon_permuts <- permutations(length(colors), 2)\nnum_of_repeat_incon_trials <- num_of_incongruent_trials/nrow(incon_permuts)\nincongruent_answer_column <- c()\nfor(i in 1:nrow(incon_permuts)){\n  current_correct_answer <- rep(colors[incon_permuts[i, 1]], num_of_repeat_incon_trials)\n  incongruent_answer_column <- c(incongruent_answer_column, current_correct_answer)\n}\n\nfull_correct_answer_column <- c(congruent_answer_column, incongruent_answer_column)\n\ngen_color <- function() {\n  color <- sample(c(1, 0, -1), num_of_feat, replace = TRUE, prob = c(1/3, 1/3, 1/3))\n  return(color)\n}\n\npatterns <- c()\nfor(i in 1:length(colors)) {\n  temp_pattern <- gen_color()\n  patterns <- rbind(patterns, temp_pattern)\n}\n\nrow.names(patterns) <- colors\nblank <- numeric(length = num_of_feat)\n\nmemory <- c()\nfor(i in 1:nrow(patterns)) {\n  ink_set <- c(patterns[i, ], blank, patterns[i, ], blank)\n  word_set <- c(blank, patterns[i, ], blank, patterns[i, ])\n  congruent_set <- c(patterns[i, ], patterns[i, ], patterns[i, ], patterns[i, ])\n  memory <- rbind(memory, ink_set, word_set, congruent_set)\n}\nfull_memory <- rbind(memory, memory, memory, memory, memory)\n\n\ncongruent_probes <- c()\nfor(i in 1:nrow(patterns)){\n  current_trials <- c(patterns[i, ], patterns[i, ], blank, blank)\n  current_combi <- matrix(current_trials, nrow = con_trials_per_color, ncol = length(current_trials), byrow=TRUE)\n  congruent_probes <- rbind(congruent_probes, current_combi)\n}\n\n\nincongruent_probes <- c()\nfor(i in 1:nrow(incon_permuts)){\n  current_trials <- c(patterns[incon_permuts[i, 1], ], patterns[incon_permuts[i, 2], ], blank, blank)\n  current_combi <- matrix(current_trials, nrow = num_of_repeat_incon_trials, ncol = length(current_trials), byrow = TRUE)\n  incongruent_probes <- rbind(incongruent_probes, current_combi)\n}\n\nfull_probes <- rbind(congruent_probes, incongruent_probes)\n\n# Activation function for single probe (borrowed from Matt)\nget_activations_3 <- function(probe, mem) {\n  \n  as.numeric(((probe %*% t(mem)) / rowSums(t((probe == 0) * t(mem == 0)) == 0))^3)\n}\n\n# Function for calculating echo content for single probe (modified from previous exercise)\ncalc_echo_content_for_each_probe <- function(activations_vector, memory_matrix) {\n    echo_content_for_each_probe <- c()\n    for(feat in 1:ncol(memory_matrix)){\n      content_temp <- 0\n        for(memory in 1:nrow(memory_matrix)) {\n          current_product <- activations_vector[memory] * memory_matrix[memory, feat]\n          content_temp <- content_temp + current_product\n        }\n      echo_content_for_each_probe <- c(echo_content_for_each_probe, content_temp)\n  }\n  return(echo_content_for_each_probe)\n}\n\n# Function for generating dampened content\n\ndampen_distractor <- function(probe_content){\n  non_zero <- FALSE\n  while(non_zero == FALSE){\n  dampen_value <- sample(attention_dampeners, 1)\n  attention_dampening_filter <- c(rep(1, num_of_feat*3), sample(c(0, 1), num_of_feat, replace = TRUE, prob = c(1 - dampen_value, dampen_value)))\n   dampened_content <- probe_content * attention_dampening_filter\n   if(sum(abs(dampened_content[(num_of_feat*3+1):(num_of_feat*4)])) != 0){\n     non_zero <- TRUE\n   }\n  }\n     dampened_output <- c(dampened_content, dampen_value)\n   return(dampened_output)\n}\n\n# Function for one Stroop trial (dampening mechanism)\nstroop_one_trial_dampen <- function(probe_vector, memory_matrix, color_patterns){\n # Attention dampening filter dampens attention to words (in favor of ink color)\n  success <- FALSE\n  start_time <- Sys.time()\n  while(success == FALSE) {\n  probe_activation <- get_activations_3(probe_vector, memory_matrix)\n   probe_content <- calc_echo_content_for_each_probe(probe_activation, memory_matrix)\n   dampened_output <- dampen_distractor(probe_content)\n   dampened_content <- dampened_output[1:(num_of_feat*4)]\n   dampen_value <- dampened_output[num_of_feat*4 + 1]\n   trial_ink_corrs <- c()\n   trial_word_corrs <- c()\n   for(j in 1:nrow(color_patterns)){\n    ink_corr <- cor(color_patterns[j, ], dampened_content[(num_of_feat*2 + 1) : (num_of_feat*3)])\n    if(is.na(ink_corr)){\n      ink_corr <- 0\n    }\n    word_corr <- cor(color_patterns[j, ], dampened_content[(num_of_feat*3 + 1) : (num_of_feat*4)])\n    if(is.na(word_corr)){\n      word_corr <- 0\n    }\n    trial_ink_corrs <- c(trial_ink_corrs, ink_corr)\n    trial_word_corrs <- c(trial_word_corrs, word_corr)\n }\n   if(max(trial_ink_corrs) >= max(trial_word_corrs)){\n     response <- color_patterns[which.max(trial_ink_corrs), ]\n   } else {\n     response <- color_patterns[which.max(trial_word_corrs), ]\n   }\n   accuracy <- all(response == probe_vector[1:num_of_feat])\n   success <- accuracy == TRUE\n  }\n  end_time <- Sys.time()\n  if(all(probe_vector[1:num_of_feat] == probe_vector[(num_of_feat+1):(num_of_feat*2)])){\n    condition <- \"congruent\"\n  } else {\n    condition <- \"incongruent\"\n  }\n  trial_outcome <- data.frame(condition = condition, dampener = dampen_value, time_elapsed = end_time - start_time, accuracy = accuracy)\n   return(trial_outcome)\n}\n\n# Function for full Stroop task simulation (dampening mechanism)\nfull_stroop_sim_dampen <- function(probe_matrix, memory_matrix, color_patterns) {\n  all_outcomes <- c()\n  for(i in 1:nrow(probe_matrix)){\n    trial_outcome <- stroop_one_trial_dampen(probe_matrix[i, ], memory_matrix, color_patterns)\n    all_outcomes <- bind_rows(all_outcomes, trial_outcome)\n  }\n  all_outcomes <- all_outcomes %>% mutate(correct_answer = full_correct_answer_column)\n  return(all_outcomes)\n}\n\n\nstroop_results <- data.frame()\n\nfor(i in 1:100){\n  current_participant <- full_stroop_sim_dampen(full_probes, full_memory, patterns)\n  stroop_results <- bind_rows(stroop_results, current_participant)\n}\n\n\nstroop_means <- stroop_results %>% group_by(condition, dampener) %>% summarize(mean_dur = mean(time_elapsed))\n\n`summarise()` has grouped output by 'condition'. You can override using the\n`.groups` argument.\n\nggplot(stroop_means, aes(x = dampener, y = mean_dur, color = condition)) + geom_line()\n\nDon't know how to automatically pick scale for object of type difftime. Defaulting to continuous.\n\n\n\n\n\nThe trend for the congruent condition makes sense, since the amount of dampening of the distractor should not influence performance, since the target and distractor do not compete for the response. However, for the incongruent condition, it doesn’t really make sense that the more the distractors is dampened, the longer it takes to give the correct response. In fact, that’s the opposite of what we would expect."
  },
  {
    "objectID": "posts/Attention2/index.html",
    "href": "posts/Attention2/index.html",
    "title": "A MINERVA model for attention (Part 2)",
    "section": "",
    "text": "# Define Functions\n\n# function to generate a single item\ngenerate_item <- function(size=20,prob=c(1/3,1/3,1/3)){\n  item <- sample(c(1,0,-1),\n           size = size,\n           replace = TRUE,\n           prob = prob)\n  return(item)\n}\n\n# compute activations\nget_activations <- function(probe,memory){\n   as.numeric(((probe %*% t(memory)) / rowSums(t((probe == 0) * t(memory == 0)) == 0)))\n}\n\n# generate echo\nget_echo <- function(probe, mem, tau=3, output='intensity') {\n    activations <- get_activations(probe,mem)\n    if(output == \"intensity\"){\n      return(sum(activations^tau))\n    }\n    if(output == \"echo\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      return(summed_echo)\n    }\n    if(output == \"both\"){\n      weighted_memory <- mem * (activations^tau)  \n      summed_echo <- colSums(weighted_memory)\n      model_output <- list(intensity = sum(activations^tau),\n                           echo = summed_echo)\n      return(model_output)\n    }\n    \n}\n\nget_cosine_sim <- function(echo_features, response_matrix) {\n  similarity_df <- data.frame()\n  \n  for(i in 1:nrow(response_matrix)){\n    current_sim <- cosine(echo_features, response_matrix[i, ])\n    similarity_df <- rbind(similarity_df, current_sim)\n  }\n  names(similarity_df) <- \"similarity\"\n  row.names(similarity_df) <- c(\"red\", \"green\", \"blue\", \"yellow\")\n  return(similarity_df)\n}\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lsa)\n\nLoading required package: SnowballC\n\n# Generate Stroop dataframe\n\ncongruent_items <- tibble(word = c(\"red\",\"green\",\"blue\",\"yellow\"),\n                          color = c(\"red\",\"green\",\"blue\",\"yellow\"),\n                          response = c(\"red\",\"green\",\"blue\",\"yellow\"),\n                          congruency = rep(\"con\",4))\n\nincongruent_items <- tibble(word = c(\"red\",\"red\",\"red\",\n                                     \"green\",\"green\",\"green\",\n                                     \"blue\",\"blue\",\"blue\",\n                                     \"yellow\",\"yellow\",\"yellow\"),\n                          color = c(\"green\",\"blue\",\"yellow\",\n                                    \"red\",\"blue\",\"yellow\",\n                                    \"red\",\"green\",\"yellow\",\n                                    \"red\",\"green\",\"blue\"),\n                          response = c(\"green\",\"blue\",\"yellow\",\n                                    \"red\",\"blue\",\"yellow\",\n                                    \"red\",\"green\",\"yellow\",\n                                    \"red\",\"green\",\"blue\"),\n                          congruency = rep(\"inc\",12))\n\n\n\ntraining_trials <- congruent_items            \n\nstroop_trials <- congruent_items %>% \n  slice(rep(1:n(),each=3)) %>%\n  rbind(incongruent_items) %>%\n  slice(sample(1:n())) %>%\n  slice(rep(1:n(),each=2))\n\n# Function for simulating multiple subjects on Stroop task\n\nsim_multiple_stroop <- function(training, stroop, n_of_sim) {\n  all_sub_results <- data.frame()\n\n  for(i in 1:n_of_sim){\n# Make vector representations\n\nword <- t(replicate(4, generate_item(size=20,prob=c(1/3,1/3,1/3))))\nrow.names(word) <- c(\"red\",\"green\",\"blue\",\"yellow\")\n\ncolor <- t(replicate(4, generate_item(size=20,prob=c(1/3,1/3,1/3))))\nrow.names(color) <- c(\"red\",\"green\",\"blue\",\"yellow\")\n\nresponse <- t(replicate(4, generate_item(size=20,prob=c(1/3,1/3,1/3))))\nrow.names(response) <- c(\"red\",\"green\",\"blue\",\"yellow\")\n\n## Trial constructor\n\nminerva_trials <- function(df,vec_length=20){\n  minerva_matrix <- matrix(0,nrow = dim(df)[1],ncol=vec_length*3)\n  \n  for(i in 1:dim(df)[1]){\n    minerva_matrix[i,] <- c(word[df[i,]$word, ], color[df[i,]$color, ], response[df[i,]$response, ])\n  }\n  \n  return(minerva_matrix)\n}\n\ntraining_matrix <- minerva_trials(training)\ntrial_matrix <- minerva_trials(stroop)\n\n## run model\n\nsubject_results <- data.frame()\n\nmemory <- training_matrix\n\nfor(i in 1:dim(trial_matrix)[1]){\n  \n  probe <- c(trial_matrix[i,1:40],rep(0,20))\n  model_output <- get_echo(probe, memory, tau=3, output = \"both\")\n  similarities <- get_cosine_sim(model_output$echo[41:60], response)\n  memory <- rbind(memory,trial_matrix[i,])\n  \n  trial_data <- data.frame(red = similarities['red', ],\n                           green = similarities['green', ],\n                           blue = similarities['blue', ],\n                           yellow = similarities['yellow', ],\n                           intensity = model_output$intensity,\n                           max_sim = row.names(similarities)[which.max(similarities$similarity)],\n                           max_sim_num = max(similarities),\n                           coactivation = max(similarities)/(sum(abs(similarities))))\n  \n  subject_results <- rbind(subject_results,trial_data)\n}\n\nrow.names(subject_results) <- 1:dim(subject_results)[1]\n\n## analyze data \n\none_sub <- cbind(stroop_trials,subject_results)\n\none_sub <- one_sub %>%\n  mutate(correct = response == max_sim)\n\nsummarized_one_sub <- one_sub %>%\n  group_by(congruency) %>%\n  summarize(mean_correct = mean(correct),\n            mean_max = mean(max_sim_num),\n            mean_coactivation = mean(coactivation))\nall_sub_results <- rbind(all_sub_results, summarized_one_sub)\n  }\nreturn(all_sub_results)\n}\n\n\n# Equal number of congruent and incongruent trials\n\nequal_sim <- sim_multiple_stroop(training_trials, stroop_trials, 100)\nggplot(equal_sim, aes(x = congruency, y = mean_max)) + geom_boxplot()\n\n\n\n# 75% congruent and 25% incongruent trials\n\nstroop_75_25 <- congruent_items %>% \n  slice(rep(1:n(),each=9)) %>%\n  rbind(incongruent_items) %>%\n  slice(sample(1:n()))\n\nmore_congruent_sim <- sim_multiple_stroop(training_trials, stroop_75_25, 100)\nggplot(more_congruent_sim, aes(x = congruency, y = mean_max)) + geom_boxplot()\n\n\n\n# 25% congruent and 75% incongruent trials\n\nrep_incongruent <- incongruent_items %>% slice(rep(1:n(), each = 3))\nstroop_25_75 <- congruent_items %>%\n  slice(rep(1:n(),each=3)) %>%\n  rbind(rep_incongruent) %>%\n  slice(sample(1:n()))\n\nmore_incongruent_sim <- sim_multiple_stroop(training_trials, stroop_25_75, 100)\nggplot(more_incongruent_sim, aes(x = congruency, y = mean_max)) + geom_boxplot()\n\n\n\n\nWhy is the Stroop effect stronger when there is an equal proportion of congruent and incongruent trials than when there are more congruent than incongruent trials?"
  }
]